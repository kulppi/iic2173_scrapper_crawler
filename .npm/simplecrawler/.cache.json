{"_id":"simplecrawler","_rev":"84-d00de18ee2b03396a28ce05ea5224ba9","name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","dist-tags":{"latest":"0.3.5"},"versions":{"0.0.3":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.0.3","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"repository":{"type":"git","url":"git://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./crawler.js","files":["./queue.js"],"engines":{"node":">=0.4.0"},"_npmUser":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"_id":"simplecrawler@0.0.3","dependencies":{},"devDependencies":{},"optionalDependencies":{},"_engineSupported":true,"_npmVersion":"1.1.0-3","_nodeVersion":"v0.6.9","_defaultsLoaded":true,"dist":{"shasum":"728ceee7465d1b0dc667de602339207635189004","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.0.3.tgz"},"readme":"","maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.0.4":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.0.4","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"repository":{"type":"git","url":"git://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./index.js","engines":{"node":">=0.4.0"},"_npmUser":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"_id":"simplecrawler@0.0.4","dependencies":{},"devDependencies":{},"optionalDependencies":{},"_engineSupported":true,"_npmVersion":"1.1.0-3","_nodeVersion":"v0.6.9","_defaultsLoaded":true,"dist":{"shasum":"afd003bfcb7f77a2ee6213618d984a77a8741ed6","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.0.4.tgz"},"readme":"","maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.0.5":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.0.5","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"repository":{"type":"git","url":"git://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./index.js","engines":{"node":">=0.4.0"},"_npmUser":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"_id":"simplecrawler@0.0.5","dependencies":{},"devDependencies":{},"optionalDependencies":{},"_engineSupported":true,"_npmVersion":"1.1.0-3","_nodeVersion":"v0.6.9","_defaultsLoaded":true,"dist":{"shasum":"605b79e7083fe2f410461c7e8c65fd2c18171210","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.0.5.tgz"},"readme":"","maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.0.6":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.0.6","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"repository":{"type":"git","url":"git://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./index.js","engines":{"node":">=0.4.0"},"_npmUser":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"_id":"simplecrawler@0.0.6","dependencies":{},"devDependencies":{},"optionalDependencies":{},"_engineSupported":true,"_npmVersion":"1.1.0-3","_nodeVersion":"v0.6.9","_defaultsLoaded":true,"dist":{"shasum":"b4bf5f70b359f45379702853d13bace7cbd8586a","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.0.6.tgz"},"readme":"","maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.0.7":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.0.7","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./index.js","engines":{"node":">=0.4.0"},"readme":"# Simple web-crawler for node.js\n\nSimplecrawler is designed to provide the most basic possible API for crawling websites, while being as flexible and robust as possible. I wrote simplecrawler to archive, analyse, and search some very large websites. It has happily chewed through 50,000 pages and written tens of gigabytes to disk without issue.\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can replace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except when discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nCreating a new crawler is very simple. First you'll need to include it:\n\n```javascript\nvar Crawler = require(\"simplecrawler\").Crawler;\n```\n\nThen create your crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your webserver. Decrease the concurrency from five simultaneous requests - and increase the request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when creating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run through its queue finding linked\nresources on the domain to download, until it can't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually queue an item yourself.)\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem )\nFired when an item is spooled for fetching.\n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http response object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided as a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size we're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned as a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a request.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does not find any more to add. This event returns no arguments.\n\n####A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters an HTTP error status in the response. If you need this information, you can listen to simplecrawler's error events, and through node's native `data` event (`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let me know. I didn't include it because I didn't need it - but if it's important to people I might put it back in. :)\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n* `crawler.domain` - The domain to scan. By default, simplecrawler will restrict all requests to this domain.\n* `crawler.initialPath` - The initial path with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.initialPort` - The initial port with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.initialProtocol` - The initial protocol with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.interval` - The interval with which the crawler will spool up new requests (one per tick.) Defaults to 250ms.\n* `crawler.maxConcurrency` - The maximum number of requests the crawler will run simultaneously. Defaults to 5 - the default number of http agents nodejs will run.\n* `crawler.timeout` - The maximum time the crawler will wait for headers before aborting the request.\n* `crawler.userAgent` - The user agent the crawler will report. Defaults to `Node/SimpleCrawler 0.1 (http://www.github.com/cgiffard/node-simplecrawler)`.\n* `crawler.queue` - The queue in use by the crawler (Must implement the `FetchQueue` interface)\n* `crawler.filterByDomain` - Specifies whether the crawler will restrict queued requests to a given domain/domains.\n* `crawler.scanSubdomains` - Enables scanning subdomains (other than www) as well as the specified domain. Defaults to false.\n* `crawler.ignoreWWWDomain` - Treats the `www` domain the same as the originally specified domain. Defaults to true.\n* `crawler.stripWWWDomain` - Or go even further and strip WWW subdomain from requests altogether!\n* `crawler.discoverResources` - Use simplecrawler's internal resource discovery function. Defaults to true. (switch it off if you'd prefer to discover and queue resources yourself!)\n* `crawler.cache` - Specify a cache architecture to use when crawling. Must implement `SimpleCache` interface.\n* `crawler.useProxy` - The crawler should use an HTTP proxy to make its requests.\n* `crawler.proxyHostname` - The hostname of the proxy to use for requests.\n* `crawler.proxyPort` - The port of the proxy to use for requests.\n* `crawler.domainWhitelist` - An array of domains the crawler is permitted to crawl from. If other settings are more permissive, they will override this setting.\n* `crawler.supportedMimeTypes` - An array of RegEx objects used to determine supported MIME types (types of data simplecrawler will scan for links.) If you're  not using simplecrawler's resource discovery function, this won't have any effect.\n* `crawler.allowedProtocols` - An array of RegEx objects used to determine whether a URL protocol is supported. This is to deal with nonstandard protocol handlers that regular HTTP is sometimes given, like `feed:`. It does not provide support for non-http protocols (and why would it!?)\n* `crawler.maxResourceSize` - The maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n* `crawler.downloadUnsupported` - Simplecrawler will download files it can't parse. Defaults to true, but if you'd rather save the RAM and GC lag, switch it off.\n* `crawler.needsAuth` - Flag to specify if the domain you are hitting requires basic authentication\n* `crawler.authUser` - Username provdied for needsAuth flag\n* `crawler.authPass` - Passowrd provided for needsAuth flag\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed at `crawler.queue` (assuming you called your Crawler() object `crawler`.) It provides array access, so you can get to queue items just with array notation and an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate interface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nYou could always just `.push` a new resource onto the queue, but you'd need to have it all in the correct format, and validate the URL yourself, and oh wouldn't that be a pain. Instead, use the `queue.add` function provided for your convenience:\n\n```javascript\ncrawler.queue.add(protocol,domain,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can remember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items, it helps to know what's inside them. These are the properties every queue item is expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `domain` - The full domain of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The upside is, the queue actually has some convenient functions for getting simple aggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network performance of your crawl (so far.) This is done live, so don't check it thirty times a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the `crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions respectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given status at any one time, and/or retreive them. That's easy with `crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.getwithStatus` returns the number of queued items with a given status, while `crawler.queue.getWithStatus` returns an array of the queue items themselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n* `crawler.queue.complete` - returns the number of queue items which have been completed (marked as fetched)\n* `crawler.queue.errors` - returns the number of requests which have failed (404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if your application fails or you need to abort the crawl for some reason. (Perhaps you just want to finish off for the night and pick it up tomorrow!) The `crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be asynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when you need to save the queue - don't call them every request or your application's performance will be incredibly poor - they block like *crazy*. That said, using them when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n### Licence\n\nYou may copy and use this library as you see fit (including commercial use) and modify it, as long as you retain my attribution comment (which includes my name, link to this github page, and library version) at the top of all script files. You may not, under any circumstances, claim you wrote this library, or remove my attribution. (Fair's fair!)\n\nI'd appreciate it if you'd contribute patches back, but you don't have to. If you do, I'll be happy to credit your contributions!\n\n### Contributors\n\nThanks to [Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support. Thanks also to [Mike Moulton](https://github.com/mmoulton) for [fixing a bug in the URL discovery mechanism](https://github.com/cgiffard/node-simplecrawler/pull/3)\nand [Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword naming collision with node 0.8's EventEmitter.","_id":"simplecrawler@0.0.7","dist":{"shasum":"992f9ffecaafc12b5deb7fdf62a1fc59432e5633","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.0.7.tgz"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.0.8":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.0.8","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./index.js","engines":{"node":">=0.4.0"},"readme":"# Simple web-crawler for node.js\n\nSimplecrawler is designed to provide the most basic possible API for crawling websites, while being as flexible and robust as possible. I wrote simplecrawler to archive, analyse, and search some very large websites. It has happily chewed through 50,000 pages and written tens of gigabytes to disk without issue.\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can replace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except when discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nCreating a new crawler is very simple. First you'll need to include it:\n\n```javascript\nvar Crawler = require(\"simplecrawler\").Crawler;\n```\n\nThen create your crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your webserver. Decrease the concurrency from five simultaneous requests - and increase the request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when creating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run through its queue finding linked\nresources on the domain to download, until it can't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually queue an item yourself.)\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem )\nFired when an item is spooled for fetching.\n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http response object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided as a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size we're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned as a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a request.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does not find any more to add. This event returns no arguments.\n\n####A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters an HTTP error status in the response. If you need this information, you can listen to simplecrawler's error events, and through node's native `data` event (`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let me know. I didn't include it because I didn't need it - but if it's important to people I might put it back in. :)\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n* `crawler.host` - The domain to scan. By default, simplecrawler will restrict all requests to this domain.\n* `crawler.initialPath` - The initial path with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.initialPort` - The initial port with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.initialProtocol` - The initial protocol with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.interval` - The interval with which the crawler will spool up new requests (one per tick.) Defaults to 250ms.\n* `crawler.maxConcurrency` - The maximum number of requests the crawler will run simultaneously. Defaults to 5 - the default number of http agents nodejs will run.\n* `crawler.timeout` - The maximum time the crawler will wait for headers before aborting the request.\n* `crawler.userAgent` - The user agent the crawler will report. Defaults to `Node/SimpleCrawler 0.1 (http://www.github.com/cgiffard/node-simplecrawler)`.\n* `crawler.queue` - The queue in use by the crawler (Must implement the `FetchQueue` interface)\n* `crawler.filterByDomain` - Specifies whether the crawler will restrict queued requests to a given domain/domains.\n* `crawler.scanSubdomains` - Enables scanning subdomains (other than www) as well as the specified domain. Defaults to false.\n* `crawler.ignoreWWWDomain` - Treats the `www` domain the same as the originally specified domain. Defaults to true.\n* `crawler.stripWWWDomain` - Or go even further and strip WWW subdomain from requests altogether!\n* `crawler.discoverResources` - Use simplecrawler's internal resource discovery function. Defaults to true. (switch it off if you'd prefer to discover and queue resources yourself!)\n* `crawler.cache` - Specify a cache architecture to use when crawling. Must implement `SimpleCache` interface.\n* `crawler.useProxy` - The crawler should use an HTTP proxy to make its requests.\n* `crawler.proxyHostname` - The hostname of the proxy to use for requests.\n* `crawler.proxyPort` - The port of the proxy to use for requests.\n* `crawler.domainWhitelist` - An array of domains the crawler is permitted to crawl from. If other settings are more permissive, they will override this setting.\n* `crawler.supportedMimeTypes` - An array of RegEx objects used to determine supported MIME types (types of data simplecrawler will scan for links.) If you're  not using simplecrawler's resource discovery function, this won't have any effect.\n* `crawler.allowedProtocols` - An array of RegEx objects used to determine whether a URL protocol is supported. This is to deal with nonstandard protocol handlers that regular HTTP is sometimes given, like `feed:`. It does not provide support for non-http protocols (and why would it!?)\n* `crawler.maxResourceSize` - The maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n* `crawler.downloadUnsupported` - Simplecrawler will download files it can't parse. Defaults to true, but if you'd rather save the RAM and GC lag, switch it off.\n* `crawler.needsAuth` - Flag to specify if the domain you are hitting requires basic authentication\n* `crawler.authUser` - Username provdied for needsAuth flag\n* `crawler.authPass` - Passowrd provided for needsAuth flag\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed at `crawler.queue` (assuming you called your Crawler() object `crawler`.) It provides array access, so you can get to queue items just with array notation and an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate interface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nYou could always just `.push` a new resource onto the queue, but you'd need to have it all in the correct format, and validate the URL yourself, and oh wouldn't that be a pain. Instead, use the `queue.add` function provided for your convenience:\n\n```javascript\ncrawler.queue.add(protocol,domain,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can remember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items, it helps to know what's inside them. These are the properties every queue item is expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `domain` - The full domain of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The upside is, the queue actually has some convenient functions for getting simple aggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network performance of your crawl (so far.) This is done live, so don't check it thirty times a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the `crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions respectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given status at any one time, and/or retreive them. That's easy with `crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.getwithStatus` returns the number of queued items with a given status, while `crawler.queue.getWithStatus` returns an array of the queue items themselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n* `crawler.queue.complete` - returns the number of queue items which have been completed (marked as fetched)\n* `crawler.queue.errors` - returns the number of requests which have failed (404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if your application fails or you need to abort the crawl for some reason. (Perhaps you just want to finish off for the night and pick it up tomorrow!) The `crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be asynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when you need to save the queue - don't call them every request or your application's performance will be incredibly poor - they block like *crazy*. That said, using them when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n### Licence\n\nYou may copy and use this library as you see fit (including commercial use) and modify it, as long as you retain my attribution comment (which includes my name, link to this github page, and library version) at the top of all script files. You may not, under any circumstances, claim you wrote this library, or remove my attribution. (Fair's fair!)\n\nI'd appreciate it if you'd contribute patches back, but you don't have to. If you do, I'll be happy to credit your contributions!\n\n### Contributors\n\nThanks to [Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support. Thanks also to [Mike Moulton](https://github.com/mmoulton) for [fixing a bug in the URL discovery mechanism](https://github.com/cgiffard/node-simplecrawler/pull/3)\nand [Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword naming collision with node 0.8's EventEmitter.","_id":"simplecrawler@0.0.8","dist":{"shasum":"6531a4073672bdf78380e7e471c6c5f9ddd41fcf","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.0.8.tgz"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.0.9":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.0.9","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./index.js","engines":{"node":">=0.4.0"},"readme":"# Simple web-crawler for node.js\n\nSimplecrawler is designed to provide the most basic possible API for crawling websites, while being as flexible and robust as possible. I wrote simplecrawler to archive, analyse, and search some very large websites. It has happily chewed through 50,000 pages and written tens of gigabytes to disk without issue.\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can replace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except when discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nCreating a new crawler is very simple. First you'll need to include it:\n\n```javascript\nvar Crawler = require(\"simplecrawler\").Crawler;\n```\n\nThen create your crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your webserver. Decrease the concurrency from five simultaneous requests - and increase the request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when creating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run through its queue finding linked\nresources on the domain to download, until it can't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually queue an item yourself.)\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem )\nFired when an item is spooled for fetching.\n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http response object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided as a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size we're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned as a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a request.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as the second parameter.\n* `discoverycomplete` ( queueItem, resources)\nFired when linked resources have been discovered. Passes an array of resources (as URLs) as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does not find any more to add. This event returns no arguments.\n\n####A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters an HTTP error status in the response. If you need this information, you can listen to simplecrawler's error events, and through node's native `data` event (`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let me know. I didn't include it because I didn't need it - but if it's important to people I might put it back in. :)\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n* `crawler.host` - The domain to scan. By default, simplecrawler will restrict all requests to this domain.\n* `crawler.initialPath` - The initial path with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.initialPort` - The initial port with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.initialProtocol` - The initial protocol with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.interval` - The interval with which the crawler will spool up new requests (one per tick.) Defaults to 250ms.\n* `crawler.maxConcurrency` - The maximum number of requests the crawler will run simultaneously. Defaults to 5 - the default number of http agents nodejs will run.\n* `crawler.timeout` - The maximum time the crawler will wait for headers before aborting the request.\n* `crawler.userAgent` - The user agent the crawler will report. Defaults to `Node/SimpleCrawler 0.1 (http://www.github.com/cgiffard/node-simplecrawler)`.\n* `crawler.queue` - The queue in use by the crawler (Must implement the `FetchQueue` interface)\n* `crawler.filterByDomain` - Specifies whether the crawler will restrict queued requests to a given domain/domains.\n* `crawler.scanSubdomains` - Enables scanning subdomains (other than www) as well as the specified domain. Defaults to false.\n* `crawler.ignoreWWWDomain` - Treats the `www` domain the same as the originally specified domain. Defaults to true.\n* `crawler.stripWWWDomain` - Or go even further and strip WWW subdomain from requests altogether!\n* `crawler.discoverResources` - Use simplecrawler's internal resource discovery function. Defaults to true. (switch it off if you'd prefer to discover and queue resources yourself!)\n* `crawler.cache` - Specify a cache architecture to use when crawling. Must implement `SimpleCache` interface.\n* `crawler.useProxy` - The crawler should use an HTTP proxy to make its requests.\n* `crawler.proxyHostname` - The hostname of the proxy to use for requests.\n* `crawler.proxyPort` - The port of the proxy to use for requests.\n* `crawler.domainWhitelist` - An array of domains the crawler is permitted to crawl from. If other settings are more permissive, they will override this setting.\n* `crawler.supportedMimeTypes` - An array of RegEx objects used to determine supported MIME types (types of data simplecrawler will scan for links.) If you're  not using simplecrawler's resource discovery function, this won't have any effect.\n* `crawler.allowedProtocols` - An array of RegEx objects used to determine whether a URL protocol is supported. This is to deal with nonstandard protocol handlers that regular HTTP is sometimes given, like `feed:`. It does not provide support for non-http protocols (and why would it!?)\n* `crawler.maxResourceSize` - The maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n* `crawler.downloadUnsupported` - Simplecrawler will download files it can't parse. Defaults to true, but if you'd rather save the RAM and GC lag, switch it off.\n* `crawler.needsAuth` - Flag to specify if the domain you are hitting requires basic authentication\n* `crawler.authUser` - Username provdied for needsAuth flag\n* `crawler.authPass` - Passowrd provided for needsAuth flag\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed at `crawler.queue` (assuming you called your Crawler() object `crawler`.) It provides array access, so you can get to queue items just with array notation and an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate interface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nYou could always just `.push` a new resource onto the queue, but you'd need to have it all in the correct format, and validate the URL yourself, and oh wouldn't that be a pain. Instead, use the `queue.add` function provided for your convenience:\n\n```javascript\ncrawler.queue.add(protocol,domain,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can remember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items, it helps to know what's inside them. These are the properties every queue item is expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `domain` - The full domain of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The upside is, the queue actually has some convenient functions for getting simple aggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network performance of your crawl (so far.) This is done live, so don't check it thirty times a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the `crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions respectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given status at any one time, and/or retreive them. That's easy with `crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.getwithStatus` returns the number of queued items with a given status, while `crawler.queue.getWithStatus` returns an array of the queue items themselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n* `crawler.queue.complete` - returns the number of queue items which have been completed (marked as fetched)\n* `crawler.queue.errors` - returns the number of requests which have failed (404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if your application fails or you need to abort the crawl for some reason. (Perhaps you just want to finish off for the night and pick it up tomorrow!) The `crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be asynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when you need to save the queue - don't call them every request or your application's performance will be incredibly poor - they block like *crazy*. That said, using them when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n### Licence\n\nYou may copy and use this library as you see fit (including commercial use) and modify it, as long as you retain my attribution comment (which includes my name, link to this github page, and library version) at the top of all script files. You may not, under any circumstances, claim you wrote this library, or remove my attribution. (Fair's fair!)\n\nI'd appreciate it if you'd contribute patches back, but you don't have to. If you do, I'll be happy to credit your contributions!\n\n### Contributors\n\nThanks to [Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support.\nThanks also to [Mike Moulton](https://github.com/mmoulton) for [fixing a bug in the URL discovery mechanism](https://github.com/cgiffard/node-simplecrawler/pull/3),\nas well as [adding the `discoverycomplete` event](https://github.com/cgiffard/node-simplecrawler/pull/10),\nand [Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword naming collision with node 0.8's EventEmitter.\n","_id":"simplecrawler@0.0.9","dist":{"shasum":"a459574a3aa9b0aede4872811f001d8c2fc407a2","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.0.9.tgz"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.0.10":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.0.10","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./index.js","engines":{"node":">=0.4.0"},"readme":"# Simple web-crawler for node.js\n\nSimplecrawler is designed to provide the most basic possible API for crawling websites, while being as flexible and robust as possible. I wrote simplecrawler to archive, analyse, and search some very large websites. It has happily chewed through 50,000 pages and written tens of gigabytes to disk without issue.\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can replace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except when discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nCreating a new crawler is very simple. First you'll need to include it:\n\n```javascript\nvar Crawler = require(\"simplecrawler\").Crawler;\n```\n\nThen create your crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your webserver. Decrease the concurrency from five simultaneous requests - and increase the request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when creating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run through its queue finding linked\nresources on the domain to download, until it can't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually queue an item yourself.)\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem )\nFired when an item is spooled for fetching.\n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http response object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided as a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size we're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned as a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a request.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as the second parameter.\n* `discoverycomplete` ( queueItem, resources)\nFired when linked resources have been discovered. Passes an array of resources (as URLs) as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does not find any more to add. This event returns no arguments.\n\n####A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters an HTTP error status in the response. If you need this information, you can listen to simplecrawler's error events, and through node's native `data` event (`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let me know. I didn't include it because I didn't need it - but if it's important to people I might put it back in. :)\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n* `crawler.host` - The domain to scan. By default, simplecrawler will restrict all requests to this domain.\n* `crawler.initialPath` - The initial path with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.initialPort` - The initial port with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.initialProtocol` - The initial protocol with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.interval` - The interval with which the crawler will spool up new requests (one per tick.) Defaults to 250ms.\n* `crawler.maxConcurrency` - The maximum number of requests the crawler will run simultaneously. Defaults to 5 - the default number of http agents nodejs will run.\n* `crawler.timeout` - The maximum time the crawler will wait for headers before aborting the request.\n* `crawler.userAgent` - The user agent the crawler will report. Defaults to `Node/SimpleCrawler <version> (http://www.github.com/cgiffard/node-simplecrawler)`.\n* `crawler.queue` - The queue in use by the crawler (Must implement the `FetchQueue` interface)\n* `crawler.filterByDomain` - Specifies whether the crawler will restrict queued requests to a given domain/domains.\n* `crawler.scanSubdomains` - Enables scanning subdomains (other than www) as well as the specified domain. Defaults to false.\n* `crawler.ignoreWWWDomain` - Treats the `www` domain the same as the originally specified domain. Defaults to true.\n* `crawler.stripWWWDomain` - Or go even further and strip WWW subdomain from requests altogether!\n* `crawler.discoverResources` - Use simplecrawler's internal resource discovery function. Defaults to true. (switch it off if you'd prefer to discover and queue resources yourself!)\n* `crawler.cache` - Specify a cache architecture to use when crawling. Must implement `SimpleCache` interface.\n* `crawler.useProxy` - The crawler should use an HTTP proxy to make its requests.\n* `crawler.proxyHostname` - The hostname of the proxy to use for requests.\n* `crawler.proxyPort` - The port of the proxy to use for requests.\n* `crawler.domainWhitelist` - An array of domains the crawler is permitted to crawl from. If other settings are more permissive, they will override this setting.\n* `crawler.supportedMimeTypes` - An array of RegEx objects used to determine supported MIME types (types of data simplecrawler will scan for links.) If you're  not using simplecrawler's resource discovery function, this won't have any effect.\n* `crawler.allowedProtocols` - An array of RegEx objects used to determine whether a URL protocol is supported. This is to deal with nonstandard protocol handlers that regular HTTP is sometimes given, like `feed:`. It does not provide support for non-http protocols (and why would it!?)\n* `crawler.maxResourceSize` - The maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n* `crawler.downloadUnsupported` - Simplecrawler will download files it can't parse. Defaults to true, but if you'd rather save the RAM and GC lag, switch it off.\n* `crawler.needsAuth` - Flag to specify if the domain you are hitting requires basic authentication\n* `crawler.authUser` - Username provdied for needsAuth flag\n* `crawler.authPass` - Passowrd provided for needsAuth flag\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed at `crawler.queue` (assuming you called your Crawler() object `crawler`.) It provides array access, so you can get to queue items just with array notation and an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate interface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nYou could always just `.push` a new resource onto the queue, but you'd need to have it all in the correct format, and validate the URL yourself, and oh wouldn't that be a pain. Instead, use the `queue.add` function provided for your convenience:\n\n```javascript\ncrawler.queue.add(protocol,domain,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can remember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items, it helps to know what's inside them. These are the properties every queue item is expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `domain` - The full domain of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The upside is, the queue actually has some convenient functions for getting simple aggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network performance of your crawl (so far.) This is done live, so don't check it thirty times a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the `crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions respectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given status at any one time, and/or retreive them. That's easy with `crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.getwithStatus` returns the number of queued items with a given status, while `crawler.queue.getWithStatus` returns an array of the queue items themselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n* `crawler.queue.complete` - returns the number of queue items which have been completed (marked as fetched)\n* `crawler.queue.errors` - returns the number of requests which have failed (404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if your application fails or you need to abort the crawl for some reason. (Perhaps you just want to finish off for the night and pick it up tomorrow!) The `crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be asynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when you need to save the queue - don't call them every request or your application's performance will be incredibly poor - they block like *crazy*. That said, using them when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n### Licence\n\nYou may copy and use this library as you see fit (including commercial use) and modify it, as long as you retain my attribution comment (which includes my name, link to this github page, and library version) at the top of all script files. You may not, under any circumstances, claim you wrote this library, or remove my attribution. (Fair's fair!)\n\nI'd appreciate it if you'd contribute patches back, but you don't have to. If you do, I'll be happy to credit your contributions!\n\n### Contributors\n\nThanks to [Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support.\nThanks also to [Mike Moulton](https://github.com/mmoulton) for [fixing a bug in the URL discovery mechanism](https://github.com/cgiffard/node-simplecrawler/pull/3),\nas well as [adding the `discoverycomplete` event](https://github.com/cgiffard/node-simplecrawler/pull/10),\nand [Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword naming collision with node 0.8's EventEmitter.\n","_id":"simplecrawler@0.0.10","dist":{"shasum":"d44cd915bddcc8ee1f0920d7a5bc47b82970bee9","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.0.10.tgz"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},{"name":"cgiffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.1.0":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.1.0","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"scripts":{"test":"mocha -R spec"},"bin":{"crawl":"./lib/cli.js"},"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./lib/index.js","engines":{"node":">=0.4.0"},"devDependencies":{"mocha":"~1.8.1","jshint":"~0.7.x","chai":"~1.2.0"},"dependencies":{"iconv":"~1.2.4","URIjs":"~1.8.3"},"readme":"# Simple web-crawler for node.js [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\nSimplecrawler is designed to provide the most basic possible API for crawling\nwebsites, while being as flexible and robust as possible. I wrote simplecrawler\nto archive, analyse, and search some very large websites. It has happily chewed\nthrough 50,000 pages and written tens of gigabytes to disk without issue.\n\n#### Example (simple mode)\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n\nCrawler.crawl(\"http://example.com/\")\n\t.on(\"fetchcomplete\",function(queueItem){\n\t\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n\t});\n```\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can replace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except when discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nThere are two ways of instantiating a new crawler - a simple but less flexible\nmethod inspired by [anemone](http://anemone.rubyforge.org), and the traditional\nmethod which provides a little more room to configure crawl parameters.\n\nRegardless of wether you use the simple or traditional methods of instantiation,\nyou'll need to require simplecrawler:\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n```\n\n#### Simple Mode\n\nSimple mode generates a new crawler for you, preconfigures it based on a URL you\nprovide, and returns the crawler to you for further configuration and so you can\nattach event handlers.\n\nSimply call `Crawler.crawl`, with a URL first parameter, and two optional\nfunctions that will be added as event listeners for `fetchcomplete` and\n`fetcherror` respectively.\n\n```javascript\nCrawler.crawl(\"http://example.com/\", function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\nAlternately, if you decide to omit these functions, you can use the returned\ncrawler object to add the event listeners yourself, and tweak configuration\noptions:\n\n```javascript\nvar crawler = Crawler.crawl(\"http://example.com/\");\n\ncrawler.interval = 500;\n\ncrawler.on(\"fetchcomplete\",function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\n#### Advanced Mode\n\nThe alternative method of creating a crawler is to call the `simplecrawler`\nconstructor yourself, and to initiate the crawl manually.\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your webserver. Decrease the concurrency from five simultaneous requests - and increase the request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when creating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run through its queue finding linked\nresources on the domain to download, until it can't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `crawlstart`\nFired when the crawl begins or is restarted.\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually queue an item yourself.)\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem )\nFired when an item is spooled for fetching.\n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http response object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided as a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size we're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned as a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a request.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as the second parameter.\n* `discoverycomplete` ( queueItem, resources )\nFired when linked resources have been discovered. Passes an array of resources (as URLs) as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does not find any more to add. This event returns no arguments.\n\n####A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters an HTTP error status in the response. If you need this information, you can listen to simplecrawler's error events, and through node's native `data` event (`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let me know. I didn't include it because I didn't need it - but if it's important to people I might put it back in. :)\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n* `crawler.host` - The domain to scan. By default, simplecrawler will restrict all requests to this domain.\n* `crawler.initialPath` - The initial path with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.initialPort` - The initial port with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.initialProtocol` - The initial protocol with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.interval` - The interval with which the crawler will spool up new requests (one per tick.) Defaults to 250ms.\n* `crawler.maxConcurrency` - The maximum number of requests the crawler will run simultaneously. Defaults to 5 - the default number of http agents nodejs will run.\n* `crawler.timeout` - The maximum time the crawler will wait for headers before aborting the request.\n* `crawler.userAgent` - The user agent the crawler will report. Defaults to `Node/SimpleCrawler <version> (http://www.github.com/cgiffard/node-simplecrawler)`.\n* `crawler.queue` - The queue in use by the crawler (Must implement the `FetchQueue` interface)\n* `crawler.filterByDomain` - Specifies whether the crawler will restrict queued requests to a given domain/domains.\n* `crawler.scanSubdomains` - Enables scanning subdomains (other than www) as well as the specified domain. Defaults to false.\n* `crawler.ignoreWWWDomain` - Treats the `www` domain the same as the originally specified domain. Defaults to true.\n* `crawler.stripWWWDomain` - Or go even further and strip WWW subdomain from requests altogether!\n* `crawler.discoverResources` - Use simplecrawler's internal resource discovery function. Defaults to true. (switch it off if you'd prefer to discover and queue resources yourself!)\n* `crawler.cache` - Specify a cache architecture to use when crawling. Must implement `SimpleCache` interface.\n* `crawler.useProxy` - The crawler should use an HTTP proxy to make its requests.\n* `crawler.proxyHostname` - The hostname of the proxy to use for requests.\n* `crawler.proxyPort` - The port of the proxy to use for requests.\n* `crawler.domainWhitelist` - An array of domains the crawler is permitted to crawl from. If other settings are more permissive, they will override this setting.\n* `crawler.supportedMimeTypes` - An array of RegEx objects used to determine supported MIME types (types of data simplecrawler will scan for links.) If you're  not using simplecrawler's resource discovery function, this won't have any effect.\n* `crawler.allowedProtocols` - An array of RegEx objects used to determine whether a URL protocol is supported. This is to deal with nonstandard protocol handlers that regular HTTP is sometimes given, like `feed:`. It does not provide support for non-http protocols (and why would it!?)\n* `crawler.maxResourceSize` - The maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n* `crawler.downloadUnsupported` - Simplecrawler will download files it can't parse. Defaults to true, but if you'd rather save the RAM and GC lag, switch it off.\n* `crawler.needsAuth` - Flag to specify if the domain you are hitting requires basic authentication\n* `crawler.authUser` - Username provdied for needsAuth flag\n* `crawler.authPass` - Passowrd provided for needsAuth flag\n\n#### Excluding certain resources from downloading\n\nSimplecrawler has a mechanism you can use to prevent certain resources from being\nfetched, based on the URL, called *Fetch Conditions**. A fetch condition is just\na function, which, when given a parsed URL object, will return a true or a false\nvalue, indicating whether a given resource should be downloaded.\n\nYou may add as many fetch conditions as you like, and remove them at runtime.\nSimplecrawler will evaluate every single condition against every queued URL, and\nshould just one of them return a falsy value (this includes null and undefined,\nso remember to always return a value!) then the resource in question will not be\nfetched.\n\n##### Adding a fetch condition\n\nThis example fetch condition prevents URLs ending in `.pdf` from downloading.\nAdding a fetch condition assigns it an ID, which the `addFetchCondition` function\nreturns. You can use this ID to remove the condition later.\n\n```javascript\nvar conditionID = myCrawler.addFetchCondition(function(parsedURL) {\n\treturn !parsedURL.path.match(/\\.pdf$/i);\n});\n```\n\n##### Removing a fetch condition\n\nIf you stored the ID of the fetch condition you added earlier, you can remove it\nfrom the crawler:\n\n```javascript\nmyCrawler.removeFetchCondition(conditionID);\n```\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed\nat `crawler.queue` (assuming you called your Crawler() object `crawler`.) It\nprovides array access, so you can get to queue items just with array notation\nand an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate\ninterface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nYou could always just `.push` a new resource onto the queue, but you'd need to\nhave it all in the correct format, and validate the URL yourself, and oh wouldn't\nthat be a pain. Instead, use the `queue.add` function provided for your\nconvenience:\n\n```javascript\ncrawler.queue.add(protocol,domain,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can\nremember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items,\nit helps to know what's inside them. These are the properties every queue item\nis expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `domain` - The full domain of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The\nupside is, the queue actually has some convenient functions for getting simple\naggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network\nperformance of your crawl (so far.) This is done live, so don't check it thirty\ntimes a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the\n`crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions\nrespectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given\nstatus at any one time, and/or retreive them. That's easy with\n`crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.getwithStatus` returns the number of queued items with a given\nstatus, while `crawler.queue.getWithStatus` returns an array of the queue items\nthemselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n* `crawler.queue.complete` - returns the number of queue items which have been completed (marked as fetched)\n* `crawler.queue.errors` - returns the number of requests which have failed (404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if\nyour application fails or you need to abort the crawl for some reason. (Perhaps\nyou just want to finish off for the night and pick it up tomorrow!) The\n`crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be\nasynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when\nyou need to save the queue - don't call them every request or your application's\nperformance will be incredibly poor - they block like *crazy*. That said, using\nthem when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n## Building and Testing\n\n#### Build Status:\n\n* Master: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n* Development: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=development)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\n## Licence\n\nYou may copy and use this library as you see fit (including commercial use) and modify it, as long as you retain my attribution comment (which includes my name, link to this github page, and library version) at the top of all script files. You may not, under any circumstances, claim you wrote this library, or remove my attribution. (Fair's fair!)\n\nI'd appreciate it if you'd contribute patches back, but you don't have to. If you do, I'll be happy to credit your contributions!\n\n## Contributors\n\nThanks to [Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support.\nThanks also to [Mike Moulton](https://github.com/mmoulton) for [fixing a bug in the URL discovery mechanism](https://github.com/cgiffard/node-simplecrawler/pull/3),\nas well as [adding the `discoverycomplete` event](https://github.com/cgiffard/node-simplecrawler/pull/10),\nand [Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword naming collision with node 0.8's EventEmitter.\n","_id":"simplecrawler@0.1.0","dist":{"shasum":"b1a06250327f90d4b81df9b50fc3b07d447c3718","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.1.0.tgz"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},{"name":"cgiffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.1.1":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.1.1","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"scripts":{"test":"mocha -R spec"},"bin":{"crawl":"./lib/cli.js"},"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./lib/index.js","engines":{"node":">=0.4.0"},"devDependencies":{"mocha":"~1.8.1","jshint":"~0.7.x","chai":"~1.2.0"},"dependencies":{"iconv":"~1.2.4","URIjs":"~1.8.3"},"readme":"# Simple web-crawler for node.js [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\nSimplecrawler is designed to provide the most basic possible API for crawling\nwebsites, while being as flexible and robust as possible. I wrote simplecrawler\nto archive, analyse, and search some very large websites. It has happily chewed\nthrough 50,000 pages and written tens of gigabytes to disk without issue.\n\n#### Example (simple mode)\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n\nCrawler.crawl(\"http://example.com/\")\n\t.on(\"fetchcomplete\",function(queueItem){\n\t\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n\t});\n```\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can replace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except when discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nThere are two ways of instantiating a new crawler - a simple but less flexible\nmethod inspired by [anemone](http://anemone.rubyforge.org), and the traditional\nmethod which provides a little more room to configure crawl parameters.\n\nRegardless of wether you use the simple or traditional methods of instantiation,\nyou'll need to require simplecrawler:\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n```\n\n#### Simple Mode\n\nSimple mode generates a new crawler for you, preconfigures it based on a URL you\nprovide, and returns the crawler to you for further configuration and so you can\nattach event handlers.\n\nSimply call `Crawler.crawl`, with a URL first parameter, and two optional\nfunctions that will be added as event listeners for `fetchcomplete` and\n`fetcherror` respectively.\n\n```javascript\nCrawler.crawl(\"http://example.com/\", function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\nAlternately, if you decide to omit these functions, you can use the returned\ncrawler object to add the event listeners yourself, and tweak configuration\noptions:\n\n```javascript\nvar crawler = Crawler.crawl(\"http://example.com/\");\n\ncrawler.interval = 500;\n\ncrawler.on(\"fetchcomplete\",function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\n#### Advanced Mode\n\nThe alternative method of creating a crawler is to call the `simplecrawler`\nconstructor yourself, and to initiate the crawl manually.\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your webserver. Decrease the concurrency from five simultaneous requests - and increase the request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when creating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run through its queue finding linked\nresources on the domain to download, until it can't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `crawlstart`\nFired when the crawl begins or is restarted.\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually queue an item yourself.)\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem )\nFired when an item is spooled for fetching.\n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http response object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided as a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size we're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned as a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a request.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as the second parameter.\n* `discoverycomplete` ( queueItem, resources )\nFired when linked resources have been discovered. Passes an array of resources (as URLs) as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does not find any more to add. This event returns no arguments.\n\n####A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters an HTTP error status in the response. If you need this information, you can listen to simplecrawler's error events, and through node's native `data` event (`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let me know. I didn't include it because I didn't need it - but if it's important to people I might put it back in. :)\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n* `crawler.host` - The domain to scan. By default, simplecrawler will restrict all requests to this domain.\n* `crawler.initialPath` - The initial path with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.initialPort` - The initial port with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.initialProtocol` - The initial protocol with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.interval` - The interval with which the crawler will spool up new requests (one per tick.) Defaults to 250ms.\n* `crawler.maxConcurrency` - The maximum number of requests the crawler will run simultaneously. Defaults to 5 - the default number of http agents nodejs will run.\n* `crawler.timeout` - The maximum time the crawler will wait for headers before aborting the request.\n* `crawler.userAgent` - The user agent the crawler will report. Defaults to `Node/SimpleCrawler <version> (http://www.github.com/cgiffard/node-simplecrawler)`.\n* `crawler.queue` - The queue in use by the crawler (Must implement the `FetchQueue` interface)\n* `crawler.filterByDomain` - Specifies whether the crawler will restrict queued requests to a given domain/domains.\n* `crawler.scanSubdomains` - Enables scanning subdomains (other than www) as well as the specified domain. Defaults to false.\n* `crawler.ignoreWWWDomain` - Treats the `www` domain the same as the originally specified domain. Defaults to true.\n* `crawler.stripWWWDomain` - Or go even further and strip WWW subdomain from requests altogether!\n* `crawler.discoverResources` - Use simplecrawler's internal resource discovery function. Defaults to true. (switch it off if you'd prefer to discover and queue resources yourself!)\n* `crawler.cache` - Specify a cache architecture to use when crawling. Must implement `SimpleCache` interface.\n* `crawler.useProxy` - The crawler should use an HTTP proxy to make its requests.\n* `crawler.proxyHostname` - The hostname of the proxy to use for requests.\n* `crawler.proxyPort` - The port of the proxy to use for requests.\n* `crawler.domainWhitelist` - An array of domains the crawler is permitted to crawl from. If other settings are more permissive, they will override this setting.\n* `crawler.supportedMimeTypes` - An array of RegEx objects used to determine supported MIME types (types of data simplecrawler will scan for links.) If you're  not using simplecrawler's resource discovery function, this won't have any effect.\n* `crawler.allowedProtocols` - An array of RegEx objects used to determine whether a URL protocol is supported. This is to deal with nonstandard protocol handlers that regular HTTP is sometimes given, like `feed:`. It does not provide support for non-http protocols (and why would it!?)\n* `crawler.maxResourceSize` - The maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n* `crawler.downloadUnsupported` - Simplecrawler will download files it can't parse. Defaults to true, but if you'd rather save the RAM and GC lag, switch it off.\n* `crawler.needsAuth` - Flag to specify if the domain you are hitting requires basic authentication\n* `crawler.authUser` - Username provdied for needsAuth flag\n* `crawler.authPass` - Passowrd provided for needsAuth flag\n\n#### Excluding certain resources from downloading\n\nSimplecrawler has a mechanism you can use to prevent certain resources from being\nfetched, based on the URL, called *Fetch Conditions**. A fetch condition is just\na function, which, when given a parsed URL object, will return a true or a false\nvalue, indicating whether a given resource should be downloaded.\n\nYou may add as many fetch conditions as you like, and remove them at runtime.\nSimplecrawler will evaluate every single condition against every queued URL, and\nshould just one of them return a falsy value (this includes null and undefined,\nso remember to always return a value!) then the resource in question will not be\nfetched.\n\n##### Adding a fetch condition\n\nThis example fetch condition prevents URLs ending in `.pdf` from downloading.\nAdding a fetch condition assigns it an ID, which the `addFetchCondition` function\nreturns. You can use this ID to remove the condition later.\n\n```javascript\nvar conditionID = myCrawler.addFetchCondition(function(parsedURL) {\n\treturn !parsedURL.path.match(/\\.pdf$/i);\n});\n```\n\n##### Removing a fetch condition\n\nIf you stored the ID of the fetch condition you added earlier, you can remove it\nfrom the crawler:\n\n```javascript\nmyCrawler.removeFetchCondition(conditionID);\n```\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed\nat `crawler.queue` (assuming you called your Crawler() object `crawler`.) It\nprovides array access, so you can get to queue items just with array notation\nand an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate\ninterface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nYou could always just `.push` a new resource onto the queue, but you'd need to\nhave it all in the correct format, and validate the URL yourself, and oh wouldn't\nthat be a pain. Instead, use the `queue.add` function provided for your\nconvenience:\n\n```javascript\ncrawler.queue.add(protocol,domain,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can\nremember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items,\nit helps to know what's inside them. These are the properties every queue item\nis expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `domain` - The full domain of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The\nupside is, the queue actually has some convenient functions for getting simple\naggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network\nperformance of your crawl (so far.) This is done live, so don't check it thirty\ntimes a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the\n`crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions\nrespectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given\nstatus at any one time, and/or retreive them. That's easy with\n`crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.getwithStatus` returns the number of queued items with a given\nstatus, while `crawler.queue.getWithStatus` returns an array of the queue items\nthemselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n* `crawler.queue.complete` - returns the number of queue items which have been completed (marked as fetched)\n* `crawler.queue.errors` - returns the number of requests which have failed (404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if\nyour application fails or you need to abort the crawl for some reason. (Perhaps\nyou just want to finish off for the night and pick it up tomorrow!) The\n`crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be\nasynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when\nyou need to save the queue - don't call them every request or your application's\nperformance will be incredibly poor - they block like *crazy*. That said, using\nthem when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n## Building and Testing\n\n#### Build Status:\n\n* Master: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n* Development: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=development)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\n## Licence\n\nYou may copy and use this library as you see fit (including commercial use) and modify it, as long as you retain my attribution comment (which includes my name, link to this github page, and library version) at the top of all script files. You may not, under any circumstances, claim you wrote this library, or remove my attribution. (Fair's fair!)\n\nI'd appreciate it if you'd contribute patches back, but you don't have to. If you do, I'll be happy to credit your contributions!\n\n## Contributors\n\nThanks to [Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support.\nThanks also to [Mike Moulton](https://github.com/mmoulton) for [fixing a bug in the URL discovery mechanism](https://github.com/cgiffard/node-simplecrawler/pull/3),\nas well as [adding the `discoverycomplete` event](https://github.com/cgiffard/node-simplecrawler/pull/10),\nand [Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword naming collision with node 0.8's EventEmitter.\n","_id":"simplecrawler@0.1.1","dist":{"shasum":"4737141c32e40835bf3e9e2644b687997c8f4348","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.1.1.tgz"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},{"name":"cgiffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.1.2":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.1.2","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"scripts":{"test":"mocha -R spec"},"bin":{"crawl":"./lib/cli.js"},"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./lib/index.js","engines":{"node":">=0.4.0"},"devDependencies":{"mocha":"~1.8.1","jshint":"~0.7.x","chai":"~1.2.0"},"dependencies":{"iconv":"~1.2.4","URIjs":"~1.8.3"},"readme":"# Simple web-crawler for node.js [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\nSimplecrawler is designed to provide the most basic possible API for crawling\nwebsites, while being as flexible and robust as possible. I wrote simplecrawler\nto archive, analyse, and search some very large websites. It has happily chewed\nthrough 50,000 pages and written tens of gigabytes to disk without issue.\n\n#### Example (simple mode)\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n\nCrawler.crawl(\"http://example.com/\")\n\t.on(\"fetchcomplete\",function(queueItem){\n\t\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n\t});\n```\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can replace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except when discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nThere are two ways of instantiating a new crawler - a simple but less flexible\nmethod inspired by [anemone](http://anemone.rubyforge.org), and the traditional\nmethod which provides a little more room to configure crawl parameters.\n\nRegardless of wether you use the simple or traditional methods of instantiation,\nyou'll need to require simplecrawler:\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n```\n\n#### Simple Mode\n\nSimple mode generates a new crawler for you, preconfigures it based on a URL you\nprovide, and returns the crawler to you for further configuration and so you can\nattach event handlers.\n\nSimply call `Crawler.crawl`, with a URL first parameter, and two optional\nfunctions that will be added as event listeners for `fetchcomplete` and\n`fetcherror` respectively.\n\n```javascript\nCrawler.crawl(\"http://example.com/\", function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\nAlternately, if you decide to omit these functions, you can use the returned\ncrawler object to add the event listeners yourself, and tweak configuration\noptions:\n\n```javascript\nvar crawler = Crawler.crawl(\"http://example.com/\");\n\ncrawler.interval = 500;\n\ncrawler.on(\"fetchcomplete\",function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\n#### Advanced Mode\n\nThe alternative method of creating a crawler is to call the `simplecrawler`\nconstructor yourself, and to initiate the crawl manually.\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your webserver. Decrease the concurrency from five simultaneous requests - and increase the request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when creating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run through its queue finding linked\nresources on the domain to download, until it can't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `crawlstart`\nFired when the crawl begins or is restarted.\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually queue an item yourself.)\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem )\nFired when an item is spooled for fetching.\n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http response object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided as a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size we're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned as a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a request.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as the second parameter.\n* `discoverycomplete` ( queueItem, resources )\nFired when linked resources have been discovered. Passes an array of resources (as URLs) as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does not find any more to add. This event returns no arguments.\n\n####A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters an HTTP error status in the response. If you need this information, you can listen to simplecrawler's error events, and through node's native `data` event (`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let me know. I didn't include it because I didn't need it - but if it's important to people I might put it back in. :)\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n* `crawler.host` - The domain to scan. By default, simplecrawler will restrict all requests to this domain.\n* `crawler.initialPath` - The initial path with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.initialPort` - The initial port with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.initialProtocol` - The initial protocol with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.interval` - The interval with which the crawler will spool up new requests (one per tick.) Defaults to 250ms.\n* `crawler.maxConcurrency` - The maximum number of requests the crawler will run simultaneously. Defaults to 5 - the default number of http agents nodejs will run.\n* `crawler.timeout` - The maximum time the crawler will wait for headers before aborting the request.\n* `crawler.userAgent` - The user agent the crawler will report. Defaults to `Node/SimpleCrawler <version> (http://www.github.com/cgiffard/node-simplecrawler)`.\n* `crawler.queue` - The queue in use by the crawler (Must implement the `FetchQueue` interface)\n* `crawler.filterByDomain` - Specifies whether the crawler will restrict queued requests to a given domain/domains.\n* `crawler.scanSubdomains` - Enables scanning subdomains (other than www) as well as the specified domain. Defaults to false.\n* `crawler.ignoreWWWDomain` - Treats the `www` domain the same as the originally specified domain. Defaults to true.\n* `crawler.stripWWWDomain` - Or go even further and strip WWW subdomain from requests altogether!\n* `crawler.discoverResources` - Use simplecrawler's internal resource discovery function. Defaults to true. (switch it off if you'd prefer to discover and queue resources yourself!)\n* `crawler.cache` - Specify a cache architecture to use when crawling. Must implement `SimpleCache` interface.\n* `crawler.useProxy` - The crawler should use an HTTP proxy to make its requests.\n* `crawler.proxyHostname` - The hostname of the proxy to use for requests.\n* `crawler.proxyPort` - The port of the proxy to use for requests.\n* `crawler.domainWhitelist` - An array of domains the crawler is permitted to crawl from. If other settings are more permissive, they will override this setting.\n* `crawler.supportedMimeTypes` - An array of RegEx objects used to determine supported MIME types (types of data simplecrawler will scan for links.) If you're  not using simplecrawler's resource discovery function, this won't have any effect.\n* `crawler.allowedProtocols` - An array of RegEx objects used to determine whether a URL protocol is supported. This is to deal with nonstandard protocol handlers that regular HTTP is sometimes given, like `feed:`. It does not provide support for non-http protocols (and why would it!?)\n* `crawler.maxResourceSize` - The maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n* `crawler.downloadUnsupported` - Simplecrawler will download files it can't parse. Defaults to true, but if you'd rather save the RAM and GC lag, switch it off.\n* `crawler.needsAuth` - Flag to specify if the domain you are hitting requires basic authentication\n* `crawler.authUser` - Username provdied for needsAuth flag\n* `crawler.authPass` - Passowrd provided for needsAuth flag\n\n#### Excluding certain resources from downloading\n\nSimplecrawler has a mechanism you can use to prevent certain resources from being\nfetched, based on the URL, called *Fetch Conditions**. A fetch condition is just\na function, which, when given a parsed URL object, will return a true or a false\nvalue, indicating whether a given resource should be downloaded.\n\nYou may add as many fetch conditions as you like, and remove them at runtime.\nSimplecrawler will evaluate every single condition against every queued URL, and\nshould just one of them return a falsy value (this includes null and undefined,\nso remember to always return a value!) then the resource in question will not be\nfetched.\n\n##### Adding a fetch condition\n\nThis example fetch condition prevents URLs ending in `.pdf` from downloading.\nAdding a fetch condition assigns it an ID, which the `addFetchCondition` function\nreturns. You can use this ID to remove the condition later.\n\n```javascript\nvar conditionID = myCrawler.addFetchCondition(function(parsedURL) {\n\treturn !parsedURL.path.match(/\\.pdf$/i);\n});\n```\n\n##### Removing a fetch condition\n\nIf you stored the ID of the fetch condition you added earlier, you can remove it\nfrom the crawler:\n\n```javascript\nmyCrawler.removeFetchCondition(conditionID);\n```\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed\nat `crawler.queue` (assuming you called your Crawler() object `crawler`.) It\nprovides array access, so you can get to queue items just with array notation\nand an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate\ninterface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nYou could always just `.push` a new resource onto the queue, but you'd need to\nhave it all in the correct format, and validate the URL yourself, and oh wouldn't\nthat be a pain. Instead, use the `queue.add` function provided for your\nconvenience:\n\n```javascript\ncrawler.queue.add(protocol,domain,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can\nremember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items,\nit helps to know what's inside them. These are the properties every queue item\nis expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `domain` - The full domain of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The\nupside is, the queue actually has some convenient functions for getting simple\naggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network\nperformance of your crawl (so far.) This is done live, so don't check it thirty\ntimes a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the\n`crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions\nrespectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given\nstatus at any one time, and/or retreive them. That's easy with\n`crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.getwithStatus` returns the number of queued items with a given\nstatus, while `crawler.queue.getWithStatus` returns an array of the queue items\nthemselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n* `crawler.queue.complete` - returns the number of queue items which have been completed (marked as fetched)\n* `crawler.queue.errors` - returns the number of requests which have failed (404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if\nyour application fails or you need to abort the crawl for some reason. (Perhaps\nyou just want to finish off for the night and pick it up tomorrow!) The\n`crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be\nasynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when\nyou need to save the queue - don't call them every request or your application's\nperformance will be incredibly poor - they block like *crazy*. That said, using\nthem when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n## Building and Testing\n\n#### Build Status:\n\n* Master: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n* Development: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=development)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\n## Licence\n\nYou may copy and use this library as you see fit (including commercial use) and modify it, as long as you retain my attribution comment (which includes my name, link to this github page, and library version) at the top of all script files. You may not, under any circumstances, claim you wrote this library, or remove my attribution. (Fair's fair!)\n\nI'd appreciate it if you'd contribute patches back, but you don't have to. If you do, I'll be happy to credit your contributions!\n\n## Contributors\n\nThanks to [Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support.\nThanks also to [Mike Moulton](https://github.com/mmoulton) for [fixing a bug in the URL discovery mechanism](https://github.com/cgiffard/node-simplecrawler/pull/3),\nas well as [adding the `discoverycomplete` event](https://github.com/cgiffard/node-simplecrawler/pull/10),\nand [Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword naming collision with node 0.8's EventEmitter.\n","_id":"simplecrawler@0.1.2","dist":{"shasum":"c5624d4c44bff3651913240d3fbbc1a294c61429","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.1.2.tgz"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},{"name":"cgiffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.1.3":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.1.3","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"scripts":{"test":"mocha -R spec"},"bin":{"crawl":"./lib/cli.js"},"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./lib/index.js","engines":{"node":">=0.4.0"},"devDependencies":{"mocha":"~1.8.1","jshint":"~0.7.x","chai":"~1.2.0"},"dependencies":{"iconv":"~1.2.4","URIjs":"~1.8.3"},"readme":"# Simple web-crawler for node.js [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\nSimplecrawler is designed to provide the most basic possible API for crawling\nwebsites, while being as flexible and robust as possible. I wrote simplecrawler\nto archive, analyse, and search some very large websites. It has happily chewed\nthrough 50,000 pages and written tens of gigabytes to disk without issue.\n\n#### Example (simple mode)\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n\nCrawler.crawl(\"http://example.com/\")\n\t.on(\"fetchcomplete\",function(queueItem){\n\t\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n\t});\n```\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can replace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except when discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nThere are two ways of instantiating a new crawler - a simple but less flexible\nmethod inspired by [anemone](http://anemone.rubyforge.org), and the traditional\nmethod which provides a little more room to configure crawl parameters.\n\nRegardless of wether you use the simple or traditional methods of instantiation,\nyou'll need to require simplecrawler:\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n```\n\n#### Simple Mode\n\nSimple mode generates a new crawler for you, preconfigures it based on a URL you\nprovide, and returns the crawler to you for further configuration and so you can\nattach event handlers.\n\nSimply call `Crawler.crawl`, with a URL first parameter, and two optional\nfunctions that will be added as event listeners for `fetchcomplete` and\n`fetcherror` respectively.\n\n```javascript\nCrawler.crawl(\"http://example.com/\", function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\nAlternately, if you decide to omit these functions, you can use the returned\ncrawler object to add the event listeners yourself, and tweak configuration\noptions:\n\n```javascript\nvar crawler = Crawler.crawl(\"http://example.com/\");\n\ncrawler.interval = 500;\n\ncrawler.on(\"fetchcomplete\",function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\n#### Advanced Mode\n\nThe alternative method of creating a crawler is to call the `simplecrawler`\nconstructor yourself, and to initiate the crawl manually.\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your webserver. Decrease the concurrency from five simultaneous requests - and increase the request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when creating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run through its queue finding linked\nresources on the domain to download, until it can't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `crawlstart`\nFired when the crawl begins or is restarted.\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually queue an item yourself.)\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem )\nFired when an item is spooled for fetching.\n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http response object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided as a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size we're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned as a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a request.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as the second parameter.\n* `discoverycomplete` ( queueItem, resources )\nFired when linked resources have been discovered. Passes an array of resources (as URLs) as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does not find any more to add. This event returns no arguments.\n\n####A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters an HTTP error status in the response. If you need this information, you can listen to simplecrawler's error events, and through node's native `data` event (`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let me know. I didn't include it because I didn't need it - but if it's important to people I might put it back in. :)\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n* `crawler.host` - The domain to scan. By default, simplecrawler will restrict all requests to this domain.\n* `crawler.initialPath` - The initial path with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.initialPort` - The initial port with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.initialProtocol` - The initial protocol with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.interval` - The interval with which the crawler will spool up new requests (one per tick.) Defaults to 250ms.\n* `crawler.maxConcurrency` - The maximum number of requests the crawler will run simultaneously. Defaults to 5 - the default number of http agents nodejs will run.\n* `crawler.timeout` - The maximum time the crawler will wait for headers before aborting the request.\n* `crawler.userAgent` - The user agent the crawler will report. Defaults to `Node/SimpleCrawler <version> (http://www.github.com/cgiffard/node-simplecrawler)`.\n* `crawler.queue` - The queue in use by the crawler (Must implement the `FetchQueue` interface)\n* `crawler.filterByDomain` - Specifies whether the crawler will restrict queued requests to a given domain/domains.\n* `crawler.scanSubdomains` - Enables scanning subdomains (other than www) as well as the specified domain. Defaults to false.\n* `crawler.ignoreWWWDomain` - Treats the `www` domain the same as the originally specified domain. Defaults to true.\n* `crawler.stripWWWDomain` - Or go even further and strip WWW subdomain from requests altogether!\n* `crawler.discoverResources` - Use simplecrawler's internal resource discovery function. Defaults to true. (switch it off if you'd prefer to discover and queue resources yourself!)\n* `crawler.cache` - Specify a cache architecture to use when crawling. Must implement `SimpleCache` interface.\n* `crawler.useProxy` - The crawler should use an HTTP proxy to make its requests.\n* `crawler.proxyHostname` - The hostname of the proxy to use for requests.\n* `crawler.proxyPort` - The port of the proxy to use for requests.\n* `crawler.domainWhitelist` - An array of domains the crawler is permitted to crawl from. If other settings are more permissive, they will override this setting.\n* `crawler.supportedMimeTypes` - An array of RegEx objects used to determine supported MIME types (types of data simplecrawler will scan for links.) If you're  not using simplecrawler's resource discovery function, this won't have any effect.\n* `crawler.allowedProtocols` - An array of RegEx objects used to determine whether a URL protocol is supported. This is to deal with nonstandard protocol handlers that regular HTTP is sometimes given, like `feed:`. It does not provide support for non-http protocols (and why would it!?)\n* `crawler.maxResourceSize` - The maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n* `crawler.downloadUnsupported` - Simplecrawler will download files it can't parse. Defaults to true, but if you'd rather save the RAM and GC lag, switch it off.\n* `crawler.needsAuth` - Flag to specify if the domain you are hitting requires basic authentication\n* `crawler.authUser` - Username provdied for needsAuth flag\n* `crawler.authPass` - Passowrd provided for needsAuth flag\n\n#### Excluding certain resources from downloading\n\nSimplecrawler has a mechanism you can use to prevent certain resources from being\nfetched, based on the URL, called *Fetch Conditions**. A fetch condition is just\na function, which, when given a parsed URL object, will return a true or a false\nvalue, indicating whether a given resource should be downloaded.\n\nYou may add as many fetch conditions as you like, and remove them at runtime.\nSimplecrawler will evaluate every single condition against every queued URL, and\nshould just one of them return a falsy value (this includes null and undefined,\nso remember to always return a value!) then the resource in question will not be\nfetched.\n\n##### Adding a fetch condition\n\nThis example fetch condition prevents URLs ending in `.pdf` from downloading.\nAdding a fetch condition assigns it an ID, which the `addFetchCondition` function\nreturns. You can use this ID to remove the condition later.\n\n```javascript\nvar conditionID = myCrawler.addFetchCondition(function(parsedURL) {\n\treturn !parsedURL.path.match(/\\.pdf$/i);\n});\n```\n\n##### Removing a fetch condition\n\nIf you stored the ID of the fetch condition you added earlier, you can remove it\nfrom the crawler:\n\n```javascript\nmyCrawler.removeFetchCondition(conditionID);\n```\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed\nat `crawler.queue` (assuming you called your Crawler() object `crawler`.) It\nprovides array access, so you can get to queue items just with array notation\nand an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate\ninterface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nYou could always just `.push` a new resource onto the queue, but you'd need to\nhave it all in the correct format, and validate the URL yourself, and oh wouldn't\nthat be a pain. Instead, use the `queue.add` function provided for your\nconvenience:\n\n```javascript\ncrawler.queue.add(protocol,domain,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can\nremember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items,\nit helps to know what's inside them. These are the properties every queue item\nis expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `domain` - The full domain of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The\nupside is, the queue actually has some convenient functions for getting simple\naggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network\nperformance of your crawl (so far.) This is done live, so don't check it thirty\ntimes a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the\n`crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions\nrespectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given\nstatus at any one time, and/or retreive them. That's easy with\n`crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.getwithStatus` returns the number of queued items with a given\nstatus, while `crawler.queue.getWithStatus` returns an array of the queue items\nthemselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n* `crawler.queue.complete` - returns the number of queue items which have been completed (marked as fetched)\n* `crawler.queue.errors` - returns the number of requests which have failed (404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if\nyour application fails or you need to abort the crawl for some reason. (Perhaps\nyou just want to finish off for the night and pick it up tomorrow!) The\n`crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be\nasynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when\nyou need to save the queue - don't call them every request or your application's\nperformance will be incredibly poor - they block like *crazy*. That said, using\nthem when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n## Building and Testing\n\n#### Build Status:\n\n* Master: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n* Development: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=development)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\n## Licence\n\nYou may copy and use this library as you see fit (including commercial use) and modify it, as long as you retain my attribution comment (which includes my name, link to this github page, and library version) at the top of all script files. You may not, under any circumstances, claim you wrote this library, or remove my attribution. (Fair's fair!)\n\nI'd appreciate it if you'd contribute patches back, but you don't have to. If you do, I'll be happy to credit your contributions!\n\n## Contributors\n\nThanks to [Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support.\nThanks also to [Mike Moulton](https://github.com/mmoulton) for [fixing a bug in the URL discovery mechanism](https://github.com/cgiffard/node-simplecrawler/pull/3),\nas well as [adding the `discoverycomplete` event](https://github.com/cgiffard/node-simplecrawler/pull/10),\nand [Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword naming collision with node 0.8's EventEmitter.\n","_id":"simplecrawler@0.1.3","dist":{"shasum":"ddd65f90e5c7518d9459398fb804ac517ee1d0fb","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.1.3.tgz"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},{"name":"cgiffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.1.4":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.1.4","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"scripts":{"test":"mocha -R spec"},"bin":{"crawl":"./lib/cli.js"},"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./lib/index.js","engines":{"node":">=0.4.0"},"devDependencies":{"mocha":"~1.8.1","jshint":"~0.7.x","chai":"~1.2.0"},"dependencies":{"iconv":"~1.2.4","URIjs":"~1.8.3"},"readme":"# Simple web-crawler for node.js [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\nSimplecrawler is designed to provide the most basic possible API for crawling\nwebsites, while being as flexible and robust as possible. I wrote simplecrawler\nto archive, analyse, and search some very large websites. It has happily chewed\nthrough 50,000 pages and written tens of gigabytes to disk without issue.\n\n#### Example (simple mode)\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n\nCrawler.crawl(\"http://example.com/\")\n\t.on(\"fetchcomplete\",function(queueItem){\n\t\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n\t});\n```\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can replace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except when discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nThere are two ways of instantiating a new crawler - a simple but less flexible\nmethod inspired by [anemone](http://anemone.rubyforge.org), and the traditional\nmethod which provides a little more room to configure crawl parameters.\n\nRegardless of wether you use the simple or traditional methods of instantiation,\nyou'll need to require simplecrawler:\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n```\n\n#### Simple Mode\n\nSimple mode generates a new crawler for you, preconfigures it based on a URL you\nprovide, and returns the crawler to you for further configuration and so you can\nattach event handlers.\n\nSimply call `Crawler.crawl`, with a URL first parameter, and two optional\nfunctions that will be added as event listeners for `fetchcomplete` and\n`fetcherror` respectively.\n\n```javascript\nCrawler.crawl(\"http://example.com/\", function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\nAlternately, if you decide to omit these functions, you can use the returned\ncrawler object to add the event listeners yourself, and tweak configuration\noptions:\n\n```javascript\nvar crawler = Crawler.crawl(\"http://example.com/\");\n\ncrawler.interval = 500;\n\ncrawler.on(\"fetchcomplete\",function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\n#### Advanced Mode\n\nThe alternative method of creating a crawler is to call the `simplecrawler`\nconstructor yourself, and to initiate the crawl manually.\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your webserver. Decrease the concurrency from five simultaneous requests - and increase the request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when creating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run through its queue finding linked\nresources on the domain to download, until it can't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `crawlstart`\nFired when the crawl begins or is restarted.\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually queue an item yourself.)\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem )\nFired when an item is spooled for fetching.\n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http response object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided as a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size we're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned as a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a request.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as the second parameter.\n* `discoverycomplete` ( queueItem, resources )\nFired when linked resources have been discovered. Passes an array of resources (as URLs) as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does not find any more to add. This event returns no arguments.\n\n####A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters an HTTP error status in the response. If you need this information, you can listen to simplecrawler's error events, and through node's native `data` event (`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let me know. I didn't include it because I didn't need it - but if it's important to people I might put it back in. :)\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n* `crawler.host` - The domain to scan. By default, simplecrawler will restrict all requests to this domain.\n* `crawler.initialPath` - The initial path with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.initialPort` - The initial port with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.initialProtocol` - The initial protocol with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.interval` - The interval with which the crawler will spool up new requests (one per tick.) Defaults to 250ms.\n* `crawler.maxConcurrency` - The maximum number of requests the crawler will run simultaneously. Defaults to 5 - the default number of http agents nodejs will run.\n* `crawler.timeout` - The maximum time the crawler will wait for headers before aborting the request.\n* `crawler.userAgent` - The user agent the crawler will report. Defaults to `Node/SimpleCrawler <version> (http://www.github.com/cgiffard/node-simplecrawler)`.\n* `crawler.queue` - The queue in use by the crawler (Must implement the `FetchQueue` interface)\n* `crawler.filterByDomain` - Specifies whether the crawler will restrict queued requests to a given domain/domains.\n* `crawler.scanSubdomains` - Enables scanning subdomains (other than www) as well as the specified domain. Defaults to false.\n* `crawler.ignoreWWWDomain` - Treats the `www` domain the same as the originally specified domain. Defaults to true.\n* `crawler.stripWWWDomain` - Or go even further and strip WWW subdomain from requests altogether!\n* `crawler.discoverResources` - Use simplecrawler's internal resource discovery function. Defaults to true. (switch it off if you'd prefer to discover and queue resources yourself!)\n* `crawler.cache` - Specify a cache architecture to use when crawling. Must implement `SimpleCache` interface.\n* `crawler.useProxy` - The crawler should use an HTTP proxy to make its requests.\n* `crawler.proxyHostname` - The hostname of the proxy to use for requests.\n* `crawler.proxyPort` - The port of the proxy to use for requests.\n* `crawler.domainWhitelist` - An array of domains the crawler is permitted to crawl from. If other settings are more permissive, they will override this setting.\n* `crawler.supportedMimeTypes` - An array of RegEx objects used to determine supported MIME types (types of data simplecrawler will scan for links.) If you're  not using simplecrawler's resource discovery function, this won't have any effect.\n* `crawler.allowedProtocols` - An array of RegEx objects used to determine whether a URL protocol is supported. This is to deal with nonstandard protocol handlers that regular HTTP is sometimes given, like `feed:`. It does not provide support for non-http protocols (and why would it!?)\n* `crawler.maxResourceSize` - The maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n* `crawler.downloadUnsupported` - Simplecrawler will download files it can't parse. Defaults to true, but if you'd rather save the RAM and GC lag, switch it off.\n* `crawler.needsAuth` - Flag to specify if the domain you are hitting requires basic authentication\n* `crawler.authUser` - Username provdied for needsAuth flag\n* `crawler.authPass` - Passowrd provided for needsAuth flag\n\n#### Excluding certain resources from downloading\n\nSimplecrawler has a mechanism you can use to prevent certain resources from being\nfetched, based on the URL, called *Fetch Conditions**. A fetch condition is just\na function, which, when given a parsed URL object, will return a true or a false\nvalue, indicating whether a given resource should be downloaded.\n\nYou may add as many fetch conditions as you like, and remove them at runtime.\nSimplecrawler will evaluate every single condition against every queued URL, and\nshould just one of them return a falsy value (this includes null and undefined,\nso remember to always return a value!) then the resource in question will not be\nfetched.\n\n##### Adding a fetch condition\n\nThis example fetch condition prevents URLs ending in `.pdf` from downloading.\nAdding a fetch condition assigns it an ID, which the `addFetchCondition` function\nreturns. You can use this ID to remove the condition later.\n\n```javascript\nvar conditionID = myCrawler.addFetchCondition(function(parsedURL) {\n\treturn !parsedURL.path.match(/\\.pdf$/i);\n});\n```\n\n##### Removing a fetch condition\n\nIf you stored the ID of the fetch condition you added earlier, you can remove it\nfrom the crawler:\n\n```javascript\nmyCrawler.removeFetchCondition(conditionID);\n```\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed\nat `crawler.queue` (assuming you called your Crawler() object `crawler`.) It\nprovides array access, so you can get to queue items just with array notation\nand an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate\ninterface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nYou could always just `.push` a new resource onto the queue, but you'd need to\nhave it all in the correct format, and validate the URL yourself, and oh wouldn't\nthat be a pain. Instead, use the `queue.add` function provided for your\nconvenience:\n\n```javascript\ncrawler.queue.add(protocol,domain,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can\nremember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items,\nit helps to know what's inside them. These are the properties every queue item\nis expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `domain` - The full domain of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The\nupside is, the queue actually has some convenient functions for getting simple\naggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network\nperformance of your crawl (so far.) This is done live, so don't check it thirty\ntimes a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the\n`crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions\nrespectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given\nstatus at any one time, and/or retreive them. That's easy with\n`crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.getwithStatus` returns the number of queued items with a given\nstatus, while `crawler.queue.getWithStatus` returns an array of the queue items\nthemselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n* `crawler.queue.complete` - returns the number of queue items which have been completed (marked as fetched)\n* `crawler.queue.errors` - returns the number of requests which have failed (404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if\nyour application fails or you need to abort the crawl for some reason. (Perhaps\nyou just want to finish off for the night and pick it up tomorrow!) The\n`crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be\nasynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when\nyou need to save the queue - don't call them every request or your application's\nperformance will be incredibly poor - they block like *crazy*. That said, using\nthem when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n## Building and Testing\n\n#### Build Status:\n\n* Master: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n* Development: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=development)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\n## Licence\n\nYou may copy and use this library as you see fit (including commercial use) and modify it, as long as you retain my attribution comment (which includes my name, link to this github page, and library version) at the top of all script files. You may not, under any circumstances, claim you wrote this library, or remove my attribution. (Fair's fair!)\n\nI'd appreciate it if you'd contribute patches back, but you don't have to. If you do, I'll be happy to credit your contributions!\n\n## Contributors\n\nThanks to [Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support.\nThanks also to [Mike Moulton](https://github.com/mmoulton) for [fixing a bug in the URL discovery mechanism](https://github.com/cgiffard/node-simplecrawler/pull/3),\nas well as [adding the `discoverycomplete` event](https://github.com/cgiffard/node-simplecrawler/pull/10),\nand [Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword naming collision with node 0.8's EventEmitter.\n","_id":"simplecrawler@0.1.4","dist":{"shasum":"af7eaa7b011d6b77c00c066b129f26833f89f8d6","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.1.4.tgz"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},{"name":"cgiffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.1.5":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.1.5","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"scripts":{"test":"mocha -R spec"},"bin":{"crawl":"./lib/cli.js"},"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./lib/index.js","engines":{"node":">=0.4.0"},"devDependencies":{"mocha":"~1.8.1","jshint":"~0.7.x","chai":"~1.2.0"},"dependencies":{"iconv":"~1.2.4","URIjs":"~1.8.3"},"readme":"# Simple web-crawler for node.js [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\nSimplecrawler is designed to provide the most basic possible API for crawling\nwebsites, while being as flexible and robust as possible. I wrote simplecrawler\nto archive, analyse, and search some very large websites. It has happily chewed\nthrough 50,000 pages and written tens of gigabytes to disk without issue.\n\n#### Example (simple mode)\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n\nCrawler.crawl(\"http://example.com/\")\n\t.on(\"fetchcomplete\",function(queueItem){\n\t\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n\t});\n```\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can replace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except when discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nThere are two ways of instantiating a new crawler - a simple but less flexible\nmethod inspired by [anemone](http://anemone.rubyforge.org), and the traditional\nmethod which provides a little more room to configure crawl parameters.\n\nRegardless of wether you use the simple or traditional methods of instantiation,\nyou'll need to require simplecrawler:\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n```\n\n#### Simple Mode\n\nSimple mode generates a new crawler for you, preconfigures it based on a URL you\nprovide, and returns the crawler to you for further configuration and so you can\nattach event handlers.\n\nSimply call `Crawler.crawl`, with a URL first parameter, and two optional\nfunctions that will be added as event listeners for `fetchcomplete` and\n`fetcherror` respectively.\n\n```javascript\nCrawler.crawl(\"http://example.com/\", function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\nAlternately, if you decide to omit these functions, you can use the returned\ncrawler object to add the event listeners yourself, and tweak configuration\noptions:\n\n```javascript\nvar crawler = Crawler.crawl(\"http://example.com/\");\n\ncrawler.interval = 500;\n\ncrawler.on(\"fetchcomplete\",function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\n#### Advanced Mode\n\nThe alternative method of creating a crawler is to call the `simplecrawler`\nconstructor yourself, and to initiate the crawl manually.\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your webserver. Decrease the concurrency from five simultaneous requests - and increase the request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when creating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run through its queue finding linked\nresources on the domain to download, until it can't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `crawlstart`\nFired when the crawl begins or is restarted.\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually queue an item yourself.)\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem )\nFired when an item is spooled for fetching.\n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http response object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided as a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size we're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned as a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a request.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as the second parameter.\n* `discoverycomplete` ( queueItem, resources )\nFired when linked resources have been discovered. Passes an array of resources (as URLs) as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does not find any more to add. This event returns no arguments.\n\n####A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters an HTTP error status in the response. If you need this information, you can listen to simplecrawler's error events, and through node's native `data` event (`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let me know. I didn't include it because I didn't need it - but if it's important to people I might put it back in. :)\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n* `crawler.host` - The domain to scan. By default, simplecrawler will restrict all requests to this domain.\n* `crawler.initialPath` - The initial path with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.initialPort` - The initial port with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.initialProtocol` - The initial protocol with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.interval` - The interval with which the crawler will spool up new requests (one per tick.) Defaults to 250ms.\n* `crawler.maxConcurrency` - The maximum number of requests the crawler will run simultaneously. Defaults to 5 - the default number of http agents nodejs will run.\n* `crawler.timeout` - The maximum time the crawler will wait for headers before aborting the request.\n* `crawler.userAgent` - The user agent the crawler will report. Defaults to `Node/SimpleCrawler <version> (http://www.github.com/cgiffard/node-simplecrawler)`.\n* `crawler.queue` - The queue in use by the crawler (Must implement the `FetchQueue` interface)\n* `crawler.filterByDomain` - Specifies whether the crawler will restrict queued requests to a given domain/domains.\n* `crawler.scanSubdomains` - Enables scanning subdomains (other than www) as well as the specified domain. Defaults to false.\n* `crawler.ignoreWWWDomain` - Treats the `www` domain the same as the originally specified domain. Defaults to true.\n* `crawler.stripWWWDomain` - Or go even further and strip WWW subdomain from requests altogether!\n* `crawler.discoverResources` - Use simplecrawler's internal resource discovery function. Defaults to true. (switch it off if you'd prefer to discover and queue resources yourself!)\n* `crawler.cache` - Specify a cache architecture to use when crawling. Must implement `SimpleCache` interface.\n* `crawler.useProxy` - The crawler should use an HTTP proxy to make its requests.\n* `crawler.proxyHostname` - The hostname of the proxy to use for requests.\n* `crawler.proxyPort` - The port of the proxy to use for requests.\n* `crawler.domainWhitelist` - An array of domains the crawler is permitted to crawl from. If other settings are more permissive, they will override this setting.\n* `crawler.supportedMimeTypes` - An array of RegEx objects used to determine supported MIME types (types of data simplecrawler will scan for links.) If you're  not using simplecrawler's resource discovery function, this won't have any effect.\n* `crawler.allowedProtocols` - An array of RegEx objects used to determine whether a URL protocol is supported. This is to deal with nonstandard protocol handlers that regular HTTP is sometimes given, like `feed:`. It does not provide support for non-http protocols (and why would it!?)\n* `crawler.maxResourceSize` - The maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n* `crawler.downloadUnsupported` - Simplecrawler will download files it can't parse. Defaults to true, but if you'd rather save the RAM and GC lag, switch it off.\n* `crawler.needsAuth` - Flag to specify if the domain you are hitting requires basic authentication\n* `crawler.authUser` - Username provdied for needsAuth flag\n* `crawler.authPass` - Passowrd provided for needsAuth flag\n\n#### Excluding certain resources from downloading\n\nSimplecrawler has a mechanism you can use to prevent certain resources from being\nfetched, based on the URL, called *Fetch Conditions**. A fetch condition is just\na function, which, when given a parsed URL object, will return a true or a false\nvalue, indicating whether a given resource should be downloaded.\n\nYou may add as many fetch conditions as you like, and remove them at runtime.\nSimplecrawler will evaluate every single condition against every queued URL, and\nshould just one of them return a falsy value (this includes null and undefined,\nso remember to always return a value!) then the resource in question will not be\nfetched.\n\n##### Adding a fetch condition\n\nThis example fetch condition prevents URLs ending in `.pdf` from downloading.\nAdding a fetch condition assigns it an ID, which the `addFetchCondition` function\nreturns. You can use this ID to remove the condition later.\n\n```javascript\nvar conditionID = myCrawler.addFetchCondition(function(parsedURL) {\n\treturn !parsedURL.path.match(/\\.pdf$/i);\n});\n```\n\n##### Removing a fetch condition\n\nIf you stored the ID of the fetch condition you added earlier, you can remove it\nfrom the crawler:\n\n```javascript\nmyCrawler.removeFetchCondition(conditionID);\n```\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed\nat `crawler.queue` (assuming you called your Crawler() object `crawler`.) It\nprovides array access, so you can get to queue items just with array notation\nand an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate\ninterface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nYou could always just `.push` a new resource onto the queue, but you'd need to\nhave it all in the correct format, and validate the URL yourself, and oh wouldn't\nthat be a pain. Instead, use the `queue.add` function provided for your\nconvenience:\n\n```javascript\ncrawler.queue.add(protocol,domain,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can\nremember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items,\nit helps to know what's inside them. These are the properties every queue item\nis expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `domain` - The full domain of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The\nupside is, the queue actually has some convenient functions for getting simple\naggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network\nperformance of your crawl (so far.) This is done live, so don't check it thirty\ntimes a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the\n`crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions\nrespectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given\nstatus at any one time, and/or retreive them. That's easy with\n`crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.getwithStatus` returns the number of queued items with a given\nstatus, while `crawler.queue.getWithStatus` returns an array of the queue items\nthemselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n* `crawler.queue.complete` - returns the number of queue items which have been completed (marked as fetched)\n* `crawler.queue.errors` - returns the number of requests which have failed (404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if\nyour application fails or you need to abort the crawl for some reason. (Perhaps\nyou just want to finish off for the night and pick it up tomorrow!) The\n`crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be\nasynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when\nyou need to save the queue - don't call them every request or your application's\nperformance will be incredibly poor - they block like *crazy*. That said, using\nthem when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n## Building and Testing\n\n#### Build Status:\n\n* Master: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n* Development: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=development)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\n## Licence\n\nYou may copy and use this library as you see fit (including commercial use) and modify it, as long as you retain my attribution comment (which includes my name, link to this github page, and library version) at the top of all script files. You may not, under any circumstances, claim you wrote this library, or remove my attribution. (Fair's fair!)\n\nI'd appreciate it if you'd contribute patches back, but you don't have to. If you do, I'll be happy to credit your contributions!\n\n## Contributors\n\nThanks to [Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support.\nThanks also to [Mike Moulton](https://github.com/mmoulton) for [fixing a bug in the URL discovery mechanism](https://github.com/cgiffard/node-simplecrawler/pull/3),\nas well as [adding the `discoverycomplete` event](https://github.com/cgiffard/node-simplecrawler/pull/10),\nand [Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword naming collision with node 0.8's EventEmitter.\n","_id":"simplecrawler@0.1.5","dist":{"shasum":"0aea87aaa3fcac5d342ed57f3611abbf222f0d65","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.1.5.tgz"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},{"name":"cgiffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.1.6":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.1.6","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"scripts":{"test":"mocha -R spec"},"bin":{"crawl":"./lib/cli.js"},"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./lib/index.js","engines":{"node":">=0.4.0"},"devDependencies":{"mocha":"~1.8.1","jshint":"~0.7.x","chai":"~1.2.0"},"dependencies":{"iconv":"~1.2.4","URIjs":"~1.8.3"},"readme":"# Simple web-crawler for node.js [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\nSimplecrawler is designed to provide the most basic possible API for crawling\nwebsites, while being as flexible and robust as possible. I wrote simplecrawler\nto archive, analyse, and search some very large websites. It has happily chewed\nthrough 50,000 pages and written tens of gigabytes to disk without issue.\n\n#### Example (simple mode)\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n\nCrawler.crawl(\"http://example.com/\")\n\t.on(\"fetchcomplete\",function(queueItem){\n\t\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n\t});\n```\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can replace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except when discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nThere are two ways of instantiating a new crawler - a simple but less flexible\nmethod inspired by [anemone](http://anemone.rubyforge.org), and the traditional\nmethod which provides a little more room to configure crawl parameters.\n\nRegardless of wether you use the simple or traditional methods of instantiation,\nyou'll need to require simplecrawler:\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n```\n\n#### Simple Mode\n\nSimple mode generates a new crawler for you, preconfigures it based on a URL you\nprovide, and returns the crawler to you for further configuration and so you can\nattach event handlers.\n\nSimply call `Crawler.crawl`, with a URL first parameter, and two optional\nfunctions that will be added as event listeners for `fetchcomplete` and\n`fetcherror` respectively.\n\n```javascript\nCrawler.crawl(\"http://example.com/\", function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\nAlternately, if you decide to omit these functions, you can use the returned\ncrawler object to add the event listeners yourself, and tweak configuration\noptions:\n\n```javascript\nvar crawler = Crawler.crawl(\"http://example.com/\");\n\ncrawler.interval = 500;\n\ncrawler.on(\"fetchcomplete\",function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\n#### Advanced Mode\n\nThe alternative method of creating a crawler is to call the `simplecrawler`\nconstructor yourself, and to initiate the crawl manually.\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your webserver. Decrease the concurrency from five simultaneous requests - and increase the request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when creating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run through its queue finding linked\nresources on the domain to download, until it can't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `crawlstart`\nFired when the crawl begins or is restarted.\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually queue an item yourself.)\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem )\nFired when an item is spooled for fetching.\n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http response object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided as a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size we're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned as a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a request.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as the second parameter.\n* `discoverycomplete` ( queueItem, resources )\nFired when linked resources have been discovered. Passes an array of resources (as URLs) as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does not find any more to add. This event returns no arguments.\n\n####A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters an HTTP error status in the response. If you need this information, you can listen to simplecrawler's error events, and through node's native `data` event (`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let me know. I didn't include it because I didn't need it - but if it's important to people I might put it back in. :)\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n* `crawler.host` - The domain to scan. By default, simplecrawler will restrict all requests to this domain.\n* `crawler.initialPath` - The initial path with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.initialPort` - The initial port with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.initialProtocol` - The initial protocol with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.interval` - The interval with which the crawler will spool up new requests (one per tick.) Defaults to 250ms.\n* `crawler.maxConcurrency` - The maximum number of requests the crawler will run simultaneously. Defaults to 5 - the default number of http agents nodejs will run.\n* `crawler.timeout` - The maximum time the crawler will wait for headers before aborting the request.\n* `crawler.userAgent` - The user agent the crawler will report. Defaults to `Node/SimpleCrawler <version> (http://www.github.com/cgiffard/node-simplecrawler)`.\n* `crawler.queue` - The queue in use by the crawler (Must implement the `FetchQueue` interface)\n* `crawler.filterByDomain` - Specifies whether the crawler will restrict queued requests to a given domain/domains.\n* `crawler.scanSubdomains` - Enables scanning subdomains (other than www) as well as the specified domain. Defaults to false.\n* `crawler.ignoreWWWDomain` - Treats the `www` domain the same as the originally specified domain. Defaults to true.\n* `crawler.stripWWWDomain` - Or go even further and strip WWW subdomain from requests altogether!\n* `crawler.discoverResources` - Use simplecrawler's internal resource discovery function. Defaults to true. (switch it off if you'd prefer to discover and queue resources yourself!)\n* `crawler.cache` - Specify a cache architecture to use when crawling. Must implement `SimpleCache` interface.\n* `crawler.useProxy` - The crawler should use an HTTP proxy to make its requests.\n* `crawler.proxyHostname` - The hostname of the proxy to use for requests.\n* `crawler.proxyPort` - The port of the proxy to use for requests.\n* `crawler.domainWhitelist` - An array of domains the crawler is permitted to crawl from. If other settings are more permissive, they will override this setting.\n* `crawler.supportedMimeTypes` - An array of RegEx objects used to determine supported MIME types (types of data simplecrawler will scan for links.) If you're  not using simplecrawler's resource discovery function, this won't have any effect.\n* `crawler.allowedProtocols` - An array of RegEx objects used to determine whether a URL protocol is supported. This is to deal with nonstandard protocol handlers that regular HTTP is sometimes given, like `feed:`. It does not provide support for non-http protocols (and why would it!?)\n* `crawler.maxResourceSize` - The maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n* `crawler.downloadUnsupported` - Simplecrawler will download files it can't parse. Defaults to true, but if you'd rather save the RAM and GC lag, switch it off.\n* `crawler.needsAuth` - Flag to specify if the domain you are hitting requires basic authentication\n* `crawler.authUser` - Username provdied for needsAuth flag\n* `crawler.authPass` - Passowrd provided for needsAuth flag\n\n#### Excluding certain resources from downloading\n\nSimplecrawler has a mechanism you can use to prevent certain resources from being\nfetched, based on the URL, called *Fetch Conditions**. A fetch condition is just\na function, which, when given a parsed URL object, will return a true or a false\nvalue, indicating whether a given resource should be downloaded.\n\nYou may add as many fetch conditions as you like, and remove them at runtime.\nSimplecrawler will evaluate every single condition against every queued URL, and\nshould just one of them return a falsy value (this includes null and undefined,\nso remember to always return a value!) then the resource in question will not be\nfetched.\n\n##### Adding a fetch condition\n\nThis example fetch condition prevents URLs ending in `.pdf` from downloading.\nAdding a fetch condition assigns it an ID, which the `addFetchCondition` function\nreturns. You can use this ID to remove the condition later.\n\n```javascript\nvar conditionID = myCrawler.addFetchCondition(function(parsedURL) {\n\treturn !parsedURL.path.match(/\\.pdf$/i);\n});\n```\n\n##### Removing a fetch condition\n\nIf you stored the ID of the fetch condition you added earlier, you can remove it\nfrom the crawler:\n\n```javascript\nmyCrawler.removeFetchCondition(conditionID);\n```\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed\nat `crawler.queue` (assuming you called your Crawler() object `crawler`.) It\nprovides array access, so you can get to queue items just with array notation\nand an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate\ninterface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nYou could always just `.push` a new resource onto the queue, but you'd need to\nhave it all in the correct format, and validate the URL yourself, and oh wouldn't\nthat be a pain. Instead, use the `queue.add` function provided for your\nconvenience:\n\n```javascript\ncrawler.queue.add(protocol,domain,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can\nremember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items,\nit helps to know what's inside them. These are the properties every queue item\nis expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `domain` - The full domain of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The\nupside is, the queue actually has some convenient functions for getting simple\naggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network\nperformance of your crawl (so far.) This is done live, so don't check it thirty\ntimes a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the\n`crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions\nrespectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given\nstatus at any one time, and/or retreive them. That's easy with\n`crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.getwithStatus` returns the number of queued items with a given\nstatus, while `crawler.queue.getWithStatus` returns an array of the queue items\nthemselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n* `crawler.queue.complete` - returns the number of queue items which have been completed (marked as fetched)\n* `crawler.queue.errors` - returns the number of requests which have failed (404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if\nyour application fails or you need to abort the crawl for some reason. (Perhaps\nyou just want to finish off for the night and pick it up tomorrow!) The\n`crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be\nasynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when\nyou need to save the queue - don't call them every request or your application's\nperformance will be incredibly poor - they block like *crazy*. That said, using\nthem when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n## Building and Testing\n\n#### Build Status:\n\n* Master: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n* Development: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=development)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\n## Licence\n\nYou may copy and use this library as you see fit (including commercial use) and modify it, as long as you retain my attribution comment (which includes my name, link to this github page, and library version) at the top of all script files. You may not, under any circumstances, claim you wrote this library, or remove my attribution. (Fair's fair!)\n\nI'd appreciate it if you'd contribute patches back, but you don't have to. If you do, I'll be happy to credit your contributions!\n\n## Contributors\n\nThanks to [Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support.\nThanks also to [Mike Moulton](https://github.com/mmoulton) for [fixing a bug in the URL discovery mechanism](https://github.com/cgiffard/node-simplecrawler/pull/3),\nas well as [adding the `discoverycomplete` event](https://github.com/cgiffard/node-simplecrawler/pull/10),\nand [Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword naming collision with node 0.8's EventEmitter.\n","_id":"simplecrawler@0.1.6","dist":{"shasum":"45fd33d4cab7fca1c4491662621e14b113317ee2","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.1.6.tgz"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},{"name":"cgiffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.1.7":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.1.7","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"scripts":{"test":"mocha -R spec"},"bin":{"crawl":"./lib/cli.js"},"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./lib/index.js","engines":{"node":">=0.4.0"},"devDependencies":{"mocha":"~1.8.1","jshint":"~0.7.x","chai":"~1.2.0"},"dependencies":{"iconv":"~1.2.4","URIjs":"~1.8.3"},"readme":"# Simple web-crawler for node.js [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\nSimplecrawler is designed to provide the most basic possible API for crawling\nwebsites, while being as flexible and robust as possible. I wrote simplecrawler\nto archive, analyse, and search some very large websites. It has happily chewed\nthrough 50,000 pages and written tens of gigabytes to disk without issue.\n\n#### Example (simple mode)\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n\nCrawler.crawl(\"http://example.com/\")\n\t.on(\"fetchcomplete\",function(queueItem){\n\t\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n\t});\n```\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can replace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except when discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nThere are two ways of instantiating a new crawler - a simple but less flexible\nmethod inspired by [anemone](http://anemone.rubyforge.org), and the traditional\nmethod which provides a little more room to configure crawl parameters.\n\nRegardless of wether you use the simple or traditional methods of instantiation,\nyou'll need to require simplecrawler:\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n```\n\n#### Simple Mode\n\nSimple mode generates a new crawler for you, preconfigures it based on a URL you\nprovide, and returns the crawler to you for further configuration and so you can\nattach event handlers.\n\nSimply call `Crawler.crawl`, with a URL first parameter, and two optional\nfunctions that will be added as event listeners for `fetchcomplete` and\n`fetcherror` respectively.\n\n```javascript\nCrawler.crawl(\"http://example.com/\", function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\nAlternately, if you decide to omit these functions, you can use the returned\ncrawler object to add the event listeners yourself, and tweak configuration\noptions:\n\n```javascript\nvar crawler = Crawler.crawl(\"http://example.com/\");\n\ncrawler.interval = 500;\n\ncrawler.on(\"fetchcomplete\",function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\n#### Advanced Mode\n\nThe alternative method of creating a crawler is to call the `simplecrawler`\nconstructor yourself, and to initiate the crawl manually.\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your webserver. Decrease the concurrency from five simultaneous requests - and increase the request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when creating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run through its queue finding linked\nresources on the domain to download, until it can't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `crawlstart`\nFired when the crawl begins or is restarted.\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually queue an item yourself.)\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem )\nFired when an item is spooled for fetching.\n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http response object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided as a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size we're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned as a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a request.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as the second parameter.\n* `discoverycomplete` ( queueItem, resources )\nFired when linked resources have been discovered. Passes an array of resources (as URLs) as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does not find any more to add. This event returns no arguments.\n\n####A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters an HTTP error status in the response. If you need this information, you can listen to simplecrawler's error events, and through node's native `data` event (`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let me know. I didn't include it because I didn't need it - but if it's important to people I might put it back in. :)\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n* `crawler.host` - The domain to scan. By default, simplecrawler will restrict all requests to this domain.\n* `crawler.initialPath` - The initial path with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.initialPort` - The initial port with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.initialProtocol` - The initial protocol with which the crawler will formulate its first request. Does not restrict subsequent requests.\n* `crawler.interval` - The interval with which the crawler will spool up new requests (one per tick.) Defaults to 250ms.\n* `crawler.maxConcurrency` - The maximum number of requests the crawler will run simultaneously. Defaults to 5 - the default number of http agents nodejs will run.\n* `crawler.timeout` - The maximum time the crawler will wait for headers before aborting the request.\n* `crawler.userAgent` - The user agent the crawler will report. Defaults to `Node/SimpleCrawler <version> (http://www.github.com/cgiffard/node-simplecrawler)`.\n* `crawler.queue` - The queue in use by the crawler (Must implement the `FetchQueue` interface)\n* `crawler.filterByDomain` - Specifies whether the crawler will restrict queued requests to a given domain/domains.\n* `crawler.scanSubdomains` - Enables scanning subdomains (other than www) as well as the specified domain. Defaults to false.\n* `crawler.ignoreWWWDomain` - Treats the `www` domain the same as the originally specified domain. Defaults to true.\n* `crawler.stripWWWDomain` - Or go even further and strip WWW subdomain from requests altogether!\n* `crawler.discoverResources` - Use simplecrawler's internal resource discovery function. Defaults to true. (switch it off if you'd prefer to discover and queue resources yourself!)\n* `crawler.cache` - Specify a cache architecture to use when crawling. Must implement `SimpleCache` interface.\n* `crawler.useProxy` - The crawler should use an HTTP proxy to make its requests.\n* `crawler.proxyHostname` - The hostname of the proxy to use for requests.\n* `crawler.proxyPort` - The port of the proxy to use for requests.\n* `crawler.domainWhitelist` - An array of domains the crawler is permitted to crawl from. If other settings are more permissive, they will override this setting.\n* `crawler.supportedMimeTypes` - An array of RegEx objects used to determine supported MIME types (types of data simplecrawler will scan for links.) If you're  not using simplecrawler's resource discovery function, this won't have any effect.\n* `crawler.allowedProtocols` - An array of RegEx objects used to determine whether a URL protocol is supported. This is to deal with nonstandard protocol handlers that regular HTTP is sometimes given, like `feed:`. It does not provide support for non-http protocols (and why would it!?)\n* `crawler.maxResourceSize` - The maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n* `crawler.downloadUnsupported` - Simplecrawler will download files it can't parse. Defaults to true, but if you'd rather save the RAM and GC lag, switch it off.\n* `crawler.needsAuth` - Flag to specify if the domain you are hitting requires basic authentication\n* `crawler.authUser` - Username provdied for needsAuth flag\n* `crawler.authPass` - Passowrd provided for needsAuth flag\n\n#### Excluding certain resources from downloading\n\nSimplecrawler has a mechanism you can use to prevent certain resources from being\nfetched, based on the URL, called *Fetch Conditions**. A fetch condition is just\na function, which, when given a parsed URL object, will return a true or a false\nvalue, indicating whether a given resource should be downloaded.\n\nYou may add as many fetch conditions as you like, and remove them at runtime.\nSimplecrawler will evaluate every single condition against every queued URL, and\nshould just one of them return a falsy value (this includes null and undefined,\nso remember to always return a value!) then the resource in question will not be\nfetched.\n\n##### Adding a fetch condition\n\nThis example fetch condition prevents URLs ending in `.pdf` from downloading.\nAdding a fetch condition assigns it an ID, which the `addFetchCondition` function\nreturns. You can use this ID to remove the condition later.\n\n```javascript\nvar conditionID = myCrawler.addFetchCondition(function(parsedURL) {\n\treturn !parsedURL.path.match(/\\.pdf$/i);\n});\n```\nNOTE: simplecrawler uses slightly different terminology to URIjs. `parsedURL.path` includes the query string too. If you want the path without the query string, use `parsedURL.uriPath`.\n\n##### Removing a fetch condition\n\nIf you stored the ID of the fetch condition you added earlier, you can remove it\nfrom the crawler:\n\n```javascript\nmyCrawler.removeFetchCondition(conditionID);\n```\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed\nat `crawler.queue` (assuming you called your Crawler() object `crawler`.) It\nprovides array access, so you can get to queue items just with array notation\nand an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate\ninterface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nYou could always just `.push` a new resource onto the queue, but you'd need to\nhave it all in the correct format, and validate the URL yourself, and oh wouldn't\nthat be a pain. Instead, use the `queue.add` function provided for your\nconvenience:\n\n```javascript\ncrawler.queue.add(protocol,hostname,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can\nremember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items,\nit helps to know what's inside them. These are the properties every queue item\nis expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `host` - The full domain/hostname of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The\nupside is, the queue actually has some convenient functions for getting simple\naggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network\nperformance of your crawl (so far.) This is done live, so don't check it thirty\ntimes a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the\n`crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions\nrespectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given\nstatus at any one time, and/or retreive them. That's easy with\n`crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.getwithStatus` returns the number of queued items with a given\nstatus, while `crawler.queue.getWithStatus` returns an array of the queue items\nthemselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n* `crawler.queue.complete` - returns the number of queue items which have been completed (marked as fetched)\n* `crawler.queue.errors` - returns the number of requests which have failed (404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if\nyour application fails or you need to abort the crawl for some reason. (Perhaps\nyou just want to finish off for the night and pick it up tomorrow!) The\n`crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be\nasynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when\nyou need to save the queue - don't call them every request or your application's\nperformance will be incredibly poor - they block like *crazy*. That said, using\nthem when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n## Building and Testing\n\n#### Build Status:\n\n* Master: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n* Development: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=development)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\n## Licence\n\nYou may copy and use this library as you see fit (including commercial use) and modify it, as long as you retain my attribution comment (which includes my name, link to this github page, and library version) at the top of all script files. You may not, under any circumstances, claim you wrote this library, or remove my attribution. (Fair's fair!)\n\nI'd appreciate it if you'd contribute patches back, but you don't have to. If you do, I'll be happy to credit your contributions!\n\n## Contributors\n\nI'd like to extend sincere thanks to:\n\n* [Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support.\n* [Mike Moulton](https://github.com/mmoulton) for [fixing a bug in the URL discovery mechanism](https://github.com/cgiffard/node-simplecrawler/pull/3), as well as [adding the `discoverycomplete` event](https://github.com/cgiffard/node-simplecrawler/pull/10),\n* [Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword naming collision with node 0.8's EventEmitter.\n* [Greg Molnar](https://github.com/gregmolnar) for [adding a querystring-free path parameter to parsed URL objects.](https://github.com/cgiffard/node-simplecrawler/pull/31)\n\nAnd everybody else who has helped out in some way! :)\n","_id":"simplecrawler@0.1.7","dist":{"shasum":"f3677b78e4b13565501b472be823ca9eaebd1d0e","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.1.7.tgz"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},{"name":"cgiffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.2.0":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.2.0","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"scripts":{"test":"mocha -R spec"},"bin":{"crawl":"./lib/cli.js"},"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./lib/index.js","engines":{"node":">=0.4.0"},"devDependencies":{"mocha":"~1.8.1","jshint":"~0.7.x","chai":"~1.2.0"},"dependencies":{"iconv":"~1.2.4","URIjs":"~1.8.3"},"readme":"# Simple web-crawler for node.js [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\nSimplecrawler is designed to provide the most basic possible API for crawling\nwebsites, while being as flexible and robust as possible. I wrote simplecrawler\nto archive, analyse, and search some very large websites. It has happily chewed\nthrough 50,000 pages and written tens of gigabytes to disk without issue.\n\n#### Example (simple mode)\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n\nCrawler.crawl(\"http://example.com/\")\n\t.on(\"fetchcomplete\",function(queueItem){\n\t\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n\t});\n```\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can\nreplace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except\nwhen discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nThere are two ways of instantiating a new crawler - a simple but less flexible\nmethod inspired by [anemone](http://anemone.rubyforge.org), and the traditional\nmethod which provides a little more room to configure crawl parameters.\n\nRegardless of wether you use the simple or traditional methods of instantiation,\nyou'll need to require simplecrawler:\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n```\n\n#### Simple Mode\n\nSimple mode generates a new crawler for you, preconfigures it based on a URL you\nprovide, and returns the crawler to you for further configuration and so you can\nattach event handlers.\n\nSimply call `Crawler.crawl`, with a URL first parameter, and two optional\nfunctions that will be added as event listeners for `fetchcomplete` and\n`fetcherror` respectively.\n\n```javascript\nCrawler.crawl(\"http://example.com/\", function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\nAlternately, if you decide to omit these functions, you can use the returned\ncrawler object to add the event listeners yourself, and tweak configuration\noptions:\n\n```javascript\nvar crawler = Crawler.crawl(\"http://example.com/\");\n\ncrawler.interval = 500;\n\ncrawler.on(\"fetchcomplete\",function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\n#### Advanced Mode\n\nThe alternative method of creating a crawler is to call the `simplecrawler`\nconstructor yourself, and to initiate the crawl manually.\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your web\nserver. Decrease the concurrency from five simultaneous requests - and increase\nthe request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when\ncreating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run\nthrough its queue finding linked resources on the domain to download, until it\ncan't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `crawlstart`\nFired when the crawl begins or is restarted.\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually\nqueue an item yourself.)\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem , requestOptions )\nFired when an item is spooled for fetching. If your event handler is synchronous,\nyou can modify the crawler request options (including headers) \n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http\nresponse object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided\nas a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size\nwe're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned\nas a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a\nrequest.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as\nthe second parameter.\n* `discoverycomplete` ( queueItem, resources )\nFired when linked resources have been discovered. Passes an array of resources\n(as URLs) as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does\nnot find any more to add. This event returns no arguments.\n\n#### A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters\nan HTTP error status in the response. If you need this information, you can listen\nto simplecrawler's error events, and through node's native `data` event\n(`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let\nme know. I didn't include it because I didn't need it - but if it's important to\npeople I might put it back in. :)\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n*\t`crawler.host` -\n\tThe domain to scan. By default, simplecrawler will restrict all requests to\n\tthis domain.\n*\t`crawler.initialPath` -\n\tThe initial path with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialPort` -\n\tThe initial port with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialProtocol` -\n\tThe initial protocol with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.interval` -\n\tThe interval with which the crawler will spool up new requests (one per\n\ttick.) Defaults to 250ms.\n*\t`crawler.maxConcurrency` -\n\tThe maximum number of requests the crawler will run simultaneously. Defaults\n\tto 5 - the default number of http agents node will run.\n*\t`crawler.timeout` -\n\tThe maximum time the crawler will wait for headers before aborting the request.\n*\t`crawler.userAgent` -\n\tThe user agent the crawler will report. Defaults to\n\t`Node/SimpleCrawler <version> (http://www.github.com/cgiffard/node-simplecrawler)`.\n*\t`crawler.queue` -\n\tThe queue in use by the crawler (Must implement the `FetchQueue` interface)\n*\t`crawler.filterByDomain` -\n\tSpecifies whether the crawler will restrict queued requests to a given\n\tdomain/domains.\n*\t`crawler.scanSubdomains` -\n\tEnables scanning subdomains (other than www) as well as the specified domain.\n\tDefaults to false.\n*\t`crawler.ignoreWWWDomain` -\n\tTreats the `www` domain the same as the originally specified domain.\n\tDefaults to true.\n*\t`crawler.stripWWWDomain` -\n\tOr go even further and strip WWW subdomain from requests altogether!\n*\t`crawler.discoverResources` -\n\tUse simplecrawler's internal resource discovery function. Defaults to true.\n\t(switch it off if you'd prefer to discover and queue resources yourself!)\n*\t`crawler.cache` -\n\tSpecify a cache architecture to use when crawling. Must implement\n\t`SimpleCache` interface.\n*\t`crawler.useProxy` -\n\tThe crawler should use an HTTP proxy to make its requests.\n*\t`crawler.proxyHostname` -\n\tThe hostname of the proxy to use for requests.\n*\t`crawler.proxyPort` -\n\tThe port of the proxy to use for requests.\n*\t`crawler.domainWhitelist` -\n\tAn array of domains the crawler is permitted to crawl from. If other settings\n\tare more permissive, they will override this setting.\n*\t`crawler.supportedMimeTypes` -\n\tAn array of RegEx objects used to determine supported MIME types (types of\n\tdata simplecrawler will scan for links.) If you're  not using simplecrawler's\n\tresource discovery function, this won't have any effect.\n*\t`crawler.allowedProtocols` -\n\tAn array of RegEx objects used to determine whether a URL protocol is supported.\n\tThis is to deal with nonstandard protocol handlers that regular HTTP is\n\tsometimes given, like `feed:`. It does not provide support for non-http\n\tprotocols (and why would it!?)\n*\t`crawler.maxResourceSize` - \n\tThe maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n*\t`crawler.downloadUnsupported` -\n\tSimplecrawler will download files it can't parse. Defaults to true, but if\n\tyou'd rather save the RAM and GC lag, switch it off.\n*\t`crawler.needsAuth` -\n\tFlag to specify if the domain you are hitting requires basic authentication\n*\t`crawler.authUser` -\n\tUsername provided for needsAuth flag\n*\t`crawler.authPass` -\n\tPassword provided for needsAuth flag\n*\t`crawler.customHeaders` -\n\tAn object specifying a number of custom headers simplecrawler will add to\n\tevery request. These override the default headers simplecrawler sets, so\n\tbe careful with them. If you want to tamper with headers on a per-request basis,\n\tsee the `fetchqueue` event.\n*\t`crawler.acceptCookies` -\n\tFlag to indicate if the crawler should hold on to cookies\n*\t`crawler.urlEncoding` -\n\tSet this to `iso8859` to trigger URIjs' re-encoding of iso8859 URLs to unicode.\n\tDefaults to `unicode`.\n\n#### Excluding certain resources from downloading\n\nSimplecrawler has a mechanism you can use to prevent certain resources from being\nfetched, based on the URL, called *Fetch Conditions**. A fetch condition is just\na function, which, when given a parsed URL object, will return a true or a false\nvalue, indicating whether a given resource should be downloaded.\n\nYou may add as many fetch conditions as you like, and remove them at runtime.\nSimplecrawler will evaluate every single condition against every queued URL, and\nshould just one of them return a falsy value (this includes null and undefined,\nso remember to always return a value!) then the resource in question will not be\nfetched.\n\n##### Adding a fetch condition\n\nThis example fetch condition prevents URLs ending in `.pdf` from downloading.\nAdding a fetch condition assigns it an ID, which the `addFetchCondition` function\nreturns. You can use this ID to remove the condition later.\n\n```javascript\nvar conditionID = myCrawler.addFetchCondition(function(parsedURL) {\n\treturn !parsedURL.path.match(/\\.pdf$/i);\n});\n```\n\nNOTE: simplecrawler uses slightly different terminology to URIjs. `parsedURL.path`\nincludes the query string too. If you want the path without the query string,\nuse `parsedURL.uriPath`.\n\n##### Removing a fetch condition\n\nIf you stored the ID of the fetch condition you added earlier, you can remove it\nfrom the crawler:\n\n```javascript\nmyCrawler.removeFetchCondition(conditionID);\n```\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed\nat `crawler.queue` (assuming you called your Crawler() object `crawler`.) It\nprovides array access, so you can get to queue items just with array notation\nand an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate\ninterface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nYou could always just `.push` a new resource onto the queue, but you'd need to\nhave it all in the correct format, and validate the URL yourself, and oh wouldn't\nthat be a pain. Instead, use the `queue.add` function provided for your\nconvenience:\n\n```javascript\ncrawler.queue.add(protocol,hostname,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can\nremember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items,\nit helps to know what's inside them. These are the properties every queue item\nis expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `host` - The full domain/hostname of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The\nupside is, the queue actually has some convenient functions for getting simple\naggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network\nperformance of your crawl (so far.) This is done live, so don't check it thirty\ntimes a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the\n`crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions\nrespectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given\nstatus at any one time, and/or retreive them. That's easy with\n`crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.getwithStatus` returns the number of queued items with a given\nstatus, while `crawler.queue.getWithStatus` returns an array of the queue items\nthemselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n*\t`crawler.queue.complete` - returns the number of queue items which have been\n\tcompleted (marked as fetched)\n*\t`crawler.queue.errors` - returns the number of requests which have failed\n\t(404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if\nyour application fails or you need to abort the crawl for some reason. (Perhaps\nyou just want to finish off for the night and pick it up tomorrow!) The\n`crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be\nasynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when\nyou need to save the queue - don't call them every request or your application's\nperformance will be incredibly poor - they block like *crazy*. That said, using\nthem when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n## Cookies\n\nSimplecrawler now has an internal cookie jar, which collects and resends cookies\nautomatically, and by default.\n\nIf you want to turn this off, set the `crawler.acceptCookies` option to `false`.\n\nThe cookie jar is accessible via `crawler.cookies`, and is an event emitter itself:\n\n### Cookie Events\n\n* `addcookie` ( cookie )\nFired when a new cookie is added to the jar.\n* `removecookie` ( cookie array )\nFired when one or more cookies are removed from the jar.\n\n## Building and Testing\n\n#### Build Status:\n\n* Master: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n* Development: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=development)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\n## Contributors\n\nI'd like to extend sincere thanks to:\n\n*\t[Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support, and\n\tinitial cookie support.\n*\t[Mike Moulton](https://github.com/mmoulton) for\n\t[fixing a bug in the URL discovery mechanism]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/3), as well as\n\t[adding the `discoverycomplete` event]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/10),\n*\t[Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword\n\tnaming collision with node 0.8's EventEmitter.\n*\t[Greg Molnar](https://github.com/gregmolnar) for\n\t[adding a querystring-free path parameter to parsed URL objects.]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/31)\n*\t[Breck Yunits](https://github.com/breck7) for contributing a useful code\n\tsample demonstrating using simplecrawler for caching a website to disk!\n\nAnd everybody else who has helped out in some way! :)\n\n## Licence\n\nCopyright (c) 2012, Christopher Giffard.\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, \nare permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n* Redistributions in binary form must reproduce the above copyright notice, this\n  list of conditions and the following disclaimer in the documentation and/or\n  other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR \nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","_id":"simplecrawler@0.2.0","dist":{"shasum":"594e580d4e6462031fcbb8986577b6d804eca2ae","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.2.0.tgz"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},{"name":"cgiffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.2.1":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.2.1","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"scripts":{"test":"mocha -R spec"},"bin":{"crawl":"./lib/cli.js"},"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./lib/index.js","engines":{"node":">=0.4.0"},"devDependencies":{"mocha":"~1.8.1","jshint":"~0.7.x","chai":"~1.2.0"},"dependencies":{"iconv":"~1.2.4","URIjs":"~1.8.3"},"readme":"# Simple web-crawler for node.js [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\nSimplecrawler is designed to provide the most basic possible API for crawling\nwebsites, while being as flexible and robust as possible. I wrote simplecrawler\nto archive, analyse, and search some very large websites. It has happily chewed\nthrough 50,000 pages and written tens of gigabytes to disk without issue.\n\n#### Example (simple mode)\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n\nCrawler.crawl(\"http://example.com/\")\n\t.on(\"fetchcomplete\",function(queueItem){\n\t\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n\t});\n```\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can\nreplace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except\nwhen discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nThere are two ways of instantiating a new crawler - a simple but less flexible\nmethod inspired by [anemone](http://anemone.rubyforge.org), and the traditional\nmethod which provides a little more room to configure crawl parameters.\n\nRegardless of wether you use the simple or traditional methods of instantiation,\nyou'll need to require simplecrawler:\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n```\n\n#### Simple Mode\n\nSimple mode generates a new crawler for you, preconfigures it based on a URL you\nprovide, and returns the crawler to you for further configuration and so you can\nattach event handlers.\n\nSimply call `Crawler.crawl`, with a URL first parameter, and two optional\nfunctions that will be added as event listeners for `fetchcomplete` and\n`fetcherror` respectively.\n\n```javascript\nCrawler.crawl(\"http://example.com/\", function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\nAlternately, if you decide to omit these functions, you can use the returned\ncrawler object to add the event listeners yourself, and tweak configuration\noptions:\n\n```javascript\nvar crawler = Crawler.crawl(\"http://example.com/\");\n\ncrawler.interval = 500;\n\ncrawler.on(\"fetchcomplete\",function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\n#### Advanced Mode\n\nThe alternative method of creating a crawler is to call the `simplecrawler`\nconstructor yourself, and to initiate the crawl manually.\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your web\nserver. Decrease the concurrency from five simultaneous requests - and increase\nthe request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when\ncreating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run\nthrough its queue finding linked resources on the domain to download, until it\ncan't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `crawlstart`\nFired when the crawl begins or is restarted.\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually\nqueue an item yourself.)\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem , requestOptions )\nFired when an item is spooled for fetching. If your event handler is synchronous,\nyou can modify the crawler request options (including headers) \n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http\nresponse object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided\nas a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size\nwe're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned\nas a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a\nrequest.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as\nthe second parameter.\n* `discoverycomplete` ( queueItem, resources )\nFired when linked resources have been discovered. Passes an array of resources\n(as URLs) as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does\nnot find any more to add. This event returns no arguments.\n\n#### A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters\nan HTTP error status in the response. If you need this information, you can listen\nto simplecrawler's error events, and through node's native `data` event\n(`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let\nme know. I didn't include it because I didn't need it - but if it's important to\npeople I might put it back in. :)\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n*\t`crawler.host` -\n\tThe domain to scan. By default, simplecrawler will restrict all requests to\n\tthis domain.\n*\t`crawler.initialPath` -\n\tThe initial path with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialPort` -\n\tThe initial port with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialProtocol` -\n\tThe initial protocol with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.interval` -\n\tThe interval with which the crawler will spool up new requests (one per\n\ttick.) Defaults to 250ms.\n*\t`crawler.maxConcurrency` -\n\tThe maximum number of requests the crawler will run simultaneously. Defaults\n\tto 5 - the default number of http agents node will run.\n*\t`crawler.timeout` -\n\tThe maximum time the crawler will wait for headers before aborting the request.\n*\t`crawler.userAgent` -\n\tThe user agent the crawler will report. Defaults to\n\t`Node/SimpleCrawler <version> (http://www.github.com/cgiffard/node-simplecrawler)`.\n*\t`crawler.queue` -\n\tThe queue in use by the crawler (Must implement the `FetchQueue` interface)\n*\t`crawler.filterByDomain` -\n\tSpecifies whether the crawler will restrict queued requests to a given\n\tdomain/domains.\n*\t`crawler.scanSubdomains` -\n\tEnables scanning subdomains (other than www) as well as the specified domain.\n\tDefaults to false.\n*\t`crawler.ignoreWWWDomain` -\n\tTreats the `www` domain the same as the originally specified domain.\n\tDefaults to true.\n*\t`crawler.stripWWWDomain` -\n\tOr go even further and strip WWW subdomain from requests altogether!\n*\t`crawler.discoverResources` -\n\tUse simplecrawler's internal resource discovery function. Defaults to true.\n\t(switch it off if you'd prefer to discover and queue resources yourself!)\n*\t`crawler.cache` -\n\tSpecify a cache architecture to use when crawling. Must implement\n\t`SimpleCache` interface.\n*\t`crawler.useProxy` -\n\tThe crawler should use an HTTP proxy to make its requests.\n*\t`crawler.proxyHostname` -\n\tThe hostname of the proxy to use for requests.\n*\t`crawler.proxyPort` -\n\tThe port of the proxy to use for requests.\n*\t`crawler.domainWhitelist` -\n\tAn array of domains the crawler is permitted to crawl from. If other settings\n\tare more permissive, they will override this setting.\n*\t`crawler.supportedMimeTypes` -\n\tAn array of RegEx objects used to determine supported MIME types (types of\n\tdata simplecrawler will scan for links.) If you're  not using simplecrawler's\n\tresource discovery function, this won't have any effect.\n*\t`crawler.allowedProtocols` -\n\tAn array of RegEx objects used to determine whether a URL protocol is supported.\n\tThis is to deal with nonstandard protocol handlers that regular HTTP is\n\tsometimes given, like `feed:`. It does not provide support for non-http\n\tprotocols (and why would it!?)\n*\t`crawler.maxResourceSize` - \n\tThe maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n*\t`crawler.downloadUnsupported` -\n\tSimplecrawler will download files it can't parse. Defaults to true, but if\n\tyou'd rather save the RAM and GC lag, switch it off.\n*\t`crawler.needsAuth` -\n\tFlag to specify if the domain you are hitting requires basic authentication\n*\t`crawler.authUser` -\n\tUsername provided for needsAuth flag\n*\t`crawler.authPass` -\n\tPassword provided for needsAuth flag\n*\t`crawler.customHeaders` -\n\tAn object specifying a number of custom headers simplecrawler will add to\n\tevery request. These override the default headers simplecrawler sets, so\n\tbe careful with them. If you want to tamper with headers on a per-request basis,\n\tsee the `fetchqueue` event.\n*\t`crawler.acceptCookies` -\n\tFlag to indicate if the crawler should hold on to cookies\n*\t`crawler.urlEncoding` -\n\tSet this to `iso8859` to trigger URIjs' re-encoding of iso8859 URLs to unicode.\n\tDefaults to `unicode`.\n\n#### Excluding certain resources from downloading\n\nSimplecrawler has a mechanism you can use to prevent certain resources from being\nfetched, based on the URL, called *Fetch Conditions**. A fetch condition is just\na function, which, when given a parsed URL object, will return a true or a false\nvalue, indicating whether a given resource should be downloaded.\n\nYou may add as many fetch conditions as you like, and remove them at runtime.\nSimplecrawler will evaluate every single condition against every queued URL, and\nshould just one of them return a falsy value (this includes null and undefined,\nso remember to always return a value!) then the resource in question will not be\nfetched.\n\n##### Adding a fetch condition\n\nThis example fetch condition prevents URLs ending in `.pdf` from downloading.\nAdding a fetch condition assigns it an ID, which the `addFetchCondition` function\nreturns. You can use this ID to remove the condition later.\n\n```javascript\nvar conditionID = myCrawler.addFetchCondition(function(parsedURL) {\n\treturn !parsedURL.path.match(/\\.pdf$/i);\n});\n```\n\nNOTE: simplecrawler uses slightly different terminology to URIjs. `parsedURL.path`\nincludes the query string too. If you want the path without the query string,\nuse `parsedURL.uriPath`.\n\n##### Removing a fetch condition\n\nIf you stored the ID of the fetch condition you added earlier, you can remove it\nfrom the crawler:\n\n```javascript\nmyCrawler.removeFetchCondition(conditionID);\n```\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed\nat `crawler.queue` (assuming you called your Crawler() object `crawler`.) It\nprovides array access, so you can get to queue items just with array notation\nand an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate\ninterface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nThe simplest way to add to the queue is to use the crawler's own method,\n`crawler.queueURL`. This method takes a complete URL, validates and deconstructs\nit, and adds it to the queue.\n\nIf you instead want to add a resource by its components, you may call the\n`queue.add` method directly:\n\n```javascript\ncrawler.queue.add(protocol,hostname,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can\nremember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items,\nit helps to know what's inside them. These are the properties every queue item\nis expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `host` - The full domain/hostname of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The\nupside is, the queue actually has some convenient functions for getting simple\naggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network\nperformance of your crawl (so far.) This is done live, so don't check it thirty\ntimes a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the\n`crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions\nrespectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given\nstatus at any one time, and/or retreive them. That's easy with\n`crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.getwithStatus` returns the number of queued items with a given\nstatus, while `crawler.queue.getWithStatus` returns an array of the queue items\nthemselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n*\t`crawler.queue.complete` - returns the number of queue items which have been\n\tcompleted (marked as fetched)\n*\t`crawler.queue.errors` - returns the number of requests which have failed\n\t(404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if\nyour application fails or you need to abort the crawl for some reason. (Perhaps\nyou just want to finish off for the night and pick it up tomorrow!) The\n`crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be\nasynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when\nyou need to save the queue - don't call them every request or your application's\nperformance will be incredibly poor - they block like *crazy*. That said, using\nthem when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n## Cookies\n\nSimplecrawler now has an internal cookie jar, which collects and resends cookies\nautomatically, and by default.\n\nIf you want to turn this off, set the `crawler.acceptCookies` option to `false`.\n\nThe cookie jar is accessible via `crawler.cookies`, and is an event emitter itself:\n\n### Cookie Events\n\n* `addcookie` ( cookie )\nFired when a new cookie is added to the jar.\n* `removecookie` ( cookie array )\nFired when one or more cookies are removed from the jar.\n\n## Building and Testing\n\n#### Build Status:\n\n* Master: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n* Development: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=development)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\n## Contributors\n\nI'd like to extend sincere thanks to:\n\n*\t[Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support, and\n\tinitial cookie support.\n*\t[Mike Moulton](https://github.com/mmoulton) for\n\t[fixing a bug in the URL discovery mechanism]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/3), as well as\n\t[adding the `discoverycomplete` event]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/10),\n*\t[Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword\n\tnaming collision with node 0.8's EventEmitter.\n*\t[Greg Molnar](https://github.com/gregmolnar) for\n\t[adding a querystring-free path parameter to parsed URL objects.]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/31)\n*\t[Breck Yunits](https://github.com/breck7) for contributing a useful code\n\tsample demonstrating using simplecrawler for caching a website to disk!\n\nAnd everybody else who has helped out in some way! :)\n\n## Licence\n\nCopyright (c) 2012, Christopher Giffard.\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, \nare permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n* Redistributions in binary form must reproduce the above copyright notice, this\n  list of conditions and the following disclaimer in the documentation and/or\n  other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR \nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","_id":"simplecrawler@0.2.1","dist":{"shasum":"953f7cf0f4637083dcf943b47dabff4ffd8dcaf6","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.2.1.tgz"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},{"name":"cgiffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.2.2":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.2.2","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"scripts":{"test":"mocha -R spec"},"bin":{"crawl":"./lib/cli.js"},"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./lib/index.js","engines":{"node":">=0.4.0"},"devDependencies":{"mocha":"~1.8.1","jshint":"~0.7.x","chai":"~1.2.0"},"dependencies":{"URIjs":"~1.8.3"},"readme":"# Simple web-crawler for node.js [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\nSimplecrawler is designed to provide the most basic possible API for crawling\nwebsites, while being as flexible and robust as possible. I wrote simplecrawler\nto archive, analyse, and search some very large websites. It has happily chewed\nthrough 50,000 pages and written tens of gigabytes to disk without issue.\n\n#### Example (simple mode)\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n\nCrawler.crawl(\"http://example.com/\")\n\t.on(\"fetchcomplete\",function(queueItem){\n\t\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n\t});\n```\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can\nreplace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except\nwhen discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nThere are two ways of instantiating a new crawler - a simple but less flexible\nmethod inspired by [anemone](http://anemone.rubyforge.org), and the traditional\nmethod which provides a little more room to configure crawl parameters.\n\nRegardless of wether you use the simple or traditional methods of instantiation,\nyou'll need to require simplecrawler:\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n```\n\n#### Simple Mode\n\nSimple mode generates a new crawler for you, preconfigures it based on a URL you\nprovide, and returns the crawler to you for further configuration and so you can\nattach event handlers.\n\nSimply call `Crawler.crawl`, with a URL first parameter, and two optional\nfunctions that will be added as event listeners for `fetchcomplete` and\n`fetcherror` respectively.\n\n```javascript\nCrawler.crawl(\"http://example.com/\", function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\nAlternately, if you decide to omit these functions, you can use the returned\ncrawler object to add the event listeners yourself, and tweak configuration\noptions:\n\n```javascript\nvar crawler = Crawler.crawl(\"http://example.com/\");\n\ncrawler.interval = 500;\n\ncrawler.on(\"fetchcomplete\",function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\n#### Advanced Mode\n\nThe alternative method of creating a crawler is to call the `simplecrawler`\nconstructor yourself, and to initiate the crawl manually.\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your web\nserver. Decrease the concurrency from five simultaneous requests - and increase\nthe request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when\ncreating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run\nthrough its queue finding linked resources on the domain to download, until it\ncan't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `crawlstart`\nFired when the crawl begins or is restarted.\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually\nqueue an item yourself.)\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem , requestOptions )\nFired when an item is spooled for fetching. If your event handler is synchronous,\nyou can modify the crawler request options (including headers) \n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http\nresponse object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided\nas a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size\nwe're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned\nas a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a\nrequest.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as\nthe second parameter.\n* `discoverycomplete` ( queueItem, resources )\nFired when linked resources have been discovered. Passes an array of resources\n(as URLs) as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does\nnot find any more to add. This event returns no arguments.\n\n#### A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters\nan HTTP error status in the response. If you need this information, you can listen\nto simplecrawler's error events, and through node's native `data` event\n(`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let\nme know. I didn't include it because I didn't need it - but if it's important to\npeople I might put it back in. :)\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n*\t`crawler.host` -\n\tThe domain to scan. By default, simplecrawler will restrict all requests to\n\tthis domain.\n*\t`crawler.initialPath` -\n\tThe initial path with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialPort` -\n\tThe initial port with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialProtocol` -\n\tThe initial protocol with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.interval` -\n\tThe interval with which the crawler will spool up new requests (one per\n\ttick.) Defaults to 250ms.\n*\t`crawler.maxConcurrency` -\n\tThe maximum number of requests the crawler will run simultaneously. Defaults\n\tto 5 - the default number of http agents node will run.\n*\t`crawler.timeout` -\n\tThe maximum time the crawler will wait for headers before aborting the request.\n*\t`crawler.userAgent` -\n\tThe user agent the crawler will report. Defaults to\n\t`Node/SimpleCrawler <version> (http://www.github.com/cgiffard/node-simplecrawler)`.\n*\t`crawler.queue` -\n\tThe queue in use by the crawler (Must implement the `FetchQueue` interface)\n*\t`crawler.filterByDomain` -\n\tSpecifies whether the crawler will restrict queued requests to a given\n\tdomain/domains.\n*\t`crawler.scanSubdomains` -\n\tEnables scanning subdomains (other than www) as well as the specified domain.\n\tDefaults to false.\n*\t`crawler.ignoreWWWDomain` -\n\tTreats the `www` domain the same as the originally specified domain.\n\tDefaults to true.\n*\t`crawler.stripWWWDomain` -\n\tOr go even further and strip WWW subdomain from requests altogether!\n*\t`crawler.discoverResources` -\n\tUse simplecrawler's internal resource discovery function. Defaults to true.\n\t(switch it off if you'd prefer to discover and queue resources yourself!)\n*\t`crawler.cache` -\n\tSpecify a cache architecture to use when crawling. Must implement\n\t`SimpleCache` interface.\n*\t`crawler.useProxy` -\n\tThe crawler should use an HTTP proxy to make its requests.\n*\t`crawler.proxyHostname` -\n\tThe hostname of the proxy to use for requests.\n*\t`crawler.proxyPort` -\n\tThe port of the proxy to use for requests.\n*\t`crawler.domainWhitelist` -\n\tAn array of domains the crawler is permitted to crawl from. If other settings\n\tare more permissive, they will override this setting.\n*\t`crawler.supportedMimeTypes` -\n\tAn array of RegEx objects used to determine supported MIME types (types of\n\tdata simplecrawler will scan for links.) If you're  not using simplecrawler's\n\tresource discovery function, this won't have any effect.\n*\t`crawler.allowedProtocols` -\n\tAn array of RegEx objects used to determine whether a URL protocol is supported.\n\tThis is to deal with nonstandard protocol handlers that regular HTTP is\n\tsometimes given, like `feed:`. It does not provide support for non-http\n\tprotocols (and why would it!?)\n*\t`crawler.maxResourceSize` - \n\tThe maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n*\t`crawler.downloadUnsupported` -\n\tSimplecrawler will download files it can't parse. Defaults to true, but if\n\tyou'd rather save the RAM and GC lag, switch it off.\n*\t`crawler.needsAuth` -\n\tFlag to specify if the domain you are hitting requires basic authentication\n*\t`crawler.authUser` -\n\tUsername provided for needsAuth flag\n*\t`crawler.authPass` -\n\tPassword provided for needsAuth flag\n*\t`crawler.customHeaders` -\n\tAn object specifying a number of custom headers simplecrawler will add to\n\tevery request. These override the default headers simplecrawler sets, so\n\tbe careful with them. If you want to tamper with headers on a per-request basis,\n\tsee the `fetchqueue` event.\n*\t`crawler.acceptCookies` -\n\tFlag to indicate if the crawler should hold on to cookies\n*\t`crawler.urlEncoding` -\n\tSet this to `iso8859` to trigger URIjs' re-encoding of iso8859 URLs to unicode.\n\tDefaults to `unicode`.\n\n#### Excluding certain resources from downloading\n\nSimplecrawler has a mechanism you can use to prevent certain resources from being\nfetched, based on the URL, called *Fetch Conditions**. A fetch condition is just\na function, which, when given a parsed URL object, will return a true or a false\nvalue, indicating whether a given resource should be downloaded.\n\nYou may add as many fetch conditions as you like, and remove them at runtime.\nSimplecrawler will evaluate every single condition against every queued URL, and\nshould just one of them return a falsy value (this includes null and undefined,\nso remember to always return a value!) then the resource in question will not be\nfetched.\n\n##### Adding a fetch condition\n\nThis example fetch condition prevents URLs ending in `.pdf` from downloading.\nAdding a fetch condition assigns it an ID, which the `addFetchCondition` function\nreturns. You can use this ID to remove the condition later.\n\n```javascript\nvar conditionID = myCrawler.addFetchCondition(function(parsedURL) {\n\treturn !parsedURL.path.match(/\\.pdf$/i);\n});\n```\n\nNOTE: simplecrawler uses slightly different terminology to URIjs. `parsedURL.path`\nincludes the query string too. If you want the path without the query string,\nuse `parsedURL.uriPath`.\n\n##### Removing a fetch condition\n\nIf you stored the ID of the fetch condition you added earlier, you can remove it\nfrom the crawler:\n\n```javascript\nmyCrawler.removeFetchCondition(conditionID);\n```\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed\nat `crawler.queue` (assuming you called your Crawler() object `crawler`.) It\nprovides array access, so you can get to queue items just with array notation\nand an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate\ninterface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nThe simplest way to add to the queue is to use the crawler's own method,\n`crawler.queueURL`. This method takes a complete URL, validates and deconstructs\nit, and adds it to the queue.\n\nIf you instead want to add a resource by its components, you may call the\n`queue.add` method directly:\n\n```javascript\ncrawler.queue.add(protocol,hostname,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can\nremember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items,\nit helps to know what's inside them. These are the properties every queue item\nis expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `host` - The full domain/hostname of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The\nupside is, the queue actually has some convenient functions for getting simple\naggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network\nperformance of your crawl (so far.) This is done live, so don't check it thirty\ntimes a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the\n`crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions\nrespectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given\nstatus at any one time, and/or retreive them. That's easy with\n`crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.getwithStatus` returns the number of queued items with a given\nstatus, while `crawler.queue.getWithStatus` returns an array of the queue items\nthemselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n*\t`crawler.queue.complete` - returns the number of queue items which have been\n\tcompleted (marked as fetched)\n*\t`crawler.queue.errors` - returns the number of requests which have failed\n\t(404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if\nyour application fails or you need to abort the crawl for some reason. (Perhaps\nyou just want to finish off for the night and pick it up tomorrow!) The\n`crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be\nasynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when\nyou need to save the queue - don't call them every request or your application's\nperformance will be incredibly poor - they block like *crazy*. That said, using\nthem when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n## Cookies\n\nSimplecrawler now has an internal cookie jar, which collects and resends cookies\nautomatically, and by default.\n\nIf you want to turn this off, set the `crawler.acceptCookies` option to `false`.\n\nThe cookie jar is accessible via `crawler.cookies`, and is an event emitter itself:\n\n### Cookie Events\n\n* `addcookie` ( cookie )\nFired when a new cookie is added to the jar.\n* `removecookie` ( cookie array )\nFired when one or more cookies are removed from the jar.\n\n## Building and Testing\n\n#### Build Status:\n\n* Master: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n* Development: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=development)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\n## Contributors\n\nI'd like to extend sincere thanks to:\n\n*\t[Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support, and\n\tinitial cookie support.\n*\t[Mike Moulton](https://github.com/mmoulton) for\n\t[fixing a bug in the URL discovery mechanism]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/3), as well as\n\t[adding the `discoverycomplete` event]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/10),\n*\t[Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword\n\tnaming collision with node 0.8's EventEmitter.\n*\t[Greg Molnar](https://github.com/gregmolnar) for\n\t[adding a querystring-free path parameter to parsed URL objects.]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/31)\n*\t[Breck Yunits](https://github.com/breck7) for contributing a useful code\n\tsample demonstrating using simplecrawler for caching a website to disk!\n\nAnd everybody else who has helped out in some way! :)\n\n## Licence\n\nCopyright (c) 2012, Christopher Giffard.\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, \nare permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n* Redistributions in binary form must reproduce the above copyright notice, this\n  list of conditions and the following disclaimer in the documentation and/or\n  other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR \nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","readmeFilename":"README.markdown","_id":"simplecrawler@0.2.2","dist":{"shasum":"6b8ad7d60750990d410accdfc528421f9a8f8dce","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.2.2.tgz"},"_from":".","_npmVersion":"1.2.18","_npmUser":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},{"name":"cgiffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.2.3":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.2.3","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"scripts":{"test":"mocha -R spec"},"bin":{"crawl":"./lib/cli.js"},"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./lib/index.js","engines":{"node":">=0.4.0"},"devDependencies":{"mocha":"~1.8.1","jshint":"~0.7.x","chai":"~1.2.0"},"dependencies":{"URIjs":"~1.8.3"},"readme":"# Simple web-crawler for node.js [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\nSimplecrawler is designed to provide the most basic possible API for crawling\nwebsites, while being as flexible and robust as possible. I wrote simplecrawler\nto archive, analyse, and search some very large websites. It has happily chewed\nthrough 50,000 pages and written tens of gigabytes to disk without issue.\n\n#### Example (simple mode)\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n\nCrawler.crawl(\"http://example.com/\")\n\t.on(\"fetchcomplete\",function(queueItem){\n\t\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n\t});\n```\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can\nreplace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except\nwhen discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nThere are two ways of instantiating a new crawler - a simple but less flexible\nmethod inspired by [anemone](http://anemone.rubyforge.org), and the traditional\nmethod which provides a little more room to configure crawl parameters.\n\nRegardless of wether you use the simple or traditional methods of instantiation,\nyou'll need to require simplecrawler:\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n```\n\n#### Simple Mode\n\nSimple mode generates a new crawler for you, preconfigures it based on a URL you\nprovide, and returns the crawler to you for further configuration and so you can\nattach event handlers.\n\nSimply call `Crawler.crawl`, with a URL first parameter, and two optional\nfunctions that will be added as event listeners for `fetchcomplete` and\n`fetcherror` respectively.\n\n```javascript\nCrawler.crawl(\"http://example.com/\", function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\nAlternately, if you decide to omit these functions, you can use the returned\ncrawler object to add the event listeners yourself, and tweak configuration\noptions:\n\n```javascript\nvar crawler = Crawler.crawl(\"http://example.com/\");\n\ncrawler.interval = 500;\n\ncrawler.on(\"fetchcomplete\",function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\n#### Advanced Mode\n\nThe alternative method of creating a crawler is to call the `simplecrawler`\nconstructor yourself, and to initiate the crawl manually.\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your web\nserver. Decrease the concurrency from five simultaneous requests - and increase\nthe request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when\ncreating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run\nthrough its queue finding linked resources on the domain to download, until it\ncan't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `crawlstart`\nFired when the crawl begins or is restarted.\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually\nqueue an item yourself.)\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem , requestOptions )\nFired when an item is spooled for fetching. If your event handler is synchronous,\nyou can modify the crawler request options (including headers) \n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http\nresponse object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided\nas a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size\nwe're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned\nas a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a\nrequest.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as\nthe second parameter.\n* `discoverycomplete` ( queueItem, resources )\nFired when linked resources have been discovered. Passes an array of resources\n(as URLs) as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does\nnot find any more to add. This event returns no arguments.\n\n#### A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters\nan HTTP error status in the response. If you need this information, you can listen\nto simplecrawler's error events, and through node's native `data` event\n(`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let\nme know. I didn't include it because I didn't need it - but if it's important to\npeople I might put it back in. :)\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n*\t`crawler.host` -\n\tThe domain to scan. By default, simplecrawler will restrict all requests to\n\tthis domain.\n*\t`crawler.initialPath` -\n\tThe initial path with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialPort` -\n\tThe initial port with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialProtocol` -\n\tThe initial protocol with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.interval` -\n\tThe interval with which the crawler will spool up new requests (one per\n\ttick.) Defaults to 250ms.\n*\t`crawler.maxConcurrency` -\n\tThe maximum number of requests the crawler will run simultaneously. Defaults\n\tto 5 - the default number of http agents node will run.\n*\t`crawler.timeout` -\n\tThe maximum time the crawler will wait for headers before aborting the request.\n*\t`crawler.userAgent` -\n\tThe user agent the crawler will report. Defaults to\n\t`Node/SimpleCrawler <version> (http://www.github.com/cgiffard/node-simplecrawler)`.\n*\t`crawler.queue` -\n\tThe queue in use by the crawler (Must implement the `FetchQueue` interface)\n*\t`crawler.filterByDomain` -\n\tSpecifies whether the crawler will restrict queued requests to a given\n\tdomain/domains.\n*\t`crawler.scanSubdomains` -\n\tEnables scanning subdomains (other than www) as well as the specified domain.\n\tDefaults to false.\n*\t`crawler.ignoreWWWDomain` -\n\tTreats the `www` domain the same as the originally specified domain.\n\tDefaults to true.\n*\t`crawler.stripWWWDomain` -\n\tOr go even further and strip WWW subdomain from requests altogether!\n*\t`crawler.discoverResources` -\n\tUse simplecrawler's internal resource discovery function. Defaults to true.\n\t(switch it off if you'd prefer to discover and queue resources yourself!)\n*\t`crawler.cache` -\n\tSpecify a cache architecture to use when crawling. Must implement\n\t`SimpleCache` interface.\n*\t`crawler.useProxy` -\n\tThe crawler should use an HTTP proxy to make its requests.\n*\t`crawler.proxyHostname` -\n\tThe hostname of the proxy to use for requests.\n*\t`crawler.proxyPort` -\n\tThe port of the proxy to use for requests.\n*\t`crawler.domainWhitelist` -\n\tAn array of domains the crawler is permitted to crawl from. If other settings\n\tare more permissive, they will override this setting.\n*\t`crawler.supportedMimeTypes` -\n\tAn array of RegEx objects used to determine supported MIME types (types of\n\tdata simplecrawler will scan for links.) If you're  not using simplecrawler's\n\tresource discovery function, this won't have any effect.\n*\t`crawler.allowedProtocols` -\n\tAn array of RegEx objects used to determine whether a URL protocol is supported.\n\tThis is to deal with nonstandard protocol handlers that regular HTTP is\n\tsometimes given, like `feed:`. It does not provide support for non-http\n\tprotocols (and why would it!?)\n*\t`crawler.maxResourceSize` - \n\tThe maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n*\t`crawler.downloadUnsupported` -\n\tSimplecrawler will download files it can't parse. Defaults to true, but if\n\tyou'd rather save the RAM and GC lag, switch it off.\n*\t`crawler.needsAuth` -\n\tFlag to specify if the domain you are hitting requires basic authentication\n*\t`crawler.authUser` -\n\tUsername provided for needsAuth flag\n*\t`crawler.authPass` -\n\tPassword provided for needsAuth flag\n*\t`crawler.customHeaders` -\n\tAn object specifying a number of custom headers simplecrawler will add to\n\tevery request. These override the default headers simplecrawler sets, so\n\tbe careful with them. If you want to tamper with headers on a per-request basis,\n\tsee the `fetchqueue` event.\n*\t`crawler.acceptCookies` -\n\tFlag to indicate if the crawler should hold on to cookies\n*\t`crawler.urlEncoding` -\n\tSet this to `iso8859` to trigger URIjs' re-encoding of iso8859 URLs to unicode.\n\tDefaults to `unicode`.\n\n#### Excluding certain resources from downloading\n\nSimplecrawler has a mechanism you can use to prevent certain resources from being\nfetched, based on the URL, called *Fetch Conditions**. A fetch condition is just\na function, which, when given a parsed URL object, will return a true or a false\nvalue, indicating whether a given resource should be downloaded.\n\nYou may add as many fetch conditions as you like, and remove them at runtime.\nSimplecrawler will evaluate every single condition against every queued URL, and\nshould just one of them return a falsy value (this includes null and undefined,\nso remember to always return a value!) then the resource in question will not be\nfetched.\n\n##### Adding a fetch condition\n\nThis example fetch condition prevents URLs ending in `.pdf` from downloading.\nAdding a fetch condition assigns it an ID, which the `addFetchCondition` function\nreturns. You can use this ID to remove the condition later.\n\n```javascript\nvar conditionID = myCrawler.addFetchCondition(function(parsedURL) {\n\treturn !parsedURL.path.match(/\\.pdf$/i);\n});\n```\n\nNOTE: simplecrawler uses slightly different terminology to URIjs. `parsedURL.path`\nincludes the query string too. If you want the path without the query string,\nuse `parsedURL.uriPath`.\n\n##### Removing a fetch condition\n\nIf you stored the ID of the fetch condition you added earlier, you can remove it\nfrom the crawler:\n\n```javascript\nmyCrawler.removeFetchCondition(conditionID);\n```\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed\nat `crawler.queue` (assuming you called your Crawler() object `crawler`.) It\nprovides array access, so you can get to queue items just with array notation\nand an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate\ninterface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nThe simplest way to add to the queue is to use the crawler's own method,\n`crawler.queueURL`. This method takes a complete URL, validates and deconstructs\nit, and adds it to the queue.\n\nIf you instead want to add a resource by its components, you may call the\n`queue.add` method directly:\n\n```javascript\ncrawler.queue.add(protocol,hostname,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can\nremember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items,\nit helps to know what's inside them. These are the properties every queue item\nis expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `host` - The full domain/hostname of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The\nupside is, the queue actually has some convenient functions for getting simple\naggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network\nperformance of your crawl (so far.) This is done live, so don't check it thirty\ntimes a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the\n`crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions\nrespectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given\nstatus at any one time, and/or retreive them. That's easy with\n`crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.getwithStatus` returns the number of queued items with a given\nstatus, while `crawler.queue.getWithStatus` returns an array of the queue items\nthemselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n*\t`crawler.queue.complete` - returns the number of queue items which have been\n\tcompleted (marked as fetched)\n*\t`crawler.queue.errors` - returns the number of requests which have failed\n\t(404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if\nyour application fails or you need to abort the crawl for some reason. (Perhaps\nyou just want to finish off for the night and pick it up tomorrow!) The\n`crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be\nasynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when\nyou need to save the queue - don't call them every request or your application's\nperformance will be incredibly poor - they block like *crazy*. That said, using\nthem when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n## Cookies\n\nSimplecrawler now has an internal cookie jar, which collects and resends cookies\nautomatically, and by default.\n\nIf you want to turn this off, set the `crawler.acceptCookies` option to `false`.\n\nThe cookie jar is accessible via `crawler.cookies`, and is an event emitter itself:\n\n### Cookie Events\n\n* `addcookie` ( cookie )\nFired when a new cookie is added to the jar.\n* `removecookie` ( cookie array )\nFired when one or more cookies are removed from the jar.\n\n## Building and Testing\n\n#### Build Status:\n\n* Master: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n* Development: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=development)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\n## Contributors\n\nI'd like to extend sincere thanks to:\n\n*\t[Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support, and\n\tinitial cookie support.\n*\t[Mike Moulton](https://github.com/mmoulton) for\n\t[fixing a bug in the URL discovery mechanism]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/3), as well as\n\t[adding the `discoverycomplete` event]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/10),\n*\t[Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword\n\tnaming collision with node 0.8's EventEmitter.\n*\t[Greg Molnar](https://github.com/gregmolnar) for\n\t[adding a querystring-free path parameter to parsed URL objects.]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/31)\n*\t[Breck Yunits](https://github.com/breck7) for contributing a useful code\n\tsample demonstrating using simplecrawler for caching a website to disk!\n*\t[Luke Plaster](https://github.com/notatestuser) for enabling protocol-agnostic\n\tlink discovery\n\nAnd everybody else who has helped out in some way! :)\n\n## Licence\n\nCopyright (c) 2012, Christopher Giffard.\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, \nare permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n* Redistributions in binary form must reproduce the above copyright notice, this\n  list of conditions and the following disclaimer in the documentation and/or\n  other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR \nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","readmeFilename":"README.markdown","_id":"simplecrawler@0.2.3","dist":{"shasum":"63aa3d45b986c70328636ea150c6735a3de6972c","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.2.3.tgz"},"_from":".","_npmVersion":"1.2.18","_npmUser":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},{"name":"cgiffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.2.4":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.2.4","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"scripts":{"test":"mocha -R spec"},"bin":{"crawl":"./lib/cli.js"},"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./lib/index.js","engines":{"node":">=0.4.0"},"devDependencies":{"mocha":"~1.8.1","jshint":"~0.7.x","chai":"~1.2.0"},"dependencies":{"URIjs":"~1.8.3"},"readme":"# Simple web-crawler for node.js [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\nSimplecrawler is designed to provide the most basic possible API for crawling\nwebsites, while being as flexible and robust as possible. I wrote simplecrawler\nto archive, analyse, and search some very large websites. It has happily chewed\nthrough 50,000 pages and written tens of gigabytes to disk without issue.\n\n#### Example (simple mode)\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n\nCrawler.crawl(\"http://example.com/\")\n\t.on(\"fetchcomplete\",function(queueItem){\n\t\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n\t});\n```\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can\nreplace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except\nwhen discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nThere are two ways of instantiating a new crawler - a simple but less flexible\nmethod inspired by [anemone](http://anemone.rubyforge.org), and the traditional\nmethod which provides a little more room to configure crawl parameters.\n\nRegardless of wether you use the simple or traditional methods of instantiation,\nyou'll need to require simplecrawler:\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n```\n\n#### Simple Mode\n\nSimple mode generates a new crawler for you, preconfigures it based on a URL you\nprovide, and returns the crawler to you for further configuration and so you can\nattach event handlers.\n\nSimply call `Crawler.crawl`, with a URL first parameter, and two optional\nfunctions that will be added as event listeners for `fetchcomplete` and\n`fetcherror` respectively.\n\n```javascript\nCrawler.crawl(\"http://example.com/\", function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\nAlternately, if you decide to omit these functions, you can use the returned\ncrawler object to add the event listeners yourself, and tweak configuration\noptions:\n\n```javascript\nvar crawler = Crawler.crawl(\"http://example.com/\");\n\ncrawler.interval = 500;\n\ncrawler.on(\"fetchcomplete\",function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\n#### Advanced Mode\n\nThe alternative method of creating a crawler is to call the `simplecrawler`\nconstructor yourself, and to initiate the crawl manually.\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your web\nserver. Decrease the concurrency from five simultaneous requests - and increase\nthe request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when\ncreating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run\nthrough its queue finding linked resources on the domain to download, until it\ncan't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `crawlstart`\nFired when the crawl begins or is restarted.\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually\nqueue an item yourself.)\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem , requestOptions )\nFired when an item is spooled for fetching. If your event handler is synchronous,\nyou can modify the crawler request options (including headers) \n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http\nresponse object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided\nas a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size\nwe're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned\nas a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a\nrequest.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as\nthe second parameter.\n* `discoverycomplete` ( queueItem, resources )\nFired when linked resources have been discovered. Passes an array of resources\n(as URLs) as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does\nnot find any more to add. This event returns no arguments.\n\n#### A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters\nan HTTP error status in the response. If you need this information, you can listen\nto simplecrawler's error events, and through node's native `data` event\n(`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let\nme know. I didn't include it because I didn't need it - but if it's important to\npeople I might put it back in. :)\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n*\t`crawler.host` -\n\tThe domain to scan. By default, simplecrawler will restrict all requests to\n\tthis domain.\n*\t`crawler.initialPath` -\n\tThe initial path with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialPort` -\n\tThe initial port with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialProtocol` -\n\tThe initial protocol with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.interval` -\n\tThe interval with which the crawler will spool up new requests (one per\n\ttick.) Defaults to 250ms.\n*\t`crawler.maxConcurrency` -\n\tThe maximum number of requests the crawler will run simultaneously. Defaults\n\tto 5 - the default number of http agents node will run.\n*\t`crawler.timeout` -\n\tThe maximum time the crawler will wait for headers before aborting the request.\n*\t`crawler.userAgent` -\n\tThe user agent the crawler will report. Defaults to\n\t`Node/SimpleCrawler <version> (http://www.github.com/cgiffard/node-simplecrawler)`.\n*\t`crawler.queue` -\n\tThe queue in use by the crawler (Must implement the `FetchQueue` interface)\n*\t`crawler.filterByDomain` -\n\tSpecifies whether the crawler will restrict queued requests to a given\n\tdomain/domains.\n*\t`crawler.scanSubdomains` -\n\tEnables scanning subdomains (other than www) as well as the specified domain.\n\tDefaults to false.\n*\t`crawler.ignoreWWWDomain` -\n\tTreats the `www` domain the same as the originally specified domain.\n\tDefaults to true.\n*\t`crawler.stripWWWDomain` -\n\tOr go even further and strip WWW subdomain from requests altogether!\n*\t`crawler.discoverResources` -\n\tUse simplecrawler's internal resource discovery function. Defaults to true.\n\t(switch it off if you'd prefer to discover and queue resources yourself!)\n*\t`crawler.cache` -\n\tSpecify a cache architecture to use when crawling. Must implement\n\t`SimpleCache` interface.\n*\t`crawler.useProxy` -\n\tThe crawler should use an HTTP proxy to make its requests.\n*\t`crawler.proxyHostname` -\n\tThe hostname of the proxy to use for requests.\n*\t`crawler.proxyPort` -\n\tThe port of the proxy to use for requests.\n*\t`crawler.domainWhitelist` -\n\tAn array of domains the crawler is permitted to crawl from. If other settings\n\tare more permissive, they will override this setting.\n*\t`crawler.supportedMimeTypes` -\n\tAn array of RegEx objects used to determine supported MIME types (types of\n\tdata simplecrawler will scan for links.) If you're  not using simplecrawler's\n\tresource discovery function, this won't have any effect.\n*\t`crawler.allowedProtocols` -\n\tAn array of RegEx objects used to determine whether a URL protocol is supported.\n\tThis is to deal with nonstandard protocol handlers that regular HTTP is\n\tsometimes given, like `feed:`. It does not provide support for non-http\n\tprotocols (and why would it!?)\n*\t`crawler.maxResourceSize` - \n\tThe maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n*\t`crawler.downloadUnsupported` -\n\tSimplecrawler will download files it can't parse. Defaults to true, but if\n\tyou'd rather save the RAM and GC lag, switch it off.\n*\t`crawler.needsAuth` -\n\tFlag to specify if the domain you are hitting requires basic authentication\n*\t`crawler.authUser` -\n\tUsername provided for needsAuth flag\n*\t`crawler.authPass` -\n\tPassword provided for needsAuth flag\n*\t`crawler.customHeaders` -\n\tAn object specifying a number of custom headers simplecrawler will add to\n\tevery request. These override the default headers simplecrawler sets, so\n\tbe careful with them. If you want to tamper with headers on a per-request basis,\n\tsee the `fetchqueue` event.\n*\t`crawler.acceptCookies` -\n\tFlag to indicate if the crawler should hold on to cookies\n*\t`crawler.urlEncoding` -\n\tSet this to `iso8859` to trigger URIjs' re-encoding of iso8859 URLs to unicode.\n\tDefaults to `unicode`.\n\n#### Excluding certain resources from downloading\n\nSimplecrawler has a mechanism you can use to prevent certain resources from being\nfetched, based on the URL, called *Fetch Conditions**. A fetch condition is just\na function, which, when given a parsed URL object, will return a true or a false\nvalue, indicating whether a given resource should be downloaded.\n\nYou may add as many fetch conditions as you like, and remove them at runtime.\nSimplecrawler will evaluate every single condition against every queued URL, and\nshould just one of them return a falsy value (this includes null and undefined,\nso remember to always return a value!) then the resource in question will not be\nfetched.\n\n##### Adding a fetch condition\n\nThis example fetch condition prevents URLs ending in `.pdf` from downloading.\nAdding a fetch condition assigns it an ID, which the `addFetchCondition` function\nreturns. You can use this ID to remove the condition later.\n\n```javascript\nvar conditionID = myCrawler.addFetchCondition(function(parsedURL) {\n\treturn !parsedURL.path.match(/\\.pdf$/i);\n});\n```\n\nNOTE: simplecrawler uses slightly different terminology to URIjs. `parsedURL.path`\nincludes the query string too. If you want the path without the query string,\nuse `parsedURL.uriPath`.\n\n##### Removing a fetch condition\n\nIf you stored the ID of the fetch condition you added earlier, you can remove it\nfrom the crawler:\n\n```javascript\nmyCrawler.removeFetchCondition(conditionID);\n```\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed\nat `crawler.queue` (assuming you called your Crawler() object `crawler`.) It\nprovides array access, so you can get to queue items just with array notation\nand an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate\ninterface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nThe simplest way to add to the queue is to use the crawler's own method,\n`crawler.queueURL`. This method takes a complete URL, validates and deconstructs\nit, and adds it to the queue.\n\nIf you instead want to add a resource by its components, you may call the\n`queue.add` method directly:\n\n```javascript\ncrawler.queue.add(protocol,hostname,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can\nremember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items,\nit helps to know what's inside them. These are the properties every queue item\nis expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `host` - The full domain/hostname of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The\nupside is, the queue actually has some convenient functions for getting simple\naggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network\nperformance of your crawl (so far.) This is done live, so don't check it thirty\ntimes a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the\n`crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions\nrespectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given\nstatus at any one time, and/or retreive them. That's easy with\n`crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.getwithStatus` returns the number of queued items with a given\nstatus, while `crawler.queue.getWithStatus` returns an array of the queue items\nthemselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n*\t`crawler.queue.complete` - returns the number of queue items which have been\n\tcompleted (marked as fetched)\n*\t`crawler.queue.errors` - returns the number of requests which have failed\n\t(404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if\nyour application fails or you need to abort the crawl for some reason. (Perhaps\nyou just want to finish off for the night and pick it up tomorrow!) The\n`crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be\nasynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when\nyou need to save the queue - don't call them every request or your application's\nperformance will be incredibly poor - they block like *crazy*. That said, using\nthem when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n## Cookies\n\nSimplecrawler now has an internal cookie jar, which collects and resends cookies\nautomatically, and by default.\n\nIf you want to turn this off, set the `crawler.acceptCookies` option to `false`.\n\nThe cookie jar is accessible via `crawler.cookies`, and is an event emitter itself:\n\n### Cookie Events\n\n* `addcookie` ( cookie )\nFired when a new cookie is added to the jar.\n* `removecookie` ( cookie array )\nFired when one or more cookies are removed from the jar.\n\n## Building and Testing\n\n#### Build Status:\n\n* Master: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n* Development: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=development)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\n## Contributors\n\nI'd like to extend sincere thanks to:\n\n*\t[Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support, and\n\tinitial cookie support.\n*\t[Mike Moulton](https://github.com/mmoulton) for\n\t[fixing a bug in the URL discovery mechanism]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/3), as well as\n\t[adding the `discoverycomplete` event]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/10),\n*\t[Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword\n\tnaming collision with node 0.8's EventEmitter.\n*\t[Greg Molnar](https://github.com/gregmolnar) for\n\t[adding a querystring-free path parameter to parsed URL objects.]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/31)\n*\t[Breck Yunits](https://github.com/breck7) for contributing a useful code\n\tsample demonstrating using simplecrawler for caching a website to disk!\n*\t[Luke Plaster](https://github.com/notatestuser) for enabling protocol-agnostic\n\tlink discovery\n\nAnd everybody else who has helped out in some way! :)\n\n## Licence\n\nCopyright (c) 2012, Christopher Giffard.\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, \nare permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n* Redistributions in binary form must reproduce the above copyright notice, this\n  list of conditions and the following disclaimer in the documentation and/or\n  other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR \nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","readmeFilename":"README.markdown","_id":"simplecrawler@0.2.4","dist":{"shasum":"279edad60463a292ec384b88490166bd9dd2a76c","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.2.4.tgz"},"_from":".","_npmVersion":"1.2.18","_npmUser":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},{"name":"cgiffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.2.5":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.2.5","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"scripts":{"test":"mocha -R spec"},"bin":{"crawl":"./lib/cli.js"},"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./lib/index.js","engines":{"node":">=0.4.0"},"devDependencies":{"mocha":"~1.8.1","jshint":"~0.7.x","chai":"~1.2.0"},"dependencies":{"URIjs":"~1.8.3"},"readme":"# Simple web-crawler for node.js [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\nSimplecrawler is designed to provide the most basic possible API for crawling\nwebsites, while being as flexible and robust as possible. I wrote simplecrawler\nto archive, analyse, and search some very large websites. It has happily chewed\nthrough 50,000 pages and written tens of gigabytes to disk without issue.\n\n#### Example (simple mode)\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n\nCrawler.crawl(\"http://example.com/\")\n\t.on(\"fetchcomplete\",function(queueItem){\n\t\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n\t});\n```\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can\nreplace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except\nwhen discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nThere are two ways of instantiating a new crawler - a simple but less flexible\nmethod inspired by [anemone](http://anemone.rubyforge.org), and the traditional\nmethod which provides a little more room to configure crawl parameters.\n\nRegardless of wether you use the simple or traditional methods of instantiation,\nyou'll need to require simplecrawler:\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n```\n\n#### Simple Mode\n\nSimple mode generates a new crawler for you, preconfigures it based on a URL you\nprovide, and returns the crawler to you for further configuration and so you can\nattach event handlers.\n\nSimply call `Crawler.crawl`, with a URL first parameter, and two optional\nfunctions that will be added as event listeners for `fetchcomplete` and\n`fetcherror` respectively.\n\n```javascript\nCrawler.crawl(\"http://example.com/\", function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\nAlternately, if you decide to omit these functions, you can use the returned\ncrawler object to add the event listeners yourself, and tweak configuration\noptions:\n\n```javascript\nvar crawler = Crawler.crawl(\"http://example.com/\");\n\ncrawler.interval = 500;\n\ncrawler.on(\"fetchcomplete\",function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\n#### Advanced Mode\n\nThe alternative method of creating a crawler is to call the `simplecrawler`\nconstructor yourself, and to initiate the crawl manually.\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your web\nserver. Decrease the concurrency from five simultaneous requests - and increase\nthe request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when\ncreating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run\nthrough its queue finding linked resources on the domain to download, until it\ncan't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `crawlstart`\nFired when the crawl begins or is restarted.\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually\nqueue an item yourself.)\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem , requestOptions )\nFired when an item is spooled for fetching. If your event handler is synchronous,\nyou can modify the crawler request options (including headers) \n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http\nresponse object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided\nas a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size\nwe're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned\nas a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a\nrequest.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as\nthe second parameter.\n* `discoverycomplete` ( queueItem, resources )\nFired when linked resources have been discovered. Passes an array of resources\n(as URLs) as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does\nnot find any more to add. This event returns no arguments.\n\n#### A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters\nan HTTP error status in the response. If you need this information, you can listen\nto simplecrawler's error events, and through node's native `data` event\n(`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let\nme know. I didn't include it because I didn't need it - but if it's important to\npeople I might put it back in. :)\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n*\t`crawler.host` -\n\tThe domain to scan. By default, simplecrawler will restrict all requests to\n\tthis domain.\n*\t`crawler.initialPath` -\n\tThe initial path with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialPort` -\n\tThe initial port with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialProtocol` -\n\tThe initial protocol with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.interval` -\n\tThe interval with which the crawler will spool up new requests (one per\n\ttick.) Defaults to 250ms.\n*\t`crawler.maxConcurrency` -\n\tThe maximum number of requests the crawler will run simultaneously. Defaults\n\tto 5 - the default number of http agents node will run.\n*\t`crawler.timeout` -\n\tThe maximum time the crawler will wait for headers before aborting the request.\n*\t`crawler.userAgent` -\n\tThe user agent the crawler will report. Defaults to\n\t`Node/SimpleCrawler <version> (http://www.github.com/cgiffard/node-simplecrawler)`.\n*\t`crawler.queue` -\n\tThe queue in use by the crawler (Must implement the `FetchQueue` interface)\n*\t`crawler.filterByDomain` -\n\tSpecifies whether the crawler will restrict queued requests to a given\n\tdomain/domains.\n*\t`crawler.scanSubdomains` -\n\tEnables scanning subdomains (other than www) as well as the specified domain.\n\tDefaults to false.\n*\t`crawler.ignoreWWWDomain` -\n\tTreats the `www` domain the same as the originally specified domain.\n\tDefaults to true.\n*\t`crawler.stripWWWDomain` -\n\tOr go even further and strip WWW subdomain from requests altogether!\n*\t`crawler.discoverResources` -\n\tUse simplecrawler's internal resource discovery function. Defaults to true.\n\t(switch it off if you'd prefer to discover and queue resources yourself!)\n*\t`crawler.cache` -\n\tSpecify a cache architecture to use when crawling. Must implement\n\t`SimpleCache` interface.\n*\t`crawler.useProxy` -\n\tThe crawler should use an HTTP proxy to make its requests.\n*\t`crawler.proxyHostname` -\n\tThe hostname of the proxy to use for requests.\n*\t`crawler.proxyPort` -\n\tThe port of the proxy to use for requests.\n*\t`crawler.domainWhitelist` -\n\tAn array of domains the crawler is permitted to crawl from. If other settings\n\tare more permissive, they will override this setting.\n*\t`crawler.supportedMimeTypes` -\n\tAn array of RegEx objects used to determine supported MIME types (types of\n\tdata simplecrawler will scan for links.) If you're  not using simplecrawler's\n\tresource discovery function, this won't have any effect.\n*\t`crawler.allowedProtocols` -\n\tAn array of RegEx objects used to determine whether a URL protocol is supported.\n\tThis is to deal with nonstandard protocol handlers that regular HTTP is\n\tsometimes given, like `feed:`. It does not provide support for non-http\n\tprotocols (and why would it!?)\n*\t`crawler.maxResourceSize` - \n\tThe maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n*\t`crawler.downloadUnsupported` -\n\tSimplecrawler will download files it can't parse. Defaults to true, but if\n\tyou'd rather save the RAM and GC lag, switch it off.\n*\t`crawler.needsAuth` -\n\tFlag to specify if the domain you are hitting requires basic authentication\n*\t`crawler.authUser` -\n\tUsername provided for needsAuth flag\n*\t`crawler.authPass` -\n\tPassword provided for needsAuth flag\n*\t`crawler.customHeaders` -\n\tAn object specifying a number of custom headers simplecrawler will add to\n\tevery request. These override the default headers simplecrawler sets, so\n\tbe careful with them. If you want to tamper with headers on a per-request basis,\n\tsee the `fetchqueue` event.\n*\t`crawler.acceptCookies` -\n\tFlag to indicate if the crawler should hold on to cookies\n*\t`crawler.urlEncoding` -\n\tSet this to `iso8859` to trigger URIjs' re-encoding of iso8859 URLs to unicode.\n\tDefaults to `unicode`.\n\n#### Excluding certain resources from downloading\n\nSimplecrawler has a mechanism you can use to prevent certain resources from being\nfetched, based on the URL, called *Fetch Conditions**. A fetch condition is just\na function, which, when given a parsed URL object, will return a true or a false\nvalue, indicating whether a given resource should be downloaded.\n\nYou may add as many fetch conditions as you like, and remove them at runtime.\nSimplecrawler will evaluate every single condition against every queued URL, and\nshould just one of them return a falsy value (this includes null and undefined,\nso remember to always return a value!) then the resource in question will not be\nfetched.\n\n##### Adding a fetch condition\n\nThis example fetch condition prevents URLs ending in `.pdf` from downloading.\nAdding a fetch condition assigns it an ID, which the `addFetchCondition` function\nreturns. You can use this ID to remove the condition later.\n\n```javascript\nvar conditionID = myCrawler.addFetchCondition(function(parsedURL) {\n\treturn !parsedURL.path.match(/\\.pdf$/i);\n});\n```\n\nNOTE: simplecrawler uses slightly different terminology to URIjs. `parsedURL.path`\nincludes the query string too. If you want the path without the query string,\nuse `parsedURL.uriPath`.\n\n##### Removing a fetch condition\n\nIf you stored the ID of the fetch condition you added earlier, you can remove it\nfrom the crawler:\n\n```javascript\nmyCrawler.removeFetchCondition(conditionID);\n```\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed\nat `crawler.queue` (assuming you called your Crawler() object `crawler`.) It\nprovides array access, so you can get to queue items just with array notation\nand an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate\ninterface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nThe simplest way to add to the queue is to use the crawler's own method,\n`crawler.queueURL`. This method takes a complete URL, validates and deconstructs\nit, and adds it to the queue.\n\nIf you instead want to add a resource by its components, you may call the\n`queue.add` method directly:\n\n```javascript\ncrawler.queue.add(protocol,hostname,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can\nremember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items,\nit helps to know what's inside them. These are the properties every queue item\nis expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `host` - The full domain/hostname of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The\nupside is, the queue actually has some convenient functions for getting simple\naggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network\nperformance of your crawl (so far.) This is done live, so don't check it thirty\ntimes a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the\n`crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions\nrespectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given\nstatus at any one time, and/or retreive them. That's easy with\n`crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.getwithStatus` returns the number of queued items with a given\nstatus, while `crawler.queue.getWithStatus` returns an array of the queue items\nthemselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n*\t`crawler.queue.complete` - returns the number of queue items which have been\n\tcompleted (marked as fetched)\n*\t`crawler.queue.errors` - returns the number of requests which have failed\n\t(404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if\nyour application fails or you need to abort the crawl for some reason. (Perhaps\nyou just want to finish off for the night and pick it up tomorrow!) The\n`crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be\nasynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when\nyou need to save the queue - don't call them every request or your application's\nperformance will be incredibly poor - they block like *crazy*. That said, using\nthem when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n## Cookies\n\nSimplecrawler now has an internal cookie jar, which collects and resends cookies\nautomatically, and by default.\n\nIf you want to turn this off, set the `crawler.acceptCookies` option to `false`.\n\nThe cookie jar is accessible via `crawler.cookies`, and is an event emitter itself:\n\n### Cookie Events\n\n* `addcookie` ( cookie )\nFired when a new cookie is added to the jar.\n* `removecookie` ( cookie array )\nFired when one or more cookies are removed from the jar.\n\n## Building and Testing\n\n#### Build Status:\n\n* Master: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n* Development: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=development)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\n## Contributors\n\nI'd like to extend sincere thanks to:\n\n*\t[Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support, and\n\tinitial cookie support.\n*\t[Mike Moulton](https://github.com/mmoulton) for\n\t[fixing a bug in the URL discovery mechanism]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/3), as well as\n\t[adding the `discoverycomplete` event]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/10),\n*\t[Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword\n\tnaming collision with node 0.8's EventEmitter.\n*\t[Greg Molnar](https://github.com/gregmolnar) for\n\t[adding a querystring-free path parameter to parsed URL objects.]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/31)\n*\t[Breck Yunits](https://github.com/breck7) for contributing a useful code\n\tsample demonstrating using simplecrawler for caching a website to disk!\n*\t[Luke Plaster](https://github.com/notatestuser) for enabling protocol-agnostic\n\tlink discovery\n*\t[Jellyfrog](https://github.com/jellyfrog) for assisting in diagnosing some\n\tnasty EventEmitter issues.\n\nAnd everybody else who has helped out in some way! :)\n\n## Licence\n\nCopyright (c) 2012, Christopher Giffard.\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, \nare permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n* Redistributions in binary form must reproduce the above copyright notice, this\n  list of conditions and the following disclaimer in the documentation and/or\n  other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR \nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","readmeFilename":"README.markdown","_id":"simplecrawler@0.2.5","dist":{"shasum":"abd1235079a161b1d438279d16d8e0028eeca8b8","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.2.5.tgz"},"_from":".","_npmVersion":"1.2.18","_npmUser":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},{"name":"cgiffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.2.6":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.2.6","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"scripts":{"test":"mocha -R spec -t 4000"},"bin":{"crawl":"./lib/cli.js"},"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./lib/index.js","engines":{"node":">=0.4.0"},"devDependencies":{"mocha":"~1.8.1","jshint":"~0.7.x","chai":"~1.2.0"},"dependencies":{"URIjs":"~1.8.3"},"readme":"# Simple web-crawler for node.js [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\nSimplecrawler is designed to provide the most basic possible API for crawling\nwebsites, while being as flexible and robust as possible. I wrote simplecrawler\nto archive, analyse, and search some very large websites. It has happily chewed\nthrough 50,000 pages and written tens of gigabytes to disk without issue.\n\n#### Example (simple mode)\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n\nCrawler.crawl(\"http://example.com/\")\n\t.on(\"fetchcomplete\",function(queueItem){\n\t\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n\t});\n```\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can\nreplace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except\nwhen discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nThere are two ways of instantiating a new crawler - a simple but less flexible\nmethod inspired by [anemone](http://anemone.rubyforge.org), and the traditional\nmethod which provides a little more room to configure crawl parameters.\n\nRegardless of wether you use the simple or traditional methods of instantiation,\nyou'll need to require simplecrawler:\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n```\n\n#### Simple Mode\n\nSimple mode generates a new crawler for you, preconfigures it based on a URL you\nprovide, and returns the crawler to you for further configuration and so you can\nattach event handlers.\n\nSimply call `Crawler.crawl`, with a URL first parameter, and two optional\nfunctions that will be added as event listeners for `fetchcomplete` and\n`fetcherror` respectively.\n\n```javascript\nCrawler.crawl(\"http://example.com/\", function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\nAlternately, if you decide to omit these functions, you can use the returned\ncrawler object to add the event listeners yourself, and tweak configuration\noptions:\n\n```javascript\nvar crawler = Crawler.crawl(\"http://example.com/\");\n\ncrawler.interval = 500;\n\ncrawler.on(\"fetchcomplete\",function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\n#### Advanced Mode\n\nThe alternative method of creating a crawler is to call the `simplecrawler`\nconstructor yourself, and to initiate the crawl manually.\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your web\nserver. Decrease the concurrency from five simultaneous requests - and increase\nthe request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when\ncreating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run\nthrough its queue finding linked resources on the domain to download, until it\ncan't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `crawlstart`\nFired when the crawl begins or is restarted.\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually\nqueue an item yourself.)\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem , requestOptions )\nFired when an item is spooled for fetching. If your event handler is synchronous,\nyou can modify the crawler request options (including headers) \n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http\nresponse object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided\nas a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size\nwe're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned\nas a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a\nrequest.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as\nthe second parameter.\n* `discoverycomplete` ( queueItem, resources )\nFired when linked resources have been discovered. Passes an array of resources\n(as URLs) as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does\nnot find any more to add. This event returns no arguments.\n\n#### A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters\nan HTTP error status in the response. If you need this information, you can listen\nto simplecrawler's error events, and through node's native `data` event\n(`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let\nme know. I didn't include it because I didn't need it - but if it's important to\npeople I might put it back in. :)\n\n#### Asynchronous Event Listeners\n\nBy default, all of these events, with the exception of `complete`, provide a\nfinal `asyncCallback` parameter, which is a function.\n\nIf your function is configured to receive this parameter in its function definition,\nsimplecrawler will treat this function as asynchronous, and will not consider it\ncompleted until you call the `asyncCallback` function provided.\n\nThe crawler will continue crawling while your event handler finishes up, but it\nwon't call the `complete` event and clean up its timer until every asynchronous\ncallback is complete.\n\n##### Example Asynchronous Event Listener\n\n```javascript\ncrawler.on(\"fetchcomplete\",function(queueItem,data,res,done) {\n\tdoSomeDiscovery(data,function(foundURLs){\n\t\tfoundURLs.forEach(crawler.queueURL.bind(crawler));\n\t\tdone();\n\t});\n});\n```\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n*\t`crawler.host` -\n\tThe domain to scan. By default, simplecrawler will restrict all requests to\n\tthis domain.\n*\t`crawler.initialPath` -\n\tThe initial path with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialPort` -\n\tThe initial port with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialProtocol` -\n\tThe initial protocol with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.interval` -\n\tThe interval with which the crawler will spool up new requests (one per\n\ttick.) Defaults to 250ms.\n*\t`crawler.maxConcurrency` -\n\tThe maximum number of requests the crawler will run simultaneously. Defaults\n\tto 5 - the default number of http agents node will run.\n*\t`crawler.timeout` -\n\tThe maximum time the crawler will wait for headers before aborting the request.\n*\t`crawler.userAgent` -\n\tThe user agent the crawler will report. Defaults to\n\t`Node/SimpleCrawler <version> (http://www.github.com/cgiffard/node-simplecrawler)`.\n*\t`crawler.queue` -\n\tThe queue in use by the crawler (Must implement the `FetchQueue` interface)\n*\t`crawler.filterByDomain` -\n\tSpecifies whether the crawler will restrict queued requests to a given\n\tdomain/domains.\n*\t`crawler.scanSubdomains` -\n\tEnables scanning subdomains (other than www) as well as the specified domain.\n\tDefaults to false.\n*\t`crawler.ignoreWWWDomain` -\n\tTreats the `www` domain the same as the originally specified domain.\n\tDefaults to true.\n*\t`crawler.stripWWWDomain` -\n\tOr go even further and strip WWW subdomain from requests altogether!\n*\t`crawler.discoverResources` -\n\tUse simplecrawler's internal resource discovery function. Defaults to true.\n\t(switch it off if you'd prefer to discover and queue resources yourself!)\n*\t`crawler.cache` -\n\tSpecify a cache architecture to use when crawling. Must implement\n\t`SimpleCache` interface.\n*\t`crawler.useProxy` -\n\tThe crawler should use an HTTP proxy to make its requests.\n*\t`crawler.proxyHostname` -\n\tThe hostname of the proxy to use for requests.\n*\t`crawler.proxyPort` -\n\tThe port of the proxy to use for requests.\n*\t`crawler.domainWhitelist` -\n\tAn array of domains the crawler is permitted to crawl from. If other settings\n\tare more permissive, they will override this setting.\n*\t`crawler.supportedMimeTypes` -\n\tAn array of RegEx objects used to determine supported MIME types (types of\n\tdata simplecrawler will scan for links.) If you're  not using simplecrawler's\n\tresource discovery function, this won't have any effect.\n*\t`crawler.allowedProtocols` -\n\tAn array of RegEx objects used to determine whether a URL protocol is supported.\n\tThis is to deal with nonstandard protocol handlers that regular HTTP is\n\tsometimes given, like `feed:`. It does not provide support for non-http\n\tprotocols (and why would it!?)\n*\t`crawler.maxResourceSize` - \n\tThe maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n*\t`crawler.downloadUnsupported` -\n\tSimplecrawler will download files it can't parse. Defaults to true, but if\n\tyou'd rather save the RAM and GC lag, switch it off.\n*\t`crawler.needsAuth` -\n\tFlag to specify if the domain you are hitting requires basic authentication\n*\t`crawler.authUser` -\n\tUsername provided for needsAuth flag\n*\t`crawler.authPass` -\n\tPassword provided for needsAuth flag\n*\t`crawler.customHeaders` -\n\tAn object specifying a number of custom headers simplecrawler will add to\n\tevery request. These override the default headers simplecrawler sets, so\n\tbe careful with them. If you want to tamper with headers on a per-request basis,\n\tsee the `fetchqueue` event.\n*\t`crawler.acceptCookies` -\n\tFlag to indicate if the crawler should hold on to cookies\n*\t`crawler.urlEncoding` -\n\tSet this to `iso8859` to trigger URIjs' re-encoding of iso8859 URLs to unicode.\n\tDefaults to `unicode`.\n\n#### Excluding certain resources from downloading\n\nSimplecrawler has a mechanism you can use to prevent certain resources from being\nfetched, based on the URL, called *Fetch Conditions**. A fetch condition is just\na function, which, when given a parsed URL object, will return a true or a false\nvalue, indicating whether a given resource should be downloaded.\n\nYou may add as many fetch conditions as you like, and remove them at runtime.\nSimplecrawler will evaluate every single condition against every queued URL, and\nshould just one of them return a falsy value (this includes null and undefined,\nso remember to always return a value!) then the resource in question will not be\nfetched.\n\n##### Adding a fetch condition\n\nThis example fetch condition prevents URLs ending in `.pdf` from downloading.\nAdding a fetch condition assigns it an ID, which the `addFetchCondition` function\nreturns. You can use this ID to remove the condition later.\n\n```javascript\nvar conditionID = myCrawler.addFetchCondition(function(parsedURL) {\n\treturn !parsedURL.path.match(/\\.pdf$/i);\n});\n```\n\nNOTE: simplecrawler uses slightly different terminology to URIjs. `parsedURL.path`\nincludes the query string too. If you want the path without the query string,\nuse `parsedURL.uriPath`.\n\n##### Removing a fetch condition\n\nIf you stored the ID of the fetch condition you added earlier, you can remove it\nfrom the crawler:\n\n```javascript\nmyCrawler.removeFetchCondition(conditionID);\n```\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed\nat `crawler.queue` (assuming you called your Crawler() object `crawler`.) It\nprovides array access, so you can get to queue items just with array notation\nand an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate\ninterface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nThe simplest way to add to the queue is to use the crawler's own method,\n`crawler.queueURL`. This method takes a complete URL, validates and deconstructs\nit, and adds it to the queue.\n\nIf you instead want to add a resource by its components, you may call the\n`queue.add` method directly:\n\n```javascript\ncrawler.queue.add(protocol,hostname,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can\nremember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items,\nit helps to know what's inside them. These are the properties every queue item\nis expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `host` - The full domain/hostname of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The\nupside is, the queue actually has some convenient functions for getting simple\naggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network\nperformance of your crawl (so far.) This is done live, so don't check it thirty\ntimes a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the\n`crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions\nrespectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given\nstatus at any one time, and/or retreive them. That's easy with\n`crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.getwithStatus` returns the number of queued items with a given\nstatus, while `crawler.queue.getWithStatus` returns an array of the queue items\nthemselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n*\t`crawler.queue.complete` - returns the number of queue items which have been\n\tcompleted (marked as fetched)\n*\t`crawler.queue.errors` - returns the number of requests which have failed\n\t(404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if\nyour application fails or you need to abort the crawl for some reason. (Perhaps\nyou just want to finish off for the night and pick it up tomorrow!) The\n`crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be\nasynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when\nyou need to save the queue - don't call them every request or your application's\nperformance will be incredibly poor - they block like *crazy*. That said, using\nthem when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n## Cookies\n\nSimplecrawler now has an internal cookie jar, which collects and resends cookies\nautomatically, and by default.\n\nIf you want to turn this off, set the `crawler.acceptCookies` option to `false`.\n\nThe cookie jar is accessible via `crawler.cookies`, and is an event emitter itself:\n\n### Cookie Events\n\n* `addcookie` ( cookie )\nFired when a new cookie is added to the jar.\n* `removecookie` ( cookie array )\nFired when one or more cookies are removed from the jar.\n\n## Building and Testing\n\n#### Build Status:\n\n* Master: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n* Development: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=development)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\n## Contributors\n\nI'd like to extend sincere thanks to:\n\n*\t[Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support, and\n\tinitial cookie support.\n*\t[Mike Moulton](https://github.com/mmoulton) for\n\t[fixing a bug in the URL discovery mechanism]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/3), as well as\n\t[adding the `discoverycomplete` event]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/10),\n*\t[Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword\n\tnaming collision with node 0.8's EventEmitter.\n*\t[Greg Molnar](https://github.com/gregmolnar) for\n\t[adding a querystring-free path parameter to parsed URL objects.]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/31)\n*\t[Breck Yunits](https://github.com/breck7) for contributing a useful code\n\tsample demonstrating using simplecrawler for caching a website to disk!\n*\t[Luke Plaster](https://github.com/notatestuser) for enabling protocol-agnostic\n\tlink discovery\n*\t[Jellyfrog](https://github.com/jellyfrog) for assisting in diagnosing some\n\tnasty EventEmitter issues.\n\nAnd everybody else who has helped out in some way! :)\n\n## Licence\n\nCopyright (c) 2012, Christopher Giffard.\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, \nare permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n* Redistributions in binary form must reproduce the above copyright notice, this\n  list of conditions and the following disclaimer in the documentation and/or\n  other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR \nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","readmeFilename":"README.markdown","_id":"simplecrawler@0.2.6","dist":{"shasum":"df1e9eef37916c4637efbf6c7f3dc7141e6e123d","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.2.6.tgz"},"_from":".","_npmVersion":"1.2.18","_npmUser":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},{"name":"cgiffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.2.7":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.2.7","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"scripts":{"test":"mocha -R spec -t 4000"},"bin":{"crawl":"./lib/cli.js"},"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./lib/index.js","engines":{"node":">=0.4.0"},"devDependencies":{"mocha":"~1.8.1","jshint":"~0.7.x","chai":"~1.2.0"},"dependencies":{"URIjs":"~1.8.3"},"readme":"# Simple web-crawler for node.js [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\nSimplecrawler is designed to provide the most basic possible API for crawling\nwebsites, while being as flexible and robust as possible. I wrote simplecrawler\nto archive, analyse, and search some very large websites. It has happily chewed\nthrough 50,000 pages and written tens of gigabytes to disk without issue.\n\n#### Example (simple mode)\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n\nCrawler.crawl(\"http://example.com/\")\n\t.on(\"fetchcomplete\",function(queueItem){\n\t\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n\t});\n```\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can\nreplace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except\nwhen discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nThere are two ways of instantiating a new crawler - a simple but less flexible\nmethod inspired by [anemone](http://anemone.rubyforge.org), and the traditional\nmethod which provides a little more room to configure crawl parameters.\n\nRegardless of wether you use the simple or traditional methods of instantiation,\nyou'll need to require simplecrawler:\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n```\n\n#### Simple Mode\n\nSimple mode generates a new crawler for you, preconfigures it based on a URL you\nprovide, and returns the crawler to you for further configuration and so you can\nattach event handlers.\n\nSimply call `Crawler.crawl`, with a URL first parameter, and two optional\nfunctions that will be added as event listeners for `fetchcomplete` and\n`fetcherror` respectively.\n\n```javascript\nCrawler.crawl(\"http://example.com/\", function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\nAlternately, if you decide to omit these functions, you can use the returned\ncrawler object to add the event listeners yourself, and tweak configuration\noptions:\n\n```javascript\nvar crawler = Crawler.crawl(\"http://example.com/\");\n\ncrawler.interval = 500;\n\ncrawler.on(\"fetchcomplete\",function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\n#### Advanced Mode\n\nThe alternative method of creating a crawler is to call the `simplecrawler`\nconstructor yourself, and to initiate the crawl manually.\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your web\nserver. Decrease the concurrency from five simultaneous requests - and increase\nthe request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when\ncreating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run\nthrough its queue finding linked resources on the domain to download, until it\ncan't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `crawlstart`\nFired when the crawl begins or is restarted.\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually\nqueue an item yourself.)\n* `queueduplicate` ( URLData )\nFired when an item cannot be added to the queue because it is already present in\nthe queue. Frequent firing of this event is normal and expected.\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem , requestOptions )\nFired when an item is spooled for fetching. If your event handler is synchronous,\nyou can modify the crawler request options (including headers) \n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http\nresponse object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided\nas a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size\nwe're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned\nas a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a\nrequest.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as\nthe second parameter.\n* `discoverycomplete` ( queueItem, resources )\nFired when linked resources have been discovered. Passes an array of resources\n(as URLs) as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does\nnot find any more to add. This event returns no arguments.\n\n#### A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters\nan HTTP error status in the response. If you need this information, you can listen\nto simplecrawler's error events, and through node's native `data` event\n(`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let\nme know. I didn't include it because I didn't need it - but if it's important to\npeople I might put it back in. :)\n\n#### Asynchronous Event Listeners\n\nBy default, all of these events, with the exception of `complete`, provide a\nfinal `asyncCallback` parameter, which is a function.\n\nIf your function is configured to receive this parameter in its function definition,\nsimplecrawler will treat this function as asynchronous, and will not consider it\ncompleted until you call the `asyncCallback` function provided.\n\nThe crawler will continue crawling while your event handler finishes up, but it\nwon't call the `complete` event and clean up its timer until every asynchronous\ncallback is complete.\n\n##### Example Asynchronous Event Listener\n\n```javascript\ncrawler.on(\"fetchcomplete\",function(queueItem,data,res,done) {\n\tdoSomeDiscovery(data,function(foundURLs){\n\t\tfoundURLs.forEach(crawler.queueURL.bind(crawler));\n\t\tdone();\n\t});\n});\n```\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n*\t`crawler.host` -\n\tThe domain to scan. By default, simplecrawler will restrict all requests to\n\tthis domain.\n*\t`crawler.initialPath` -\n\tThe initial path with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialPort` -\n\tThe initial port with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialProtocol` -\n\tThe initial protocol with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.interval` -\n\tThe interval with which the crawler will spool up new requests (one per\n\ttick.) Defaults to 250ms.\n*\t`crawler.maxConcurrency` -\n\tThe maximum number of requests the crawler will run simultaneously. Defaults\n\tto 5 - the default number of http agents node will run.\n*\t`crawler.timeout` -\n\tThe maximum time the crawler will wait for headers before aborting the request.\n*\t`crawler.userAgent` -\n\tThe user agent the crawler will report. Defaults to\n\t`Node/SimpleCrawler <version> (http://www.github.com/cgiffard/node-simplecrawler)`.\n*\t`crawler.queue` -\n\tThe queue in use by the crawler (Must implement the `FetchQueue` interface)\n*\t`crawler.filterByDomain` -\n\tSpecifies whether the crawler will restrict queued requests to a given\n\tdomain/domains.\n*\t`crawler.scanSubdomains` -\n\tEnables scanning subdomains (other than www) as well as the specified domain.\n\tDefaults to false.\n*\t`crawler.ignoreWWWDomain` -\n\tTreats the `www` domain the same as the originally specified domain.\n\tDefaults to true.\n*\t`crawler.stripWWWDomain` -\n\tOr go even further and strip WWW subdomain from requests altogether!\n*\t`crawler.discoverResources` -\n\tUse simplecrawler's internal resource discovery function. Defaults to true.\n\t(switch it off if you'd prefer to discover and queue resources yourself!)\n*\t`crawler.cache` -\n\tSpecify a cache architecture to use when crawling. Must implement\n\t`SimpleCache` interface.\n*\t`crawler.useProxy` -\n\tThe crawler should use an HTTP proxy to make its requests.\n*\t`crawler.proxyHostname` -\n\tThe hostname of the proxy to use for requests.\n*\t`crawler.proxyPort` -\n\tThe port of the proxy to use for requests.\n*\t`crawler.domainWhitelist` -\n\tAn array of domains the crawler is permitted to crawl from. If other settings\n\tare more permissive, they will override this setting.\n*\t`crawler.supportedMimeTypes` -\n\tAn array of RegEx objects used to determine supported MIME types (types of\n\tdata simplecrawler will scan for links.) If you're  not using simplecrawler's\n\tresource discovery function, this won't have any effect.\n*\t`crawler.allowedProtocols` -\n\tAn array of RegEx objects used to determine whether a URL protocol is supported.\n\tThis is to deal with nonstandard protocol handlers that regular HTTP is\n\tsometimes given, like `feed:`. It does not provide support for non-http\n\tprotocols (and why would it!?)\n*\t`crawler.maxResourceSize` - \n\tThe maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n*\t`crawler.downloadUnsupported` -\n\tSimplecrawler will download files it can't parse. Defaults to true, but if\n\tyou'd rather save the RAM and GC lag, switch it off.\n*\t`crawler.needsAuth` -\n\tFlag to specify if the domain you are hitting requires basic authentication\n*\t`crawler.authUser` -\n\tUsername provided for needsAuth flag\n*\t`crawler.authPass` -\n\tPassword provided for needsAuth flag\n*\t`crawler.customHeaders` -\n\tAn object specifying a number of custom headers simplecrawler will add to\n\tevery request. These override the default headers simplecrawler sets, so\n\tbe careful with them. If you want to tamper with headers on a per-request basis,\n\tsee the `fetchqueue` event.\n*\t`crawler.acceptCookies` -\n\tFlag to indicate if the crawler should hold on to cookies\n*\t`crawler.urlEncoding` -\n\tSet this to `iso8859` to trigger URIjs' re-encoding of iso8859 URLs to unicode.\n\tDefaults to `unicode`.\n\n#### Excluding certain resources from downloading\n\nSimplecrawler has a mechanism you can use to prevent certain resources from being\nfetched, based on the URL, called *Fetch Conditions**. A fetch condition is just\na function, which, when given a parsed URL object, will return a true or a false\nvalue, indicating whether a given resource should be downloaded.\n\nYou may add as many fetch conditions as you like, and remove them at runtime.\nSimplecrawler will evaluate every single condition against every queued URL, and\nshould just one of them return a falsy value (this includes null and undefined,\nso remember to always return a value!) then the resource in question will not be\nfetched.\n\n##### Adding a fetch condition\n\nThis example fetch condition prevents URLs ending in `.pdf` from downloading.\nAdding a fetch condition assigns it an ID, which the `addFetchCondition` function\nreturns. You can use this ID to remove the condition later.\n\n```javascript\nvar conditionID = myCrawler.addFetchCondition(function(parsedURL) {\n\treturn !parsedURL.path.match(/\\.pdf$/i);\n});\n```\n\nNOTE: simplecrawler uses slightly different terminology to URIjs. `parsedURL.path`\nincludes the query string too. If you want the path without the query string,\nuse `parsedURL.uriPath`.\n\n##### Removing a fetch condition\n\nIf you stored the ID of the fetch condition you added earlier, you can remove it\nfrom the crawler:\n\n```javascript\nmyCrawler.removeFetchCondition(conditionID);\n```\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed\nat `crawler.queue` (assuming you called your Crawler() object `crawler`.) It\nprovides array access, so you can get to queue items just with array notation\nand an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate\ninterface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nThe simplest way to add to the queue is to use the crawler's own method,\n`crawler.queueURL`. This method takes a complete URL, validates and deconstructs\nit, and adds it to the queue.\n\nIf you instead want to add a resource by its components, you may call the\n`queue.add` method directly:\n\n```javascript\ncrawler.queue.add(protocol,hostname,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can\nremember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items,\nit helps to know what's inside them. These are the properties every queue item\nis expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `host` - The full domain/hostname of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The\nupside is, the queue actually has some convenient functions for getting simple\naggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network\nperformance of your crawl (so far.) This is done live, so don't check it thirty\ntimes a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the\n`crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions\nrespectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given\nstatus at any one time, and/or retreive them. That's easy with\n`crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.getwithStatus` returns the number of queued items with a given\nstatus, while `crawler.queue.getWithStatus` returns an array of the queue items\nthemselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n*\t`crawler.queue.complete` - returns the number of queue items which have been\n\tcompleted (marked as fetched)\n*\t`crawler.queue.errors` - returns the number of requests which have failed\n\t(404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if\nyour application fails or you need to abort the crawl for some reason. (Perhaps\nyou just want to finish off for the night and pick it up tomorrow!) The\n`crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be\nasynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when\nyou need to save the queue - don't call them every request or your application's\nperformance will be incredibly poor - they block like *crazy*. That said, using\nthem when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n## Cookies\n\nSimplecrawler now has an internal cookie jar, which collects and resends cookies\nautomatically, and by default.\n\nIf you want to turn this off, set the `crawler.acceptCookies` option to `false`.\n\nThe cookie jar is accessible via `crawler.cookies`, and is an event emitter itself:\n\n### Cookie Events\n\n* `addcookie` ( cookie )\nFired when a new cookie is added to the jar.\n* `removecookie` ( cookie array )\nFired when one or more cookies are removed from the jar.\n\n## Building and Testing\n\n#### Build Status:\n\n* Master: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n* Development: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=development)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\n## Contributors\n\nI'd like to extend sincere thanks to:\n\n*\t[Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support, and\n\tinitial cookie support.\n*\t[Mike Moulton](https://github.com/mmoulton) for\n\t[fixing a bug in the URL discovery mechanism]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/3), as well as\n\t[adding the `discoverycomplete` event]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/10),\n*\t[Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword\n\tnaming collision with node 0.8's EventEmitter.\n*\t[Greg Molnar](https://github.com/gregmolnar) for\n\t[adding a querystring-free path parameter to parsed URL objects.]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/31)\n*\t[Breck Yunits](https://github.com/breck7) for contributing a useful code\n\tsample demonstrating using simplecrawler for caching a website to disk!\n*\t[Luke Plaster](https://github.com/notatestuser) for enabling protocol-agnostic\n\tlink discovery\n*\t[Zeus](https://github.com/distracteddev) for fixing a bug where [default port\n\tinfo was wrongly specified in requests]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/40)\n*\t[Jellyfrog](https://github.com/jellyfrog) for assisting in diagnosing some\n\tnasty EventEmitter issues.\n\nAnd everybody else who has helped out in some way! :)\n\n## Licence\n\nCopyright (c) 2012, Christopher Giffard.\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, \nare permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n* Redistributions in binary form must reproduce the above copyright notice, this\n  list of conditions and the following disclaimer in the documentation and/or\n  other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR \nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","readmeFilename":"README.markdown","_id":"simplecrawler@0.2.7","dist":{"shasum":"83ef7d17f84764b15bd96c700f4117d04bef2312","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.2.7.tgz"},"_from":".","_npmVersion":"1.2.21","_npmUser":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},{"name":"cgiffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.2.8":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.2.8","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"scripts":{"test":"mocha -R spec -t 5000"},"bin":{"crawl":"./lib/cli.js"},"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./lib/index.js","engines":{"node":">=0.4.0"},"devDependencies":{"mocha":"~1.8.1","jshint":"~0.7.x","chai":"~1.2.0"},"dependencies":{"URIjs":"~1.10.2"},"readme":"# Simple web-crawler for node.js [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\nSimplecrawler is designed to provide the most basic possible API for crawling\nwebsites, while being as flexible and robust as possible. I wrote simplecrawler\nto archive, analyse, and search some very large websites. It has happily chewed\nthrough 50,000 pages and written tens of gigabytes to disk without issue.\n\n#### Example (simple mode)\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n\nCrawler.crawl(\"http://example.com/\")\n\t.on(\"fetchcomplete\",function(queueItem){\n\t\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n\t});\n```\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can\nreplace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except\nwhen discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nThere are two ways of instantiating a new crawler - a simple but less flexible\nmethod inspired by [anemone](http://anemone.rubyforge.org), and the traditional\nmethod which provides a little more room to configure crawl parameters.\n\nRegardless of wether you use the simple or traditional methods of instantiation,\nyou'll need to require simplecrawler:\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n```\n\n#### Simple Mode\n\nSimple mode generates a new crawler for you, preconfigures it based on a URL you\nprovide, and returns the crawler to you for further configuration and so you can\nattach event handlers.\n\nSimply call `Crawler.crawl`, with a URL first parameter, and two optional\nfunctions that will be added as event listeners for `fetchcomplete` and\n`fetcherror` respectively.\n\n```javascript\nCrawler.crawl(\"http://example.com/\", function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\nAlternately, if you decide to omit these functions, you can use the returned\ncrawler object to add the event listeners yourself, and tweak configuration\noptions:\n\n```javascript\nvar crawler = Crawler.crawl(\"http://example.com/\");\n\ncrawler.interval = 500;\n\ncrawler.on(\"fetchcomplete\",function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\n#### Advanced Mode\n\nThe alternative method of creating a crawler is to call the `simplecrawler`\nconstructor yourself, and to initiate the crawl manually.\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your web\nserver. Decrease the concurrency from five simultaneous requests - and increase\nthe request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when\ncreating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run\nthrough its queue finding linked resources on the domain to download, until it\ncan't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `crawlstart`\nFired when the crawl begins or is restarted.\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually\nqueue an item yourself.)\n* `queueduplicate` ( URLData )\nFired when an item cannot be added to the queue because it is already present in\nthe queue. Frequent firing of this event is normal and expected.\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem , requestOptions )\nFired when an item is spooled for fetching. If your event handler is synchronous,\nyou can modify the crawler request options (including headers) \n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http\nresponse object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided\nas a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size\nwe're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned\nas a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a\nrequest.\n* `fetchtimeout` ( queueItem, crawlerTimeoutValue )\nFired when a request time exceeds the internal crawler threshold.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as\nthe second parameter.\n* `discoverycomplete` ( queueItem, resources )\nFired when linked resources have been discovered. Passes an array of resources\n(as URLs) as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does\nnot find any more to add. This event returns no arguments.\n\n#### A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters\nan HTTP error status in the response. If you need this information, you can listen\nto simplecrawler's error events, and through node's native `data` event\n(`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let\nme know. I didn't include it because I didn't need it - but if it's important to\npeople I might put it back in. :)\n\n#### Asynchronous Event Listeners\n\nBy default, all of these events, with the exception of `complete`, provide a\nfinal `asyncCallback` parameter, which is a function.\n\nIf your function is configured to receive this parameter in its function definition,\nsimplecrawler will treat this function as asynchronous, and will not consider it\ncompleted until you call the `asyncCallback` function provided.\n\nThe crawler will continue crawling while your event handler finishes up, but it\nwon't call the `complete` event and clean up its timer until every asynchronous\ncallback is complete.\n\n##### Example Asynchronous Event Listener\n\n```javascript\ncrawler.on(\"fetchcomplete\",function(queueItem,data,res,done) {\n\tdoSomeDiscovery(data,function(foundURLs){\n\t\tfoundURLs.forEach(crawler.queueURL.bind(crawler));\n\t\tdone();\n\t});\n});\n```\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n*\t`crawler.host` -\n\tThe domain to scan. By default, simplecrawler will restrict all requests to\n\tthis domain.\n*\t`crawler.initialPath` -\n\tThe initial path with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialPort` -\n\tThe initial port with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialProtocol` -\n\tThe initial protocol with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.interval` -\n\tThe interval with which the crawler will spool up new requests (one per\n\ttick.) Defaults to 250ms.\n*\t`crawler.maxConcurrency` -\n\tThe maximum number of requests the crawler will run simultaneously. Defaults\n\tto 5 - the default number of http agents node will run.\n*\t`crawler.timeout` -\n\tThe maximum time the crawler will wait for headers before aborting the request.\n*\t`crawler.userAgent` -\n\tThe user agent the crawler will report. Defaults to\n\t`Node/SimpleCrawler <version> (http://www.github.com/cgiffard/node-simplecrawler)`.\n*\t`crawler.queue` -\n\tThe queue in use by the crawler (Must implement the `FetchQueue` interface)\n*\t`crawler.filterByDomain` -\n\tSpecifies whether the crawler will restrict queued requests to a given\n\tdomain/domains.\n*\t`crawler.scanSubdomains` -\n\tEnables scanning subdomains (other than www) as well as the specified domain.\n\tDefaults to false.\n*\t`crawler.ignoreWWWDomain` -\n\tTreats the `www` domain the same as the originally specified domain.\n\tDefaults to true.\n*\t`crawler.stripWWWDomain` -\n\tOr go even further and strip WWW subdomain from requests altogether!\n*\t`crawler.discoverResources` -\n\tUse simplecrawler's internal resource discovery function. Defaults to true.\n\t(switch it off if you'd prefer to discover and queue resources yourself!)\n*\t`crawler.cache` -\n\tSpecify a cache architecture to use when crawling. Must implement\n\t`SimpleCache` interface.\n*\t`crawler.useProxy` -\n\tThe crawler should use an HTTP proxy to make its requests.\n*\t`crawler.proxyHostname` -\n\tThe hostname of the proxy to use for requests.\n*\t`crawler.proxyPort` -\n\tThe port of the proxy to use for requests.\n*\t`crawler.domainWhitelist` -\n\tAn array of domains the crawler is permitted to crawl from. If other settings\n\tare more permissive, they will override this setting.\n*\t`crawler.supportedMimeTypes` -\n\tAn array of RegEx objects used to determine supported MIME types (types of\n\tdata simplecrawler will scan for links.) If you're  not using simplecrawler's\n\tresource discovery function, this won't have any effect.\n*\t`crawler.allowedProtocols` -\n\tAn array of RegEx objects used to determine whether a URL protocol is supported.\n\tThis is to deal with nonstandard protocol handlers that regular HTTP is\n\tsometimes given, like `feed:`. It does not provide support for non-http\n\tprotocols (and why would it!?)\n*\t`crawler.maxResourceSize` - \n\tThe maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n*\t`crawler.downloadUnsupported` -\n\tSimplecrawler will download files it can't parse. Defaults to true, but if\n\tyou'd rather save the RAM and GC lag, switch it off.\n*\t`crawler.needsAuth` -\n\tFlag to specify if the domain you are hitting requires basic authentication\n*\t`crawler.authUser` -\n\tUsername provided for needsAuth flag\n*\t`crawler.authPass` -\n\tPassword provided for needsAuth flag\n*\t`crawler.customHeaders` -\n\tAn object specifying a number of custom headers simplecrawler will add to\n\tevery request. These override the default headers simplecrawler sets, so\n\tbe careful with them. If you want to tamper with headers on a per-request basis,\n\tsee the `fetchqueue` event.\n*\t`crawler.acceptCookies` -\n\tFlag to indicate if the crawler should hold on to cookies\n*\t`crawler.urlEncoding` -\n\tSet this to `iso8859` to trigger URIjs' re-encoding of iso8859 URLs to unicode.\n\tDefaults to `unicode`.\n\n#### Excluding certain resources from downloading\n\nSimplecrawler has a mechanism you can use to prevent certain resources from being\nfetched, based on the URL, called *Fetch Conditions**. A fetch condition is just\na function, which, when given a parsed URL object, will return a true or a false\nvalue, indicating whether a given resource should be downloaded.\n\nYou may add as many fetch conditions as you like, and remove them at runtime.\nSimplecrawler will evaluate every single condition against every queued URL, and\nshould just one of them return a falsy value (this includes null and undefined,\nso remember to always return a value!) then the resource in question will not be\nfetched.\n\n##### Adding a fetch condition\n\nThis example fetch condition prevents URLs ending in `.pdf` from downloading.\nAdding a fetch condition assigns it an ID, which the `addFetchCondition` function\nreturns. You can use this ID to remove the condition later.\n\n```javascript\nvar conditionID = myCrawler.addFetchCondition(function(parsedURL) {\n\treturn !parsedURL.path.match(/\\.pdf$/i);\n});\n```\n\nNOTE: simplecrawler uses slightly different terminology to URIjs. `parsedURL.path`\nincludes the query string too. If you want the path without the query string,\nuse `parsedURL.uriPath`.\n\n##### Removing a fetch condition\n\nIf you stored the ID of the fetch condition you added earlier, you can remove it\nfrom the crawler:\n\n```javascript\nmyCrawler.removeFetchCondition(conditionID);\n```\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed\nat `crawler.queue` (assuming you called your Crawler() object `crawler`.) It\nprovides array access, so you can get to queue items just with array notation\nand an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate\ninterface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nThe simplest way to add to the queue is to use the crawler's own method,\n`crawler.queueURL`. This method takes a complete URL, validates and deconstructs\nit, and adds it to the queue.\n\nIf you instead want to add a resource by its components, you may call the\n`queue.add` method directly:\n\n```javascript\ncrawler.queue.add(protocol,hostname,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can\nremember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items,\nit helps to know what's inside them. These are the properties every queue item\nis expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `host` - The full domain/hostname of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The\nupside is, the queue actually has some convenient functions for getting simple\naggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network\nperformance of your crawl (so far.) This is done live, so don't check it thirty\ntimes a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the\n`crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions\nrespectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given\nstatus at any one time, and/or retreive them. That's easy with\n`crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.getwithStatus` returns the number of queued items with a given\nstatus, while `crawler.queue.getWithStatus` returns an array of the queue items\nthemselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n*\t`crawler.queue.complete` - returns the number of queue items which have been\n\tcompleted (marked as fetched)\n*\t`crawler.queue.errors` - returns the number of requests which have failed\n\t(404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if\nyour application fails or you need to abort the crawl for some reason. (Perhaps\nyou just want to finish off for the night and pick it up tomorrow!) The\n`crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be\nasynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when\nyou need to save the queue - don't call them every request or your application's\nperformance will be incredibly poor - they block like *crazy*. That said, using\nthem when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n## Cookies\n\nSimplecrawler now has an internal cookie jar, which collects and resends cookies\nautomatically, and by default.\n\nIf you want to turn this off, set the `crawler.acceptCookies` option to `false`.\n\nThe cookie jar is accessible via `crawler.cookies`, and is an event emitter itself:\n\n### Cookie Events\n\n* `addcookie` ( cookie )\nFired when a new cookie is added to the jar.\n* `removecookie` ( cookie array )\nFired when one or more cookies are removed from the jar.\n\n## Building and Testing\n\n#### Build Status:\n\n* Master: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n* Development: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=development)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\n## Contributors\n\nI'd like to extend sincere thanks to:\n\n*\t[Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support, and\n\tinitial cookie support.\n*\t[Mike Moulton](https://github.com/mmoulton) for\n\t[fixing a bug in the URL discovery mechanism]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/3), as well as\n\t[adding the `discoverycomplete` event]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/10),\n*\t[Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword\n\tnaming collision with node 0.8's EventEmitter.\n*\t[Greg Molnar](https://github.com/gregmolnar) for\n\t[adding a querystring-free path parameter to parsed URL objects.]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/31)\n*\t[Breck Yunits](https://github.com/breck7) for contributing a useful code\n\tsample demonstrating using simplecrawler for caching a website to disk!\n*\t[Luke Plaster](https://github.com/notatestuser) for enabling protocol-agnostic\n\tlink discovery\n*\t[Zeus](https://github.com/distracteddev) for fixing a bug where [default port\n\tinfo was wrongly specified in requests]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/40)\n\tand for fixing the missing request timeout handling!\n*\t[Jellyfrog](https://github.com/jellyfrog) for assisting in diagnosing some\n\tnasty EventEmitter issues.\n\nAnd everybody else who has helped out in some way! :)\n\n## Licence\n\nCopyright (c) 2012, Christopher Giffard.\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, \nare permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n* Redistributions in binary form must reproduce the above copyright notice, this\n  list of conditions and the following disclaimer in the documentation and/or\n  other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR \nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.","readmeFilename":"README.markdown","_id":"simplecrawler@0.2.8","dist":{"shasum":"89b64d035657f8953a1430cac87d720fa026eb0d","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.2.8.tgz"},"_from":".","_npmVersion":"1.2.21","_npmUser":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},{"name":"cgiffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.2.9":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.2.9","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"scripts":{"test":"mocha -R spec -t 5000"},"bin":{"crawl":"./lib/cli.js"},"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./lib/index.js","engines":{"node":">=0.4.0"},"devDependencies":{"mocha":"~1.8.1","jshint":"~0.7.x","chai":"~1.2.0"},"dependencies":{"URIjs":"~1.10.2"},"readme":"# Simple web-crawler for node.js [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\nSimplecrawler is designed to provide the most basic possible API for crawling\nwebsites, while being as flexible and robust as possible. I wrote simplecrawler\nto archive, analyse, and search some very large websites. It has happily chewed\nthrough 50,000 pages and written tens of gigabytes to disk without issue.\n\n#### Example (simple mode)\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n\nCrawler.crawl(\"http://example.com/\")\n\t.on(\"fetchcomplete\",function(queueItem){\n\t\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n\t});\n```\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can\nreplace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except\nwhen discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nThere are two ways of instantiating a new crawler - a simple but less flexible\nmethod inspired by [anemone](http://anemone.rubyforge.org), and the traditional\nmethod which provides a little more room to configure crawl parameters.\n\nRegardless of wether you use the simple or traditional methods of instantiation,\nyou'll need to require simplecrawler:\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n```\n\n#### Simple Mode\n\nSimple mode generates a new crawler for you, preconfigures it based on a URL you\nprovide, and returns the crawler to you for further configuration and so you can\nattach event handlers.\n\nSimply call `Crawler.crawl`, with a URL first parameter, and two optional\nfunctions that will be added as event listeners for `fetchcomplete` and\n`fetcherror` respectively.\n\n```javascript\nCrawler.crawl(\"http://example.com/\", function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\nAlternately, if you decide to omit these functions, you can use the returned\ncrawler object to add the event listeners yourself, and tweak configuration\noptions:\n\n```javascript\nvar crawler = Crawler.crawl(\"http://example.com/\");\n\ncrawler.interval = 500;\n\ncrawler.on(\"fetchcomplete\",function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\n#### Advanced Mode\n\nThe alternative method of creating a crawler is to call the `simplecrawler`\nconstructor yourself, and to initiate the crawl manually.\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your web\nserver. Decrease the concurrency from five simultaneous requests - and increase\nthe request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when\ncreating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run\nthrough its queue finding linked resources on the domain to download, until it\ncan't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `crawlstart`\nFired when the crawl begins or is restarted.\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually\nqueue an item yourself.)\n* `queueduplicate` ( URLData )\nFired when an item cannot be added to the queue because it is already present in\nthe queue. Frequent firing of this event is normal and expected.\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem , requestOptions )\nFired when an item is spooled for fetching. If your event handler is synchronous,\nyou can modify the crawler request options (including headers) \n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http\nresponse object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided\nas a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size\nwe're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned\nas a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a\nrequest.\n* `fetchtimeout` ( queueItem, crawlerTimeoutValue )\nFired when a request time exceeds the internal crawler threshold.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as\nthe second parameter.\n* `discoverycomplete` ( queueItem, resources )\nFired when linked resources have been discovered. Passes an array of resources\n(as URLs) as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does\nnot find any more to add. This event returns no arguments.\n\n#### A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters\nan HTTP error status in the response. If you need this information, you can listen\nto simplecrawler's error events, and through node's native `data` event\n(`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let\nme know. I didn't include it because I didn't need it - but if it's important to\npeople I might put it back in. :)\n\n#### Asynchronous Event Listeners\n\nBy default, all of these events, with the exception of `complete`, provide a\nfinal `asyncCallback` parameter, which is a function.\n\nIf your function is configured to receive this parameter in its function definition,\nsimplecrawler will treat this function as asynchronous, and will not consider it\ncompleted until you call the `asyncCallback` function provided.\n\nThe crawler will continue crawling while your event handler finishes up, but it\nwon't call the `complete` event and clean up its timer until every asynchronous\ncallback is complete.\n\n##### Example Asynchronous Event Listener\n\n```javascript\ncrawler.on(\"fetchcomplete\",function(queueItem,data,res,done) {\n\tdoSomeDiscovery(data,function(foundURLs){\n\t\tfoundURLs.forEach(crawler.queueURL.bind(crawler));\n\t\tdone();\n\t});\n});\n```\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n*\t`crawler.host` -\n\tThe domain to scan. By default, simplecrawler will restrict all requests to\n\tthis domain.\n*\t`crawler.initialPath` -\n\tThe initial path with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialPort` -\n\tThe initial port with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialProtocol` -\n\tThe initial protocol with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.interval` -\n\tThe interval with which the crawler will spool up new requests (one per\n\ttick.) Defaults to 250ms.\n*\t`crawler.maxConcurrency` -\n\tThe maximum number of requests the crawler will run simultaneously. Defaults\n\tto 5 - the default number of http agents node will run.\n*\t`crawler.timeout` -\n\tThe maximum time the crawler will wait for headers before aborting the request.\n*\t`crawler.userAgent` -\n\tThe user agent the crawler will report. Defaults to\n\t`Node/SimpleCrawler <version> (http://www.github.com/cgiffard/node-simplecrawler)`.\n*\t`crawler.queue` -\n\tThe queue in use by the crawler (Must implement the `FetchQueue` interface)\n*\t`crawler.filterByDomain` -\n\tSpecifies whether the crawler will restrict queued requests to a given\n\tdomain/domains.\n*\t`crawler.scanSubdomains` -\n\tEnables scanning subdomains (other than www) as well as the specified domain.\n\tDefaults to false.\n*\t`crawler.ignoreWWWDomain` -\n\tTreats the `www` domain the same as the originally specified domain.\n\tDefaults to true.\n*\t`crawler.stripWWWDomain` -\n\tOr go even further and strip WWW subdomain from requests altogether!\n*\t`crawler.discoverResources` -\n\tUse simplecrawler's internal resource discovery function. Defaults to true.\n\t(switch it off if you'd prefer to discover and queue resources yourself!)\n*\t`crawler.cache` -\n\tSpecify a cache architecture to use when crawling. Must implement\n\t`SimpleCache` interface.\n*\t`crawler.useProxy` -\n\tThe crawler should use an HTTP proxy to make its requests.\n*\t`crawler.proxyHostname` -\n\tThe hostname of the proxy to use for requests.\n*\t`crawler.proxyPort` -\n\tThe port of the proxy to use for requests.\n*\t`crawler.domainWhitelist` -\n\tAn array of domains the crawler is permitted to crawl from. If other settings\n\tare more permissive, they will override this setting.\n*\t`crawler.supportedMimeTypes` -\n\tAn array of RegEx objects used to determine supported MIME types (types of\n\tdata simplecrawler will scan for links.) If you're  not using simplecrawler's\n\tresource discovery function, this won't have any effect.\n*\t`crawler.allowedProtocols` -\n\tAn array of RegEx objects used to determine whether a URL protocol is supported.\n\tThis is to deal with nonstandard protocol handlers that regular HTTP is\n\tsometimes given, like `feed:`. It does not provide support for non-http\n\tprotocols (and why would it!?)\n*\t`crawler.maxResourceSize` - \n\tThe maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n*\t`crawler.downloadUnsupported` -\n\tSimplecrawler will download files it can't parse. Defaults to true, but if\n\tyou'd rather save the RAM and GC lag, switch it off.\n*\t`crawler.needsAuth` -\n\tFlag to specify if the domain you are hitting requires basic authentication\n*\t`crawler.authUser` -\n\tUsername provided for needsAuth flag\n*\t`crawler.authPass` -\n\tPassword provided for needsAuth flag\n*\t`crawler.customHeaders` -\n\tAn object specifying a number of custom headers simplecrawler will add to\n\tevery request. These override the default headers simplecrawler sets, so\n\tbe careful with them. If you want to tamper with headers on a per-request basis,\n\tsee the `fetchqueue` event.\n*\t`crawler.acceptCookies` -\n\tFlag to indicate if the crawler should hold on to cookies\n*\t`crawler.urlEncoding` -\n\tSet this to `iso8859` to trigger URIjs' re-encoding of iso8859 URLs to unicode.\n\tDefaults to `unicode`.\n\n#### Excluding certain resources from downloading\n\nSimplecrawler has a mechanism you can use to prevent certain resources from being\nfetched, based on the URL, called *Fetch Conditions**. A fetch condition is just\na function, which, when given a parsed URL object, will return a true or a false\nvalue, indicating whether a given resource should be downloaded.\n\nYou may add as many fetch conditions as you like, and remove them at runtime.\nSimplecrawler will evaluate every single condition against every queued URL, and\nshould just one of them return a falsy value (this includes null and undefined,\nso remember to always return a value!) then the resource in question will not be\nfetched.\n\n##### Adding a fetch condition\n\nThis example fetch condition prevents URLs ending in `.pdf` from downloading.\nAdding a fetch condition assigns it an ID, which the `addFetchCondition` function\nreturns. You can use this ID to remove the condition later.\n\n```javascript\nvar conditionID = myCrawler.addFetchCondition(function(parsedURL) {\n\treturn !parsedURL.path.match(/\\.pdf$/i);\n});\n```\n\nNOTE: simplecrawler uses slightly different terminology to URIjs. `parsedURL.path`\nincludes the query string too. If you want the path without the query string,\nuse `parsedURL.uriPath`.\n\n##### Removing a fetch condition\n\nIf you stored the ID of the fetch condition you added earlier, you can remove it\nfrom the crawler:\n\n```javascript\nmyCrawler.removeFetchCondition(conditionID);\n```\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed\nat `crawler.queue` (assuming you called your Crawler() object `crawler`.) It\nprovides array access, so you can get to queue items just with array notation\nand an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate\ninterface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nThe simplest way to add to the queue is to use the crawler's own method,\n`crawler.queueURL`. This method takes a complete URL, validates and deconstructs\nit, and adds it to the queue.\n\nIf you instead want to add a resource by its components, you may call the\n`queue.add` method directly:\n\n```javascript\ncrawler.queue.add(protocol,hostname,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can\nremember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items,\nit helps to know what's inside them. These are the properties every queue item\nis expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `host` - The full domain/hostname of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The\nupside is, the queue actually has some convenient functions for getting simple\naggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network\nperformance of your crawl (so far.) This is done live, so don't check it thirty\ntimes a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the\n`crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions\nrespectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given\nstatus at any one time, and/or retreive them. That's easy with\n`crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.countWithStatus` returns the number of queued items with a given\nstatus, while `crawler.queue.getWithStatus` returns an array of the queue items\nthemselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n*\t`crawler.queue.complete` - returns the number of queue items which have been\n\tcompleted (marked as fetched)\n*\t`crawler.queue.errors` - returns the number of requests which have failed\n\t(404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if\nyour application fails or you need to abort the crawl for some reason. (Perhaps\nyou just want to finish off for the night and pick it up tomorrow!) The\n`crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be\nasynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when\nyou need to save the queue - don't call them every request or your application's\nperformance will be incredibly poor - they block like *crazy*. That said, using\nthem when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n## Cookies\n\nSimplecrawler now has an internal cookie jar, which collects and resends cookies\nautomatically, and by default.\n\nIf you want to turn this off, set the `crawler.acceptCookies` option to `false`.\n\nThe cookie jar is accessible via `crawler.cookies`, and is an event emitter itself:\n\n### Cookie Events\n\n* `addcookie` ( cookie )\nFired when a new cookie is added to the jar.\n* `removecookie` ( cookie array )\nFired when one or more cookies are removed from the jar.\n\n## Building and Testing\n\n#### Build Status:\n\n* Master: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n* Development: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=development)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\n## Contributors\n\nI'd like to extend sincere thanks to:\n\n*\t[Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support, and\n\tinitial cookie support.\n*\t[Mike Moulton](https://github.com/mmoulton) for\n\t[fixing a bug in the URL discovery mechanism]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/3), as well as\n\t[adding the `discoverycomplete` event]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/10),\n*\t[Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword\n\tnaming collision with node 0.8's EventEmitter.\n*\t[Greg Molnar](https://github.com/gregmolnar) for\n\t[adding a querystring-free path parameter to parsed URL objects.]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/31)\n*\t[Breck Yunits](https://github.com/breck7) for contributing a useful code\n\tsample demonstrating using simplecrawler for caching a website to disk!\n*\t[Luke Plaster](https://github.com/notatestuser) for enabling protocol-agnostic\n\tlink discovery\n*\t[Zeus](https://github.com/distracteddev) for fixing a bug where [default port\n\tinfo was wrongly specified in requests]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/40)\n\tand for fixing the missing request timeout handling!\n*\t[Jellyfrog](https://github.com/jellyfrog) for assisting in diagnosing some\n\tnasty EventEmitter issues.\n\nAnd everybody else who has helped out in some way! :)\n\n## Licence\n\nCopyright (c) 2012, Christopher Giffard.\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, \nare permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n* Redistributions in binary form must reproduce the above copyright notice, this\n  list of conditions and the following disclaimer in the documentation and/or\n  other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR \nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n","readmeFilename":"README.markdown","_id":"simplecrawler@0.2.9","dist":{"shasum":"a7551c33e1ed2c9548836edc3cd9a10167c6e0a6","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.2.9.tgz"},"_from":".","_npmVersion":"1.2.21","_npmUser":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},{"name":"cgiffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.2.10":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.2.10","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"scripts":{"test":"mocha -R spec -t 5000"},"bin":{"crawl":"./lib/cli.js"},"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./lib/index.js","engines":{"node":">=0.4.0"},"devDependencies":{"mocha":"~1.8.1","jshint":"~0.7.x","chai":"~1.2.0"},"dependencies":{"URIjs":"~1.10.2"},"readme":"# Simple web-crawler for node.js [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\nSimplecrawler is designed to provide the most basic possible API for crawling\nwebsites, while being as flexible and robust as possible. I wrote simplecrawler\nto archive, analyse, and search some very large websites. It has happily chewed\nthrough 50,000 pages and written tens of gigabytes to disk without issue.\n\n#### Example (simple mode)\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n\nCrawler.crawl(\"http://example.com/\")\n\t.on(\"fetchcomplete\",function(queueItem){\n\t\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n\t});\n```\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can\nreplace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except\nwhen discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nThere are two ways of instantiating a new crawler - a simple but less flexible\nmethod inspired by [anemone](http://anemone.rubyforge.org), and the traditional\nmethod which provides a little more room to configure crawl parameters.\n\nRegardless of wether you use the simple or traditional methods of instantiation,\nyou'll need to require simplecrawler:\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n```\n\n#### Simple Mode\n\nSimple mode generates a new crawler for you, preconfigures it based on a URL you\nprovide, and returns the crawler to you for further configuration and so you can\nattach event handlers.\n\nSimply call `Crawler.crawl`, with a URL first parameter, and two optional\nfunctions that will be added as event listeners for `fetchcomplete` and\n`fetcherror` respectively.\n\n```javascript\nCrawler.crawl(\"http://example.com/\", function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\nAlternately, if you decide to omit these functions, you can use the returned\ncrawler object to add the event listeners yourself, and tweak configuration\noptions:\n\n```javascript\nvar crawler = Crawler.crawl(\"http://example.com/\");\n\ncrawler.interval = 500;\n\ncrawler.on(\"fetchcomplete\",function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\n#### Advanced Mode\n\nThe alternative method of creating a crawler is to call the `simplecrawler`\nconstructor yourself, and to initiate the crawl manually.\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your web\nserver. Decrease the concurrency from five simultaneous requests - and increase\nthe request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when\ncreating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run\nthrough its queue finding linked resources on the domain to download, until it\ncan't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `crawlstart`\nFired when the crawl begins or is restarted.\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually\nqueue an item yourself.)\n* `queueduplicate` ( URLData )\nFired when an item cannot be added to the queue because it is already present in\nthe queue. Frequent firing of this event is normal and expected.\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem , requestOptions )\nFired when an item is spooled for fetching. If your event handler is synchronous,\nyou can modify the crawler request options (including headers) \n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http\nresponse object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided\nas a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size\nwe're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned\nas a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a\nrequest.\n* `fetchtimeout` ( queueItem, crawlerTimeoutValue )\nFired when a request time exceeds the internal crawler threshold.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as\nthe second parameter.\n* `discoverycomplete` ( queueItem, resources )\nFired when linked resources have been discovered. Passes an array of resources\n(as URLs) as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does\nnot find any more to add. This event returns no arguments.\n\n#### A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters\nan HTTP error status in the response. If you need this information, you can listen\nto simplecrawler's error events, and through node's native `data` event\n(`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let\nme know. I didn't include it because I didn't need it - but if it's important to\npeople I might put it back in. :)\n\n#### Asynchronous Event Listeners\n\nBy default, all of these events, with the exception of `complete`, provide a\nfinal `asyncCallback` parameter, which is a function.\n\nIf your function is configured to receive this parameter in its function definition,\nsimplecrawler will treat this function as asynchronous, and will not consider it\ncompleted until you call the `asyncCallback` function provided.\n\nThe crawler will continue crawling while your event handler finishes up, but it\nwon't call the `complete` event and clean up its timer until every asynchronous\ncallback is complete.\n\n##### Example Asynchronous Event Listener\n\n```javascript\ncrawler.on(\"fetchcomplete\",function(queueItem,data,res,done) {\n\tdoSomeDiscovery(data,function(foundURLs){\n\t\tfoundURLs.forEach(crawler.queueURL.bind(crawler));\n\t\tdone();\n\t});\n});\n```\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n*\t`crawler.host` -\n\tThe domain to scan. By default, simplecrawler will restrict all requests to\n\tthis domain.\n*\t`crawler.initialPath` -\n\tThe initial path with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialPort` -\n\tThe initial port with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialProtocol` -\n\tThe initial protocol with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.interval` -\n\tThe interval with which the crawler will spool up new requests (one per\n\ttick.) Defaults to 250ms.\n*\t`crawler.maxConcurrency` -\n\tThe maximum number of requests the crawler will run simultaneously. Defaults\n\tto 5 - the default number of http agents node will run.\n*\t`crawler.timeout` -\n\tThe maximum time the crawler will wait for headers before aborting the request.\n*\t`crawler.userAgent` -\n\tThe user agent the crawler will report. Defaults to\n\t`Node/SimpleCrawler <version> (http://www.github.com/cgiffard/node-simplecrawler)`.\n*\t`crawler.queue` -\n\tThe queue in use by the crawler (Must implement the `FetchQueue` interface)\n*\t`crawler.filterByDomain` -\n\tSpecifies whether the crawler will restrict queued requests to a given\n\tdomain/domains.\n*\t`crawler.scanSubdomains` -\n\tEnables scanning subdomains (other than www) as well as the specified domain.\n\tDefaults to false.\n*\t`crawler.ignoreWWWDomain` -\n\tTreats the `www` domain the same as the originally specified domain.\n\tDefaults to true.\n*\t`crawler.stripWWWDomain` -\n\tOr go even further and strip WWW subdomain from requests altogether!\n*\t`crawler.stripQuerystring` -\n\tSpecify to strip querystring parameters from URLs. Defaults to false.\n*\t`crawler.discoverResources` -\n\tUse simplecrawler's internal resource discovery function. Defaults to true.\n\t(switch it off if you'd prefer to discover and queue resources yourself!)\n*\t`crawler.cache` -\n\tSpecify a cache architecture to use when crawling. Must implement\n\t`SimpleCache` interface.\n*\t`crawler.useProxy` -\n\tThe crawler should use an HTTP proxy to make its requests.\n*\t`crawler.proxyHostname` -\n\tThe hostname of the proxy to use for requests.\n*\t`crawler.proxyPort` -\n\tThe port of the proxy to use for requests.\n*\t`crawler.domainWhitelist` -\n\tAn array of domains the crawler is permitted to crawl from. If other settings\n\tare more permissive, they will override this setting.\n*\t`crawler.supportedMimeTypes` -\n\tAn array of RegEx objects used to determine supported MIME types (types of\n\tdata simplecrawler will scan for links.) If you're  not using simplecrawler's\n\tresource discovery function, this won't have any effect.\n*\t`crawler.allowedProtocols` -\n\tAn array of RegEx objects used to determine whether a URL protocol is supported.\n\tThis is to deal with nonstandard protocol handlers that regular HTTP is\n\tsometimes given, like `feed:`. It does not provide support for non-http\n\tprotocols (and why would it!?)\n*\t`crawler.maxResourceSize` - \n\tThe maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n*\t`crawler.downloadUnsupported` -\n\tSimplecrawler will download files it can't parse. Defaults to true, but if\n\tyou'd rather save the RAM and GC lag, switch it off.\n*\t`crawler.needsAuth` -\n\tFlag to specify if the domain you are hitting requires basic authentication\n*\t`crawler.authUser` -\n\tUsername provided for needsAuth flag\n*\t`crawler.authPass` -\n\tPassword provided for needsAuth flag\n*\t`crawler.customHeaders` -\n\tAn object specifying a number of custom headers simplecrawler will add to\n\tevery request. These override the default headers simplecrawler sets, so\n\tbe careful with them. If you want to tamper with headers on a per-request basis,\n\tsee the `fetchqueue` event.\n*\t`crawler.acceptCookies` -\n\tFlag to indicate if the crawler should hold on to cookies\n*\t`crawler.urlEncoding` -\n\tSet this to `iso8859` to trigger URIjs' re-encoding of iso8859 URLs to unicode.\n\tDefaults to `unicode`.\n\n#### Excluding certain resources from downloading\n\nSimplecrawler has a mechanism you can use to prevent certain resources from being\nfetched, based on the URL, called *Fetch Conditions**. A fetch condition is just\na function, which, when given a parsed URL object, will return a true or a false\nvalue, indicating whether a given resource should be downloaded.\n\nYou may add as many fetch conditions as you like, and remove them at runtime.\nSimplecrawler will evaluate every single condition against every queued URL, and\nshould just one of them return a falsy value (this includes null and undefined,\nso remember to always return a value!) then the resource in question will not be\nfetched.\n\n##### Adding a fetch condition\n\nThis example fetch condition prevents URLs ending in `.pdf` from downloading.\nAdding a fetch condition assigns it an ID, which the `addFetchCondition` function\nreturns. You can use this ID to remove the condition later.\n\n```javascript\nvar conditionID = myCrawler.addFetchCondition(function(parsedURL) {\n\treturn !parsedURL.path.match(/\\.pdf$/i);\n});\n```\n\nNOTE: simplecrawler uses slightly different terminology to URIjs. `parsedURL.path`\nincludes the query string too. If you want the path without the query string,\nuse `parsedURL.uriPath`.\n\n##### Removing a fetch condition\n\nIf you stored the ID of the fetch condition you added earlier, you can remove it\nfrom the crawler:\n\n```javascript\nmyCrawler.removeFetchCondition(conditionID);\n```\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed\nat `crawler.queue` (assuming you called your Crawler() object `crawler`.) It\nprovides array access, so you can get to queue items just with array notation\nand an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate\ninterface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nThe simplest way to add to the queue is to use the crawler's own method,\n`crawler.queueURL`. This method takes a complete URL, validates and deconstructs\nit, and adds it to the queue.\n\nIf you instead want to add a resource by its components, you may call the\n`queue.add` method directly:\n\n```javascript\ncrawler.queue.add(protocol,hostname,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can\nremember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items,\nit helps to know what's inside them. These are the properties every queue item\nis expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `host` - The full domain/hostname of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The\nupside is, the queue actually has some convenient functions for getting simple\naggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network\nperformance of your crawl (so far.) This is done live, so don't check it thirty\ntimes a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the\n`crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions\nrespectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given\nstatus at any one time, and/or retreive them. That's easy with\n`crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.countWithStatus` returns the number of queued items with a given\nstatus, while `crawler.queue.getWithStatus` returns an array of the queue items\nthemselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n*\t`crawler.queue.complete` - returns the number of queue items which have been\n\tcompleted (marked as fetched)\n*\t`crawler.queue.errors` - returns the number of requests which have failed\n\t(404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if\nyour application fails or you need to abort the crawl for some reason. (Perhaps\nyou just want to finish off for the night and pick it up tomorrow!) The\n`crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be\nasynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when\nyou need to save the queue - don't call them every request or your application's\nperformance will be incredibly poor - they block like *crazy*. That said, using\nthem when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n## Cookies\n\nSimplecrawler now has an internal cookie jar, which collects and resends cookies\nautomatically, and by default.\n\nIf you want to turn this off, set the `crawler.acceptCookies` option to `false`.\n\nThe cookie jar is accessible via `crawler.cookies`, and is an event emitter itself:\n\n### Cookie Events\n\n* `addcookie` ( cookie )\nFired when a new cookie is added to the jar.\n* `removecookie` ( cookie array )\nFired when one or more cookies are removed from the jar.\n\n## Building and Testing\n\n#### Build Status:\n\n* Master: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n* Development: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=development)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\n## Contributors\n\nI'd like to extend sincere thanks to:\n\n*\t[Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support, and\n\tinitial cookie support.\n*\t[Mike Moulton](https://github.com/mmoulton) for\n\t[fixing a bug in the URL discovery mechanism]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/3), as well as\n\t[adding the `discoverycomplete` event]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/10),\n*\t[Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword\n\tnaming collision with node 0.8's EventEmitter.\n*\t[Greg Molnar](https://github.com/gregmolnar) for\n\t[adding a querystring-free path parameter to parsed URL objects.]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/31)\n*\t[Breck Yunits](https://github.com/breck7) for contributing a useful code\n\tsample demonstrating using simplecrawler for caching a website to disk!\n*\t[Luke Plaster](https://github.com/notatestuser) for enabling protocol-agnostic\n\tlink discovery\n*\t[Zeus](https://github.com/distracteddev) for fixing a bug where [default port\n\tinfo was wrongly specified in requests]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/40)\n\tand for fixing the missing request timeout handling!\n*\t[Graham Hutchinson](https://github.com/ghhutch) for adding\n\tquerystring-stripping option \n*\t[Jellyfrog](https://github.com/jellyfrog) for assisting in diagnosing some\n\tnasty EventEmitter issues.\n\nAnd everybody else who has helped out in some way! :)\n\n## Licence\n\nCopyright (c) 2012, Christopher Giffard.\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, \nare permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n* Redistributions in binary form must reproduce the above copyright notice, this\n  list of conditions and the following disclaimer in the documentation and/or\n  other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR \nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n","readmeFilename":"README.markdown","_id":"simplecrawler@0.2.10","dist":{"shasum":"e300eada8c96d0f27863a1d3418195209cf1aa43","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.2.10.tgz"},"_from":".","_npmVersion":"1.2.21","_npmUser":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},{"name":"cgiffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.3.0":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.3.0","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"scripts":{"test":"mocha -R spec -t 5000"},"bin":{"crawl":"./lib/cli.js"},"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./lib/index.js","engines":{"node":">=0.4.0"},"devDependencies":{"mocha":"~1.8.1","jshint":"~0.7.x","chai":"~1.2.0"},"dependencies":{"URIjs":"~1.10.2"},"readme":"# Simple web-crawler for node.js [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\nSimplecrawler is designed to provide the most basic possible API for crawling\nwebsites, while being as flexible and robust as possible. I wrote simplecrawler\nto archive, analyse, and search some very large websites. It has happily chewed\nthrough 50,000 pages and written tens of gigabytes to disk without issue.\n\n#### Example (simple mode)\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n\nCrawler.crawl(\"http://example.com/\")\n\t.on(\"fetchcomplete\",function(queueItem){\n\t\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n\t});\n```\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can\nreplace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except\nwhen discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nThere are two ways of instantiating a new crawler - a simple but less flexible\nmethod inspired by [anemone](http://anemone.rubyforge.org), and the traditional\nmethod which provides a little more room to configure crawl parameters.\n\nRegardless of wether you use the simple or traditional methods of instantiation,\nyou'll need to require simplecrawler:\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n```\n\n#### Simple Mode\n\nSimple mode generates a new crawler for you, preconfigures it based on a URL you\nprovide, and returns the crawler to you for further configuration and so you can\nattach event handlers.\n\nSimply call `Crawler.crawl`, with a URL first parameter, and two optional\nfunctions that will be added as event listeners for `fetchcomplete` and\n`fetcherror` respectively.\n\n```javascript\nCrawler.crawl(\"http://example.com/\", function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\nAlternately, if you decide to omit these functions, you can use the returned\ncrawler object to add the event listeners yourself, and tweak configuration\noptions:\n\n```javascript\nvar crawler = Crawler.crawl(\"http://example.com/\");\n\ncrawler.interval = 500;\n\ncrawler.on(\"fetchcomplete\",function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\n#### Advanced Mode\n\nThe alternative method of creating a crawler is to call the `simplecrawler`\nconstructor yourself, and to initiate the crawl manually.\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your web\nserver. Decrease the concurrency from five simultaneous requests - and increase\nthe request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when\ncreating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run\nthrough its queue finding linked resources on the domain to download, until it\ncan't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `crawlstart`\nFired when the crawl begins or is restarted.\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually\nqueue an item yourself.)\n* `queueduplicate` ( URLData )\nFired when an item cannot be added to the queue because it is already present in\nthe queue. Frequent firing of this event is normal and expected.\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem , requestOptions )\nFired when an item is spooled for fetching. If your event handler is synchronous,\nyou can modify the crawler request options (including headers) \n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http\nresponse object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided\nas a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size\nwe're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned\nas a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a\nrequest.\n* `fetchtimeout` ( queueItem, crawlerTimeoutValue )\nFired when a request time exceeds the internal crawler threshold.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as\nthe second parameter.\n* `discoverycomplete` ( queueItem, resources )\nFired when linked resources have been discovered. Passes an array of resources\n(as URLs) as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does\nnot find any more to add. This event returns no arguments.\n\n#### A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters\nan HTTP error status in the response. If you need this information, you can listen\nto simplecrawler's error events, and through node's native `data` event\n(`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let\nme know. I didn't include it because I didn't need it - but if it's important to\npeople I might put it back in. :)\n\n#### Waiting for Asynchronous Event Listeners\n\nSometimes, you might want to wait for simplecrawler to wait for you while you\nperform sone asynchronous tasks in an event listener, instead of having the it\nracing off and firing the `complete` event, halting your crawl. For example,\nif you're doing your own link discovery using an asynchronous library method.\n\nSimplecrawler provides a `wait` method you can call at any time. It is available\nvia `this` from inside listeners, and on the crawler object itself. It returns\na callback function.\n\nOnce you've called this method, simplecrawler will not fire the `complete` event\nuntil either you execute the callback it returns, or a timeout is reached\n(configured in `crawler.listenerTTL`, by default 5000 msec.)\n\n##### Example Asynchronous Event Listener\n\n```javascript\ncrawler.on(\"fetchcomplete\",function(queueItem,data,res) {\n\tvar continue = this.wait();\n\tdoSomeDiscovery(data,function(foundURLs){\n\t\tfoundURLs.forEach(crawler.queueURL.bind(crawler));\n\t\tcontinue();\n\t});\n});\n```\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n*\t`crawler.host` -\n\tThe domain to scan. By default, simplecrawler will restrict all requests to\n\tthis domain.\n*\t`crawler.initialPath` -\n\tThe initial path with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialPort` -\n\tThe initial port with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialProtocol` -\n\tThe initial protocol with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.interval` -\n\tThe interval with which the crawler will spool up new requests (one per\n\ttick.) Defaults to 250ms.\n*\t`crawler.maxConcurrency` -\n\tThe maximum number of requests the crawler will run simultaneously. Defaults\n\tto 5 - the default number of http agents node will run.\n*\t`crawler.timeout` -\n\tThe maximum time in milliseconds the crawler will wait for headers before\n\taborting the request.\n*\t`crawler.listenerTTL` -\n\tThe maximum time in milliseconds the crawler will wait for async listeners.\n*\t`crawler.userAgent` -\n\tThe user agent the crawler will report. Defaults to\n\t`Node/SimpleCrawler <version> (http://www.github.com/cgiffard/node-simplecrawler)`.\n*\t`crawler.queue` -\n\tThe queue in use by the crawler (Must implement the `FetchQueue` interface)\n*\t`crawler.filterByDomain` -\n\tSpecifies whether the crawler will restrict queued requests to a given\n\tdomain/domains.\n*\t`crawler.scanSubdomains` -\n\tEnables scanning subdomains (other than www) as well as the specified domain.\n\tDefaults to false.\n*\t`crawler.ignoreWWWDomain` -\n\tTreats the `www` domain the same as the originally specified domain.\n\tDefaults to true.\n*\t`crawler.stripWWWDomain` -\n\tOr go even further and strip WWW subdomain from requests altogether!\n*\t`crawler.stripQuerystring` -\n\tSpecify to strip querystring parameters from URLs. Defaults to false.\n*\t`crawler.discoverResources` -\n\tUse simplecrawler's internal resource discovery function. Defaults to true.\n\t(switch it off if you'd prefer to discover and queue resources yourself!)\n*\t`crawler.cache` -\n\tSpecify a cache architecture to use when crawling. Must implement\n\t`SimpleCache` interface.\n*\t`crawler.useProxy` -\n\tThe crawler should use an HTTP proxy to make its requests.\n*\t`crawler.proxyHostname` -\n\tThe hostname of the proxy to use for requests.\n*\t`crawler.proxyPort` -\n\tThe port of the proxy to use for requests.\n*\t`crawler.domainWhitelist` -\n\tAn array of domains the crawler is permitted to crawl from. If other settings\n\tare more permissive, they will override this setting.\n*\t`crawler.supportedMimeTypes` -\n\tAn array of RegEx objects used to determine supported MIME types (types of\n\tdata simplecrawler will scan for links.) If you're  not using simplecrawler's\n\tresource discovery function, this won't have any effect.\n*\t`crawler.allowedProtocols` -\n\tAn array of RegEx objects used to determine whether a URL protocol is supported.\n\tThis is to deal with nonstandard protocol handlers that regular HTTP is\n\tsometimes given, like `feed:`. It does not provide support for non-http\n\tprotocols (and why would it!?)\n*\t`crawler.maxResourceSize` - \n\tThe maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n*\t`crawler.downloadUnsupported` -\n\tSimplecrawler will download files it can't parse. Defaults to true, but if\n\tyou'd rather save the RAM and GC lag, switch it off.\n*\t`crawler.needsAuth` -\n\tFlag to specify if the domain you are hitting requires basic authentication\n*\t`crawler.authUser` -\n\tUsername provided for needsAuth flag\n*\t`crawler.authPass` -\n\tPassword provided for needsAuth flag\n*\t`crawler.customHeaders` -\n\tAn object specifying a number of custom headers simplecrawler will add to\n\tevery request. These override the default headers simplecrawler sets, so\n\tbe careful with them. If you want to tamper with headers on a per-request basis,\n\tsee the `fetchqueue` event.\n*\t`crawler.acceptCookies` -\n\tFlag to indicate if the crawler should hold on to cookies\n*\t`crawler.urlEncoding` -\n\tSet this to `iso8859` to trigger URIjs' re-encoding of iso8859 URLs to unicode.\n\tDefaults to `unicode`.\n\n#### Excluding certain resources from downloading\n\nSimplecrawler has a mechanism you can use to prevent certain resources from being\nfetched, based on the URL, called *Fetch Conditions**. A fetch condition is just\na function, which, when given a parsed URL object, will return a true or a false\nvalue, indicating whether a given resource should be downloaded.\n\nYou may add as many fetch conditions as you like, and remove them at runtime.\nSimplecrawler will evaluate every single condition against every queued URL, and\nshould just one of them return a falsy value (this includes null and undefined,\nso remember to always return a value!) then the resource in question will not be\nfetched.\n\n##### Adding a fetch condition\n\nThis example fetch condition prevents URLs ending in `.pdf` from downloading.\nAdding a fetch condition assigns it an ID, which the `addFetchCondition` function\nreturns. You can use this ID to remove the condition later.\n\n```javascript\nvar conditionID = myCrawler.addFetchCondition(function(parsedURL) {\n\treturn !parsedURL.path.match(/\\.pdf$/i);\n});\n```\n\nNOTE: simplecrawler uses slightly different terminology to URIjs. `parsedURL.path`\nincludes the query string too. If you want the path without the query string,\nuse `parsedURL.uriPath`.\n\n##### Removing a fetch condition\n\nIf you stored the ID of the fetch condition you added earlier, you can remove it\nfrom the crawler:\n\n```javascript\nmyCrawler.removeFetchCondition(conditionID);\n```\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed\nat `crawler.queue` (assuming you called your Crawler() object `crawler`.) It\nprovides array access, so you can get to queue items just with array notation\nand an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate\ninterface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nThe simplest way to add to the queue is to use the crawler's own method,\n`crawler.queueURL`. This method takes a complete URL, validates and deconstructs\nit, and adds it to the queue.\n\nIf you instead want to add a resource by its components, you may call the\n`queue.add` method directly:\n\n```javascript\ncrawler.queue.add(protocol,hostname,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can\nremember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items,\nit helps to know what's inside them. These are the properties every queue item\nis expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `host` - The full domain/hostname of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The\nupside is, the queue actually has some convenient functions for getting simple\naggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network\nperformance of your crawl (so far.) This is done live, so don't check it thirty\ntimes a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the\n`crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions\nrespectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given\nstatus at any one time, and/or retreive them. That's easy with\n`crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.countWithStatus` returns the number of queued items with a given\nstatus, while `crawler.queue.getWithStatus` returns an array of the queue items\nthemselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n*\t`crawler.queue.complete` - returns the number of queue items which have been\n\tcompleted (marked as fetched)\n*\t`crawler.queue.errors` - returns the number of requests which have failed\n\t(404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if\nyour application fails or you need to abort the crawl for some reason. (Perhaps\nyou just want to finish off for the night and pick it up tomorrow!) The\n`crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be\nasynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when\nyou need to save the queue - don't call them every request or your application's\nperformance will be incredibly poor - they block like *crazy*. That said, using\nthem when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n## Cookies\n\nSimplecrawler now has an internal cookie jar, which collects and resends cookies\nautomatically, and by default.\n\nIf you want to turn this off, set the `crawler.acceptCookies` option to `false`.\n\nThe cookie jar is accessible via `crawler.cookies`, and is an event emitter itself:\n\n### Cookie Events\n\n* `addcookie` ( cookie )\nFired when a new cookie is added to the jar.\n* `removecookie` ( cookie array )\nFired when one or more cookies are removed from the jar.\n\n## Building and Testing\n\n#### Build Status:\n\n* Master: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n* Development: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=development)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\n## Contributors\n\nI'd like to extend sincere thanks to:\n\n*\t[Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support, and\n\tinitial cookie support.\n*\t[Mike Moulton](https://github.com/mmoulton) for\n\t[fixing a bug in the URL discovery mechanism]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/3), as well as\n\t[adding the `discoverycomplete` event]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/10),\n*\t[Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword\n\tnaming collision with node 0.8's EventEmitter.\n*\t[Greg Molnar](https://github.com/gregmolnar) for\n\t[adding a querystring-free path parameter to parsed URL objects.]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/31)\n*\t[Breck Yunits](https://github.com/breck7) for contributing a useful code\n\tsample demonstrating using simplecrawler for caching a website to disk!\n*\t[Luke Plaster](https://github.com/notatestuser) for enabling protocol-agnostic\n\tlink discovery\n*\t[Zeus](https://github.com/distracteddev) for fixing a bug where [default port\n\tinfo was wrongly specified in requests]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/40)\n\tand for fixing the missing request timeout handling!\n*\t[Graham Hutchinson](https://github.com/ghhutch) for adding\n\tquerystring-stripping option \n*\t[Jellyfrog](https://github.com/jellyfrog) for assisting in diagnosing some\n\tnasty EventEmitter issues.\n\nAnd everybody else who has helped out in some way! :)\n\n## Licence\n\nCopyright (c) 2012, Christopher Giffard.\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, \nare permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n* Redistributions in binary form must reproduce the above copyright notice, this\n  list of conditions and the following disclaimer in the documentation and/or\n  other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR \nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n","readmeFilename":"README.markdown","_id":"simplecrawler@0.3.0","dist":{"shasum":"c995aad93f6f763231c26981f6236d7083ce14e2","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.3.0.tgz"},"_from":".","_npmVersion":"1.3.8","_npmUser":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},{"name":"cgiffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.3.1":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.3.1","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"scripts":{"test":"mocha -R spec -t 5000"},"bin":{"crawl":"./lib/cli.js"},"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./lib/index.js","engines":{"node":">=0.4.0"},"devDependencies":{"mocha":"~1.8.1","jshint":"~0.7.x","chai":"~1.2.0"},"dependencies":{"URIjs":"~1.10.2"},"readme":"# Simple web-crawler for node.js [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\nSimplecrawler is designed to provide the most basic possible API for crawling\nwebsites, while being as flexible and robust as possible. I wrote simplecrawler\nto archive, analyse, and search some very large websites. It has happily chewed\nthrough 50,000 pages and written tens of gigabytes to disk without issue.\n\n#### Example (simple mode)\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n\nCrawler.crawl(\"http://example.com/\")\n\t.on(\"fetchcomplete\",function(queueItem){\n\t\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n\t});\n```\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can\nreplace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except\nwhen discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nThere are two ways of instantiating a new crawler - a simple but less flexible\nmethod inspired by [anemone](http://anemone.rubyforge.org), and the traditional\nmethod which provides a little more room to configure crawl parameters.\n\nRegardless of wether you use the simple or traditional methods of instantiation,\nyou'll need to require simplecrawler:\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n```\n\n#### Simple Mode\n\nSimple mode generates a new crawler for you, preconfigures it based on a URL you\nprovide, and returns the crawler to you for further configuration and so you can\nattach event handlers.\n\nSimply call `Crawler.crawl`, with a URL first parameter, and two optional\nfunctions that will be added as event listeners for `fetchcomplete` and\n`fetcherror` respectively.\n\n```javascript\nCrawler.crawl(\"http://example.com/\", function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\nAlternately, if you decide to omit these functions, you can use the returned\ncrawler object to add the event listeners yourself, and tweak configuration\noptions:\n\n```javascript\nvar crawler = Crawler.crawl(\"http://example.com/\");\n\ncrawler.interval = 500;\n\ncrawler.on(\"fetchcomplete\",function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\n#### Advanced Mode\n\nThe alternative method of creating a crawler is to call the `simplecrawler`\nconstructor yourself, and to initiate the crawl manually.\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your web\nserver. Decrease the concurrency from five simultaneous requests - and increase\nthe request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when\ncreating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run\nthrough its queue finding linked resources on the domain to download, until it\ncan't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `crawlstart`\nFired when the crawl begins or is restarted.\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually\nqueue an item yourself.)\n* `queueduplicate` ( URLData )\nFired when an item cannot be added to the queue because it is already present in\nthe queue. Frequent firing of this event is normal and expected.\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem , requestOptions )\nFired when an item is spooled for fetching. If your event handler is synchronous,\nyou can modify the crawler request options (including headers) \n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http\nresponse object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided\nas a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size\nwe're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned\nas a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a\nrequest.\n* `fetchtimeout` ( queueItem, crawlerTimeoutValue )\nFired when a request time exceeds the internal crawler threshold.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as\nthe second parameter.\n* `discoverycomplete` ( queueItem, resources )\nFired when linked resources have been discovered. Passes an array of resources\n(as URLs) as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does\nnot find any more to add. This event returns no arguments.\n\n#### A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters\nan HTTP error status in the response. If you need this information, you can listen\nto simplecrawler's error events, and through node's native `data` event\n(`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let\nme know. I didn't include it because I didn't need it - but if it's important to\npeople I might put it back in. :)\n\n#### Waiting for Asynchronous Event Listeners\n\nSometimes, you might want to wait for simplecrawler to wait for you while you\nperform sone asynchronous tasks in an event listener, instead of having it\nracing off and firing the `complete` event, halting your crawl. For example,\nif you're doing your own link discovery using an asynchronous library method.\n\nSimplecrawler provides a `wait` method you can call at any time. It is available\nvia `this` from inside listeners, and on the crawler object itself. It returns\na callback function.\n\nOnce you've called this method, simplecrawler will not fire the `complete` event\nuntil either you execute the callback it returns, or a timeout is reached\n(configured in `crawler.listenerTTL`, by default 10000 msec.)\n\n##### Example Asynchronous Event Listener\n\n```javascript\ncrawler.on(\"fetchcomplete\",function(queueItem,data,res) {\n\tvar continue = this.wait();\n\tdoSomeDiscovery(data,function(foundURLs){\n\t\tfoundURLs.forEach(crawler.queueURL.bind(crawler));\n\t\tcontinue();\n\t});\n});\n```\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n*\t`crawler.host` -\n\tThe domain to scan. By default, simplecrawler will restrict all requests to\n\tthis domain.\n*\t`crawler.initialPath` -\n\tThe initial path with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialPort` -\n\tThe initial port with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialProtocol` -\n\tThe initial protocol with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.interval` -\n\tThe interval with which the crawler will spool up new requests (one per\n\ttick.) Defaults to 250ms.\n*\t`crawler.maxConcurrency` -\n\tThe maximum number of requests the crawler will run simultaneously. Defaults\n\tto 5 - the default number of http agents node will run.\n*\t`crawler.timeout` -\n\tThe maximum time in milliseconds the crawler will wait for headers before\n\taborting the request.\n*\t`crawler.listenerTTL` -\n\tThe maximum time in milliseconds the crawler will wait for async listeners.\n*\t`crawler.userAgent` -\n\tThe user agent the crawler will report. Defaults to\n\t`Node/SimpleCrawler <version> (http://www.github.com/cgiffard/node-simplecrawler)`.\n*\t`crawler.queue` -\n\tThe queue in use by the crawler (Must implement the `FetchQueue` interface)\n*\t`crawler.filterByDomain` -\n\tSpecifies whether the crawler will restrict queued requests to a given\n\tdomain/domains.\n*\t`crawler.scanSubdomains` -\n\tEnables scanning subdomains (other than www) as well as the specified domain.\n\tDefaults to false.\n*\t`crawler.ignoreWWWDomain` -\n\tTreats the `www` domain the same as the originally specified domain.\n\tDefaults to true.\n*\t`crawler.stripWWWDomain` -\n\tOr go even further and strip WWW subdomain from requests altogether!\n*\t`crawler.stripQuerystring` -\n\tSpecify to strip querystring parameters from URLs. Defaults to false.\n*\t`crawler.discoverResources` -\n\tUse simplecrawler's internal resource discovery function. Defaults to true.\n\t(switch it off if you'd prefer to discover and queue resources yourself!)\n*\t`crawler.discoverRegex` -\n\tArray of regex objects that simplecrawler uses to discover resources.\n*\t`crawler.cache` -\n\tSpecify a cache architecture to use when crawling. Must implement\n\t`SimpleCache` interface.\n*\t`crawler.useProxy` -\n\tThe crawler should use an HTTP proxy to make its requests.\n*\t`crawler.proxyHostname` -\n\tThe hostname of the proxy to use for requests.\n*\t`crawler.proxyPort` -\n\tThe port of the proxy to use for requests.\n*\t`crawler.domainWhitelist` -\n\tAn array of domains the crawler is permitted to crawl from. If other settings\n\tare more permissive, they will override this setting.\n*\t`crawler.supportedMimeTypes` -\n\tAn array of RegEx objects used to determine supported MIME types (types of\n\tdata simplecrawler will scan for links.) If you're  not using simplecrawler's\n\tresource discovery function, this won't have any effect.\n*\t`crawler.allowedProtocols` -\n\tAn array of RegEx objects used to determine whether a URL protocol is supported.\n\tThis is to deal with nonstandard protocol handlers that regular HTTP is\n\tsometimes given, like `feed:`. It does not provide support for non-http\n\tprotocols (and why would it!?)\n*\t`crawler.maxResourceSize` - \n\tThe maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n*\t`crawler.downloadUnsupported` -\n\tSimplecrawler will download files it can't parse. Defaults to true, but if\n\tyou'd rather save the RAM and GC lag, switch it off.\n*\t`crawler.needsAuth` -\n\tFlag to specify if the domain you are hitting requires basic authentication\n*\t`crawler.authUser` -\n\tUsername provided for needsAuth flag\n*\t`crawler.authPass` -\n\tPassword provided for needsAuth flag\n*\t`crawler.customHeaders` -\n\tAn object specifying a number of custom headers simplecrawler will add to\n\tevery request. These override the default headers simplecrawler sets, so\n\tbe careful with them. If you want to tamper with headers on a per-request basis,\n\tsee the `fetchqueue` event.\n*\t`crawler.acceptCookies` -\n\tFlag to indicate if the crawler should hold on to cookies\n*\t`crawler.urlEncoding` -\n\tSet this to `iso8859` to trigger URIjs' re-encoding of iso8859 URLs to unicode.\n\tDefaults to `unicode`.\n\n#### Excluding certain resources from downloading\n\nSimplecrawler has a mechanism you can use to prevent certain resources from being\nfetched, based on the URL, called *Fetch Conditions**. A fetch condition is just\na function, which, when given a parsed URL object, will return a true or a false\nvalue, indicating whether a given resource should be downloaded.\n\nYou may add as many fetch conditions as you like, and remove them at runtime.\nSimplecrawler will evaluate every single condition against every queued URL, and\nshould just one of them return a falsy value (this includes null and undefined,\nso remember to always return a value!) then the resource in question will not be\nfetched.\n\n##### Adding a fetch condition\n\nThis example fetch condition prevents URLs ending in `.pdf` from downloading.\nAdding a fetch condition assigns it an ID, which the `addFetchCondition` function\nreturns. You can use this ID to remove the condition later.\n\n```javascript\nvar conditionID = myCrawler.addFetchCondition(function(parsedURL) {\n\treturn !parsedURL.path.match(/\\.pdf$/i);\n});\n```\n\nNOTE: simplecrawler uses slightly different terminology to URIjs. `parsedURL.path`\nincludes the query string too. If you want the path without the query string,\nuse `parsedURL.uriPath`.\n\n##### Removing a fetch condition\n\nIf you stored the ID of the fetch condition you added earlier, you can remove it\nfrom the crawler:\n\n```javascript\nmyCrawler.removeFetchCondition(conditionID);\n```\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed\nat `crawler.queue` (assuming you called your Crawler() object `crawler`.) It\nprovides array access, so you can get to queue items just with array notation\nand an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate\ninterface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nThe simplest way to add to the queue is to use the crawler's own method,\n`crawler.queueURL`. This method takes a complete URL, validates and deconstructs\nit, and adds it to the queue.\n\nIf you instead want to add a resource by its components, you may call the\n`queue.add` method directly:\n\n```javascript\ncrawler.queue.add(protocol,hostname,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can\nremember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items,\nit helps to know what's inside them. These are the properties every queue item\nis expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `host` - The full domain/hostname of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The\nupside is, the queue actually has some convenient functions for getting simple\naggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network\nperformance of your crawl (so far.) This is done live, so don't check it thirty\ntimes a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the\n`crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions\nrespectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given\nstatus at any one time, and/or retreive them. That's easy with\n`crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.countWithStatus` returns the number of queued items with a given\nstatus, while `crawler.queue.getWithStatus` returns an array of the queue items\nthemselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n*\t`crawler.queue.complete` - returns the number of queue items which have been\n\tcompleted (marked as fetched)\n*\t`crawler.queue.errors` - returns the number of requests which have failed\n\t(404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if\nyour application fails or you need to abort the crawl for some reason. (Perhaps\nyou just want to finish off for the night and pick it up tomorrow!) The\n`crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be\nasynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when\nyou need to save the queue - don't call them every request or your application's\nperformance will be incredibly poor - they block like *crazy*. That said, using\nthem when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n## Cookies\n\nSimplecrawler now has an internal cookie jar, which collects and resends cookies\nautomatically, and by default.\n\nIf you want to turn this off, set the `crawler.acceptCookies` option to `false`.\n\nThe cookie jar is accessible via `crawler.cookies`, and is an event emitter itself:\n\n### Cookie Events\n\n* `addcookie` ( cookie )\nFired when a new cookie is added to the jar.\n* `removecookie` ( cookie array )\nFired when one or more cookies are removed from the jar.\n\n## Building and Testing\n\n#### Build Status:\n\n* Master: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n* Development: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=development)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\n## Contributors\n\nI'd like to extend sincere thanks to:\n\n*\t[Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support, and\n\tinitial cookie support.\n*\t[Mike Moulton](https://github.com/mmoulton) for\n\t[fixing a bug in the URL discovery mechanism]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/3), as well as\n\t[adding the `discoverycomplete` event]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/10),\n*\t[Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword\n\tnaming collision with node 0.8's EventEmitter.\n*\t[Greg Molnar](https://github.com/gregmolnar) for\n\t[adding a querystring-free path parameter to parsed URL objects.]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/31)\n*\t[Breck Yunits](https://github.com/breck7) for contributing a useful code\n\tsample demonstrating using simplecrawler for caching a website to disk!\n*\t[Luke Plaster](https://github.com/notatestuser) for enabling protocol-agnostic\n\tlink discovery\n*\t[Zeus](https://github.com/distracteddev) for fixing a bug where [default port\n\tinfo was wrongly specified in requests]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/40)\n\tand for fixing the missing request timeout handling!\n*\t[Graham Hutchinson](https://github.com/ghhutch) for adding\n\tquerystring-stripping option \n*\t[Jellyfrog](https://github.com/jellyfrog) for assisting in diagnosing some\n\tnasty EventEmitter issues.\n*\t[Brian Moeskau](https://github.com/bmoeskau) for helping to fix the confusing\n\t'async' events API, and providing invaluable feedback.\n\nAnd everybody else who has helped out in some way! :)\n\n## Licence\n\nCopyright (c) 2013, Christopher Giffard.\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, \nare permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n* Redistributions in binary form must reproduce the above copyright notice, this\n  list of conditions and the following disclaimer in the documentation and/or\n  other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR \nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n","readmeFilename":"README.markdown","_id":"simplecrawler@0.3.1","dist":{"shasum":"40bf82ac452069b22df7d0504718ed8a8fd15b77","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.3.1.tgz"},"_from":".","_npmVersion":"1.3.8","_npmUser":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},{"name":"cgiffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.3.2":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.3.2","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"scripts":{"test":"mocha -R spec -t 5000"},"bin":{"crawl":"./lib/cli.js"},"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./lib/index.js","engines":{"node":">=0.8.0"},"devDependencies":{"mocha":"~1.8.1","jshint":"~0.7.x","chai":"~1.2.0"},"dependencies":{"URIjs":"~1.10.2"},"readme":"# Simple web-crawler for node.js [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\nSimplecrawler is designed to provide the most basic possible API for crawling\nwebsites, while being as flexible and robust as possible. I wrote simplecrawler\nto archive, analyse, and search some very large websites. It has happily chewed\nthrough 50,000 pages and written tens of gigabytes to disk without issue.\n\n#### Example (simple mode)\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n\nCrawler.crawl(\"http://example.com/\")\n\t.on(\"fetchcomplete\",function(queueItem){\n\t\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n\t});\n```\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can\nreplace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except\nwhen discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nThere are two ways of instantiating a new crawler - a simple but less flexible\nmethod inspired by [anemone](http://anemone.rubyforge.org), and the traditional\nmethod which provides a little more room to configure crawl parameters.\n\nRegardless of wether you use the simple or traditional methods of instantiation,\nyou'll need to require simplecrawler:\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n```\n\n#### Simple Mode\n\nSimple mode generates a new crawler for you, preconfigures it based on a URL you\nprovide, and returns the crawler to you for further configuration and so you can\nattach event handlers.\n\nSimply call `Crawler.crawl`, with a URL first parameter, and two optional\nfunctions that will be added as event listeners for `fetchcomplete` and\n`fetcherror` respectively.\n\n```javascript\nCrawler.crawl(\"http://example.com/\", function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\nAlternately, if you decide to omit these functions, you can use the returned\ncrawler object to add the event listeners yourself, and tweak configuration\noptions:\n\n```javascript\nvar crawler = Crawler.crawl(\"http://example.com/\");\n\ncrawler.interval = 500;\n\ncrawler.on(\"fetchcomplete\",function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\n#### Advanced Mode\n\nThe alternative method of creating a crawler is to call the `simplecrawler`\nconstructor yourself, and to initiate the crawl manually.\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your web\nserver. Decrease the concurrency from five simultaneous requests - and increase\nthe request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when\ncreating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run\nthrough its queue finding linked resources on the domain to download, until it\ncan't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `crawlstart`\nFired when the crawl begins or is restarted.\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually\nqueue an item yourself.)\n* `queueduplicate` ( URLData )\nFired when an item cannot be added to the queue because it is already present in\nthe queue. Frequent firing of this event is normal and expected.\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem , requestOptions )\nFired when an item is spooled for fetching. If your event handler is synchronous,\nyou can modify the crawler request options (including headers) \n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http\nresponse object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided\nas a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size\nwe're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned\nas a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a\nrequest.\n* `fetchtimeout` ( queueItem, crawlerTimeoutValue )\nFired when a request time exceeds the internal crawler threshold.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as\nthe second parameter.\n* `discoverycomplete` ( queueItem, resources )\nFired when linked resources have been discovered. Passes an array of resources\n(as URLs) as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does\nnot find any more to add. This event returns no arguments.\n\n#### A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters\nan HTTP error status in the response. If you need this information, you can listen\nto simplecrawler's error events, and through node's native `data` event\n(`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let\nme know. I didn't include it because I didn't need it - but if it's important to\npeople I might put it back in. :)\n\n#### Waiting for Asynchronous Event Listeners\n\nSometimes, you might want to wait for simplecrawler to wait for you while you\nperform sone asynchronous tasks in an event listener, instead of having it\nracing off and firing the `complete` event, halting your crawl. For example,\nif you're doing your own link discovery using an asynchronous library method.\n\nSimplecrawler provides a `wait` method you can call at any time. It is available\nvia `this` from inside listeners, and on the crawler object itself. It returns\na callback function.\n\nOnce you've called this method, simplecrawler will not fire the `complete` event\nuntil either you execute the callback it returns, or a timeout is reached\n(configured in `crawler.listenerTTL`, by default 10000 msec.)\n\n##### Example Asynchronous Event Listener\n\n```javascript\ncrawler.on(\"fetchcomplete\",function(queueItem,data,res) {\n\tvar continue = this.wait();\n\tdoSomeDiscovery(data,function(foundURLs){\n\t\tfoundURLs.forEach(crawler.queueURL.bind(crawler));\n\t\tcontinue();\n\t});\n});\n```\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n*\t`crawler.host` -\n\tThe domain to scan. By default, simplecrawler will restrict all requests to\n\tthis domain.\n*\t`crawler.initialPath` -\n\tThe initial path with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialPort` -\n\tThe initial port with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialProtocol` -\n\tThe initial protocol with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.interval` -\n\tThe interval with which the crawler will spool up new requests (one per\n\ttick.) Defaults to 250ms.\n*\t`crawler.maxConcurrency` -\n\tThe maximum number of requests the crawler will run simultaneously. Defaults\n\tto 5 - the default number of http agents node will run.\n*\t`crawler.timeout` -\n\tThe maximum time in milliseconds the crawler will wait for headers before\n\taborting the request.\n*\t`crawler.listenerTTL` -\n\tThe maximum time in milliseconds the crawler will wait for async listeners.\n*\t`crawler.userAgent` -\n\tThe user agent the crawler will report. Defaults to\n\t`Node/SimpleCrawler <version> (http://www.github.com/cgiffard/node-simplecrawler)`.\n*\t`crawler.queue` -\n\tThe queue in use by the crawler (Must implement the `FetchQueue` interface)\n*\t`crawler.filterByDomain` -\n\tSpecifies whether the crawler will restrict queued requests to a given\n\tdomain/domains.\n*\t`crawler.scanSubdomains` -\n\tEnables scanning subdomains (other than www) as well as the specified domain.\n\tDefaults to false.\n*\t`crawler.ignoreWWWDomain` -\n\tTreats the `www` domain the same as the originally specified domain.\n\tDefaults to true.\n*\t`crawler.stripWWWDomain` -\n\tOr go even further and strip WWW subdomain from requests altogether!\n*\t`crawler.stripQuerystring` -\n\tSpecify to strip querystring parameters from URLs. Defaults to false.\n*\t`crawler.discoverResources` -\n\tUse simplecrawler's internal resource discovery function. Defaults to true.\n\t(switch it off if you'd prefer to discover and queue resources yourself!)\n*\t`crawler.discoverRegex` -\n\tArray of regex objects that simplecrawler uses to discover resources.\n*\t`crawler.cache` -\n\tSpecify a cache architecture to use when crawling. Must implement\n\t`SimpleCache` interface.\n*\t`crawler.useProxy` -\n\tThe crawler should use an HTTP proxy to make its requests.\n*\t`crawler.proxyHostname` -\n\tThe hostname of the proxy to use for requests.\n*\t`crawler.proxyPort` -\n\tThe port of the proxy to use for requests.\n*\t`crawler.domainWhitelist` -\n\tAn array of domains the crawler is permitted to crawl from. If other settings\n\tare more permissive, they will override this setting.\n*\t`crawler.supportedMimeTypes` -\n\tAn array of RegEx objects used to determine supported MIME types (types of\n\tdata simplecrawler will scan for links.) If you're  not using simplecrawler's\n\tresource discovery function, this won't have any effect.\n*\t`crawler.allowedProtocols` -\n\tAn array of RegEx objects used to determine whether a URL protocol is supported.\n\tThis is to deal with nonstandard protocol handlers that regular HTTP is\n\tsometimes given, like `feed:`. It does not provide support for non-http\n\tprotocols (and why would it!?)\n*\t`crawler.maxResourceSize` - \n\tThe maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n*\t`crawler.downloadUnsupported` -\n\tSimplecrawler will download files it can't parse. Defaults to true, but if\n\tyou'd rather save the RAM and GC lag, switch it off.\n*\t`crawler.needsAuth` -\n\tFlag to specify if the domain you are hitting requires basic authentication\n*\t`crawler.authUser` -\n\tUsername provided for needsAuth flag\n*\t`crawler.authPass` -\n\tPassword provided for needsAuth flag\n*\t`crawler.customHeaders` -\n\tAn object specifying a number of custom headers simplecrawler will add to\n\tevery request. These override the default headers simplecrawler sets, so\n\tbe careful with them. If you want to tamper with headers on a per-request basis,\n\tsee the `fetchqueue` event.\n*\t`crawler.acceptCookies` -\n\tFlag to indicate if the crawler should hold on to cookies\n*\t`crawler.urlEncoding` -\n\tSet this to `iso8859` to trigger URIjs' re-encoding of iso8859 URLs to unicode.\n\tDefaults to `unicode`.\n\n#### Excluding certain resources from downloading\n\nSimplecrawler has a mechanism you can use to prevent certain resources from being\nfetched, based on the URL, called *Fetch Conditions**. A fetch condition is just\na function, which, when given a parsed URL object, will return a true or a false\nvalue, indicating whether a given resource should be downloaded.\n\nYou may add as many fetch conditions as you like, and remove them at runtime.\nSimplecrawler will evaluate every single condition against every queued URL, and\nshould just one of them return a falsy value (this includes null and undefined,\nso remember to always return a value!) then the resource in question will not be\nfetched.\n\n##### Adding a fetch condition\n\nThis example fetch condition prevents URLs ending in `.pdf` from downloading.\nAdding a fetch condition assigns it an ID, which the `addFetchCondition` function\nreturns. You can use this ID to remove the condition later.\n\n```javascript\nvar conditionID = myCrawler.addFetchCondition(function(parsedURL) {\n\treturn !parsedURL.path.match(/\\.pdf$/i);\n});\n```\n\nNOTE: simplecrawler uses slightly different terminology to URIjs. `parsedURL.path`\nincludes the query string too. If you want the path without the query string,\nuse `parsedURL.uriPath`.\n\n##### Removing a fetch condition\n\nIf you stored the ID of the fetch condition you added earlier, you can remove it\nfrom the crawler:\n\n```javascript\nmyCrawler.removeFetchCondition(conditionID);\n```\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed\nat `crawler.queue` (assuming you called your Crawler() object `crawler`.) It\nprovides array access, so you can get to queue items just with array notation\nand an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate\ninterface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nThe simplest way to add to the queue is to use the crawler's own method,\n`crawler.queueURL`. This method takes a complete URL, validates and deconstructs\nit, and adds it to the queue.\n\nIf you instead want to add a resource by its components, you may call the\n`queue.add` method directly:\n\n```javascript\ncrawler.queue.add(protocol,hostname,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can\nremember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items,\nit helps to know what's inside them. These are the properties every queue item\nis expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `host` - The full domain/hostname of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The\nupside is, the queue actually has some convenient functions for getting simple\naggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network\nperformance of your crawl (so far.) This is done live, so don't check it thirty\ntimes a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the\n`crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions\nrespectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given\nstatus at any one time, and/or retreive them. That's easy with\n`crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.countWithStatus` returns the number of queued items with a given\nstatus, while `crawler.queue.getWithStatus` returns an array of the queue items\nthemselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n*\t`crawler.queue.complete` - returns the number of queue items which have been\n\tcompleted (marked as fetched)\n*\t`crawler.queue.errors` - returns the number of requests which have failed\n\t(404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if\nyour application fails or you need to abort the crawl for some reason. (Perhaps\nyou just want to finish off for the night and pick it up tomorrow!) The\n`crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be\nasynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when\nyou need to save the queue - don't call them every request or your application's\nperformance will be incredibly poor - they block like *crazy*. That said, using\nthem when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n## Cookies\n\nSimplecrawler now has an internal cookie jar, which collects and resends cookies\nautomatically, and by default.\n\nIf you want to turn this off, set the `crawler.acceptCookies` option to `false`.\n\nThe cookie jar is accessible via `crawler.cookies`, and is an event emitter itself:\n\n### Cookie Events\n\n* `addcookie` ( cookie )\nFired when a new cookie is added to the jar.\n* `removecookie` ( cookie array )\nFired when one or more cookies are removed from the jar.\n\n## Building and Testing\n\n#### Build Status:\n\n* Master: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n* Development: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=development)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\n## Contributors\n\nI'd like to extend sincere thanks to:\n\n*\t[Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support, and\n\tinitial cookie support.\n*\t[Mike Moulton](https://github.com/mmoulton) for\n\t[fixing a bug in the URL discovery mechanism]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/3), as well as\n\t[adding the `discoverycomplete` event]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/10),\n*\t[Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword\n\tnaming collision with node 0.8's EventEmitter.\n*\t[Greg Molnar](https://github.com/gregmolnar) for\n\t[adding a querystring-free path parameter to parsed URL objects.]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/31)\n*\t[Breck Yunits](https://github.com/breck7) for contributing a useful code\n\tsample demonstrating using simplecrawler for caching a website to disk!\n*\t[Luke Plaster](https://github.com/notatestuser) for enabling protocol-agnostic\n\tlink discovery\n*\t[Zeus](https://github.com/distracteddev) for fixing a bug where [default port\n\tinfo was wrongly specified in requests]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/40)\n\tand for fixing the missing request timeout handling!\n*\t[Graham Hutchinson](https://github.com/ghhutch) for adding\n\tquerystring-stripping option \n*\t[Jellyfrog](https://github.com/jellyfrog) for assisting in diagnosing some\n\tnasty EventEmitter issues.\n*\t[Brian Moeskau](https://github.com/bmoeskau) for helping to fix the confusing\n\t'async' events API, and providing invaluable feedback.\n\nAnd everybody else who has helped out in some way! :)\n\n## Licence\n\nCopyright (c) 2013, Christopher Giffard.\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, \nare permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n* Redistributions in binary form must reproduce the above copyright notice, this\n  list of conditions and the following disclaimer in the documentation and/or\n  other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR \nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n","readmeFilename":"README.markdown","_id":"simplecrawler@0.3.2","dist":{"shasum":"e470bfaf8395671d84d45d5a2af5adf176929cd7","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.3.2.tgz"},"_from":".","_npmVersion":"1.3.8","_npmUser":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},{"name":"cgiffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.3.3":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.3.3","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["crawler","spider","cache","queue","simplecrawler","eventemitter"],"scripts":{"test":"mocha -R spec -t 5000"},"bin":{"crawl":"./lib/cli.js"},"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./lib/index.js","engines":{"node":">=0.8.0"},"devDependencies":{"mocha":"~1.8.1","jshint":"~0.7.x","chai":"~1.2.0"},"dependencies":{"URIjs":"~1.10.2"},"readme":"# Simple web-crawler for node.js [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\nSimplecrawler is designed to provide the most basic possible API for crawling\nwebsites, while being as flexible and robust as possible. I wrote simplecrawler\nto archive, analyse, and search some very large websites. It has happily chewed\nthrough 50,000 pages and written tens of gigabytes to disk without issue.\n\n#### Example (simple mode)\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n\nCrawler.crawl(\"http://example.com/\")\n\t.on(\"fetchcomplete\",function(queueItem){\n\t\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n\t});\n```\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can\nreplace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except\nwhen discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nThere are two ways of instantiating a new crawler - a simple but less flexible\nmethod inspired by [anemone](http://anemone.rubyforge.org), and the traditional\nmethod which provides a little more room to configure crawl parameters.\n\nRegardless of wether you use the simple or traditional methods of instantiation,\nyou'll need to require simplecrawler:\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n```\n\n#### Simple Mode\n\nSimple mode generates a new crawler for you, preconfigures it based on a URL you\nprovide, and returns the crawler to you for further configuration and so you can\nattach event handlers.\n\nSimply call `Crawler.crawl`, with a URL first parameter, and two optional\nfunctions that will be added as event listeners for `fetchcomplete` and\n`fetcherror` respectively.\n\n```javascript\nCrawler.crawl(\"http://example.com/\", function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\nAlternately, if you decide to omit these functions, you can use the returned\ncrawler object to add the event listeners yourself, and tweak configuration\noptions:\n\n```javascript\nvar crawler = Crawler.crawl(\"http://example.com/\");\n\ncrawler.interval = 500;\n\ncrawler.on(\"fetchcomplete\",function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\n#### Advanced Mode\n\nThe alternative method of creating a crawler is to call the `simplecrawler`\nconstructor yourself, and to initiate the crawl manually.\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your web\nserver. Decrease the concurrency from five simultaneous requests - and increase\nthe request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when\ncreating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run\nthrough its queue finding linked resources on the domain to download, until it\ncan't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `crawlstart`\nFired when the crawl begins or is restarted.\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually\nqueue an item yourself.)\n* `queueduplicate` ( URLData )\nFired when an item cannot be added to the queue because it is already present in\nthe queue. Frequent firing of this event is normal and expected.\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem , requestOptions )\nFired when an item is spooled for fetching. If your event handler is synchronous,\nyou can modify the crawler request options (including headers) \n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http\nresponse object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided\nas a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size\nwe're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned\nas a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a\nrequest.\n* `fetchtimeout` ( queueItem, crawlerTimeoutValue )\nFired when a request time exceeds the internal crawler threshold.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as\nthe second parameter.\n* `discoverycomplete` ( queueItem, resources )\nFired when linked resources have been discovered. Passes an array of resources\n(as URLs) as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does\nnot find any more to add. This event returns no arguments.\n\n#### A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters\nan HTTP error status in the response. If you need this information, you can listen\nto simplecrawler's error events, and through node's native `data` event\n(`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let\nme know. I didn't include it because I didn't need it - but if it's important to\npeople I might put it back in. :)\n\n#### Waiting for Asynchronous Event Listeners\n\nSometimes, you might want to wait for simplecrawler to wait for you while you\nperform sone asynchronous tasks in an event listener, instead of having it\nracing off and firing the `complete` event, halting your crawl. For example,\nif you're doing your own link discovery using an asynchronous library method.\n\nSimplecrawler provides a `wait` method you can call at any time. It is available\nvia `this` from inside listeners, and on the crawler object itself. It returns\na callback function.\n\nOnce you've called this method, simplecrawler will not fire the `complete` event\nuntil either you execute the callback it returns, or a timeout is reached\n(configured in `crawler.listenerTTL`, by default 10000 msec.)\n\n##### Example Asynchronous Event Listener\n\n```javascript\ncrawler.on(\"fetchcomplete\",function(queueItem,data,res) {\n\tvar continue = this.wait();\n\tdoSomeDiscovery(data,function(foundURLs){\n\t\tfoundURLs.forEach(crawler.queueURL.bind(crawler));\n\t\tcontinue();\n\t});\n});\n```\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n*\t`crawler.host` -\n\tThe domain to scan. By default, simplecrawler will restrict all requests to\n\tthis domain.\n*\t`crawler.initialPath` -\n\tThe initial path with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialPort` -\n\tThe initial port with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialProtocol` -\n\tThe initial protocol with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.interval` -\n\tThe interval with which the crawler will spool up new requests (one per\n\ttick.) Defaults to 250ms.\n*\t`crawler.maxConcurrency` -\n\tThe maximum number of requests the crawler will run simultaneously. Defaults\n\tto 5 - the default number of http agents node will run.\n*\t`crawler.timeout` -\n\tThe maximum time in milliseconds the crawler will wait for headers before\n\taborting the request.\n*\t`crawler.listenerTTL` -\n\tThe maximum time in milliseconds the crawler will wait for async listeners.\n*\t`crawler.userAgent` -\n\tThe user agent the crawler will report. Defaults to\n\t`Node/SimpleCrawler <version> (http://www.github.com/cgiffard/node-simplecrawler)`.\n*\t`crawler.queue` -\n\tThe queue in use by the crawler (Must implement the `FetchQueue` interface)\n*\t`crawler.filterByDomain` -\n\tSpecifies whether the crawler will restrict queued requests to a given\n\tdomain/domains.\n*\t`crawler.scanSubdomains` -\n\tEnables scanning subdomains (other than www) as well as the specified domain.\n\tDefaults to false.\n*\t`crawler.ignoreWWWDomain` -\n\tTreats the `www` domain the same as the originally specified domain.\n\tDefaults to true.\n*\t`crawler.stripWWWDomain` -\n\tOr go even further and strip WWW subdomain from requests altogether!\n*\t`crawler.stripQuerystring` -\n\tSpecify to strip querystring parameters from URLs. Defaults to false.\n*\t`crawler.discoverResources` -\n\tUse simplecrawler's internal resource discovery function. Defaults to true.\n\t(switch it off if you'd prefer to discover and queue resources yourself!)\n*\t`crawler.discoverRegex` -\n\tArray of regex objects that simplecrawler uses to discover resources.\n*\t`crawler.cache` -\n\tSpecify a cache architecture to use when crawling. Must implement\n\t`SimpleCache` interface.\n*\t`crawler.useProxy` -\n\tThe crawler should use an HTTP proxy to make its requests.\n*\t`crawler.proxyHostname` -\n\tThe hostname of the proxy to use for requests.\n*\t`crawler.proxyPort` -\n\tThe port of the proxy to use for requests.\n*\t`crawler.domainWhitelist` -\n\tAn array of domains the crawler is permitted to crawl from. If other settings\n\tare more permissive, they will override this setting.\n*\t`crawler.supportedMimeTypes` -\n\tAn array of RegEx objects used to determine supported MIME types (types of\n\tdata simplecrawler will scan for links.) If you're  not using simplecrawler's\n\tresource discovery function, this won't have any effect.\n*\t`crawler.allowedProtocols` -\n\tAn array of RegEx objects used to determine whether a URL protocol is supported.\n\tThis is to deal with nonstandard protocol handlers that regular HTTP is\n\tsometimes given, like `feed:`. It does not provide support for non-http\n\tprotocols (and why would it!?)\n*\t`crawler.maxResourceSize` - \n\tThe maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n*\t`crawler.downloadUnsupported` -\n\tSimplecrawler will download files it can't parse. Defaults to true, but if\n\tyou'd rather save the RAM and GC lag, switch it off.\n*\t`crawler.needsAuth` -\n\tFlag to specify if the domain you are hitting requires basic authentication\n*\t`crawler.authUser` -\n\tUsername provided for needsAuth flag\n*\t`crawler.authPass` -\n\tPassword provided for needsAuth flag\n*\t`crawler.customHeaders` -\n\tAn object specifying a number of custom headers simplecrawler will add to\n\tevery request. These override the default headers simplecrawler sets, so\n\tbe careful with them. If you want to tamper with headers on a per-request basis,\n\tsee the `fetchqueue` event.\n*\t`crawler.acceptCookies` -\n\tFlag to indicate if the crawler should hold on to cookies\n*\t`crawler.urlEncoding` -\n\tSet this to `iso8859` to trigger URIjs' re-encoding of iso8859 URLs to unicode.\n\tDefaults to `unicode`.\n\n#### Excluding certain resources from downloading\n\nSimplecrawler has a mechanism you can use to prevent certain resources from being\nfetched, based on the URL, called *Fetch Conditions**. A fetch condition is just\na function, which, when given a parsed URL object, will return a true or a false\nvalue, indicating whether a given resource should be downloaded.\n\nYou may add as many fetch conditions as you like, and remove them at runtime.\nSimplecrawler will evaluate every single condition against every queued URL, and\nshould just one of them return a falsy value (this includes null and undefined,\nso remember to always return a value!) then the resource in question will not be\nfetched.\n\n##### Adding a fetch condition\n\nThis example fetch condition prevents URLs ending in `.pdf` from downloading.\nAdding a fetch condition assigns it an ID, which the `addFetchCondition` function\nreturns. You can use this ID to remove the condition later.\n\n```javascript\nvar conditionID = myCrawler.addFetchCondition(function(parsedURL) {\n\treturn !parsedURL.path.match(/\\.pdf$/i);\n});\n```\n\nNOTE: simplecrawler uses slightly different terminology to URIjs. `parsedURL.path`\nincludes the query string too. If you want the path without the query string,\nuse `parsedURL.uriPath`.\n\n##### Removing a fetch condition\n\nIf you stored the ID of the fetch condition you added earlier, you can remove it\nfrom the crawler:\n\n```javascript\nmyCrawler.removeFetchCondition(conditionID);\n```\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed\nat `crawler.queue` (assuming you called your Crawler() object `crawler`.) It\nprovides array access, so you can get to queue items just with array notation\nand an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate\ninterface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nThe simplest way to add to the queue is to use the crawler's own method,\n`crawler.queueURL`. This method takes a complete URL, validates and deconstructs\nit, and adds it to the queue.\n\nIf you instead want to add a resource by its components, you may call the\n`queue.add` method directly:\n\n```javascript\ncrawler.queue.add(protocol,hostname,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can\nremember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items,\nit helps to know what's inside them. These are the properties every queue item\nis expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `host` - The full domain/hostname of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The\nupside is, the queue actually has some convenient functions for getting simple\naggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network\nperformance of your crawl (so far.) This is done live, so don't check it thirty\ntimes a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the\n`crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions\nrespectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given\nstatus at any one time, and/or retreive them. That's easy with\n`crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.countWithStatus` returns the number of queued items with a given\nstatus, while `crawler.queue.getWithStatus` returns an array of the queue items\nthemselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n*\t`crawler.queue.complete` - returns the number of queue items which have been\n\tcompleted (marked as fetched)\n*\t`crawler.queue.errors` - returns the number of requests which have failed\n\t(404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if\nyour application fails or you need to abort the crawl for some reason. (Perhaps\nyou just want to finish off for the night and pick it up tomorrow!) The\n`crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be\nasynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when\nyou need to save the queue - don't call them every request or your application's\nperformance will be incredibly poor - they block like *crazy*. That said, using\nthem when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n## Cookies\n\nSimplecrawler now has an internal cookie jar, which collects and resends cookies\nautomatically, and by default.\n\nIf you want to turn this off, set the `crawler.acceptCookies` option to `false`.\n\nThe cookie jar is accessible via `crawler.cookies`, and is an event emitter itself:\n\n### Cookie Events\n\n* `addcookie` ( cookie )\nFired when a new cookie is added to the jar.\n* `removecookie` ( cookie array )\nFired when one or more cookies are removed from the jar.\n\n## Building and Testing\n\n#### Build Status:\n\n* Master: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n* Development: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=development)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\n## Contributors\n\nI'd like to extend sincere thanks to:\n\n*\t[Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support, and\n\tinitial cookie support.\n*\t[Mike Moulton](https://github.com/mmoulton) for\n\t[fixing a bug in the URL discovery mechanism]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/3), as well as\n\t[adding the `discoverycomplete` event]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/10),\n*\t[Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword\n\tnaming collision with node 0.8's EventEmitter.\n*\t[Greg Molnar](https://github.com/gregmolnar) for\n\t[adding a querystring-free path parameter to parsed URL objects.]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/31)\n*\t[Breck Yunits](https://github.com/breck7) for contributing a useful code\n\tsample demonstrating using simplecrawler for caching a website to disk!\n*\t[Luke Plaster](https://github.com/notatestuser) for enabling protocol-agnostic\n\tlink discovery\n*\t[Zeus](https://github.com/distracteddev) for fixing a bug where [default port\n\tinfo was wrongly specified in requests]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/40)\n\tand for fixing the missing request timeout handling!\n*\t[Graham Hutchinson](https://github.com/ghhutch) for adding\n\tquerystring-stripping option \n*\t[Jellyfrog](https://github.com/jellyfrog) for assisting in diagnosing some\n\tnasty EventEmitter issues.\n*\t[Brian Moeskau](https://github.com/bmoeskau) for helping to fix the confusing\n\t'async' events API, and providing invaluable feedback.\n\nAnd everybody else who has helped out in some way! :)\n\n## Licence\n\nCopyright (c) 2013, Christopher Giffard.\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, \nare permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n* Redistributions in binary form must reproduce the above copyright notice, this\n  list of conditions and the following disclaimer in the documentation and/or\n  other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR \nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n","readmeFilename":"README.markdown","_id":"simplecrawler@0.3.3","dist":{"shasum":"2166ee68fe50438522f1ee02a7c19cc68b066fbe","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.3.3.tgz"},"_from":".","_npmVersion":"1.3.8","_npmUser":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},{"name":"cgiffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.3.4":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.3.4","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["simple","crawler","spider","cache","queue","simplecrawler","eventemitter"],"scripts":{"test":"mocha -R spec -t 5000"},"bin":{"crawl":"./lib/cli.js"},"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./lib/index.js","engines":{"node":">=0.8.0"},"devDependencies":{"mocha":"~1.8.1","jshint":"~0.7.x","chai":"~1.2.0"},"dependencies":{"URIjs":"~1.10.2"},"readme":"# Simple web-crawler for node.js [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\nSimplecrawler is designed to provide the most basic possible API for crawling\nwebsites, while being as flexible and robust as possible. I wrote simplecrawler\nto archive, analyse, and search some very large websites. It has happily chewed\nthrough 50,000 pages and written tens of gigabytes to disk without issue.\n\n#### Example (simple mode)\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n\nCrawler.crawl(\"http://example.com/\")\n\t.on(\"fetchcomplete\",function(queueItem){\n\t\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n\t});\n```\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can\nreplace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except\nwhen discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nThere are two ways of instantiating a new crawler - a simple but less flexible\nmethod inspired by [anemone](http://anemone.rubyforge.org), and the traditional\nmethod which provides a little more room to configure crawl parameters.\n\nRegardless of wether you use the simple or traditional methods of instantiation,\nyou'll need to require simplecrawler:\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n```\n\n#### Simple Mode\n\nSimple mode generates a new crawler for you, preconfigures it based on a URL you\nprovide, and returns the crawler to you for further configuration and so you can\nattach event handlers.\n\nSimply call `Crawler.crawl`, with a URL first parameter, and two optional\nfunctions that will be added as event listeners for `fetchcomplete` and\n`fetcherror` respectively.\n\n```javascript\nCrawler.crawl(\"http://example.com/\", function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\nAlternately, if you decide to omit these functions, you can use the returned\ncrawler object to add the event listeners yourself, and tweak configuration\noptions:\n\n```javascript\nvar crawler = Crawler.crawl(\"http://example.com/\");\n\ncrawler.interval = 500;\n\ncrawler.on(\"fetchcomplete\",function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\n#### Advanced Mode\n\nThe alternative method of creating a crawler is to call the `simplecrawler`\nconstructor yourself, and to initiate the crawl manually.\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your web\nserver. Decrease the concurrency from five simultaneous requests - and increase\nthe request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when\ncreating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run\nthrough its queue finding linked resources on the domain to download, until it\ncan't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `crawlstart`\nFired when the crawl begins or is restarted.\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually\nqueue an item yourself.)\n* `queueduplicate` ( URLData )\nFired when an item cannot be added to the queue because it is already present in\nthe queue. Frequent firing of this event is normal and expected.\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem , requestOptions )\nFired when an item is spooled for fetching. If your event handler is synchronous,\nyou can modify the crawler request options (including headers) \n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http\nresponse object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided\nas a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size\nwe're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned\nas a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a\nrequest.\n* `fetchtimeout` ( queueItem, crawlerTimeoutValue )\nFired when a request time exceeds the internal crawler threshold.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as\nthe second parameter.\n* `discoverycomplete` ( queueItem, resources )\nFired when linked resources have been discovered. Passes an array of resources\n(as URLs) as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does\nnot find any more to add. This event returns no arguments.\n\n#### A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters\nan HTTP error status in the response. If you need this information, you can listen\nto simplecrawler's error events, and through node's native `data` event\n(`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let\nme know. I didn't include it because I didn't need it - but if it's important to\npeople I might put it back in. :)\n\n#### Waiting for Asynchronous Event Listeners\n\nSometimes, you might want to wait for simplecrawler to wait for you while you\nperform sone asynchronous tasks in an event listener, instead of having it\nracing off and firing the `complete` event, halting your crawl. For example,\nif you're doing your own link discovery using an asynchronous library method.\n\nSimplecrawler provides a `wait` method you can call at any time. It is available\nvia `this` from inside listeners, and on the crawler object itself. It returns\na callback function.\n\nOnce you've called this method, simplecrawler will not fire the `complete` event\nuntil either you execute the callback it returns, or a timeout is reached\n(configured in `crawler.listenerTTL`, by default 10000 msec.)\n\n##### Example Asynchronous Event Listener\n\n```javascript\ncrawler.on(\"fetchcomplete\",function(queueItem,data,res) {\n\tvar continue = this.wait();\n\tdoSomeDiscovery(data,function(foundURLs){\n\t\tfoundURLs.forEach(crawler.queueURL.bind(crawler));\n\t\tcontinue();\n\t});\n});\n```\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n*\t`crawler.host` -\n\tThe domain to scan. By default, simplecrawler will restrict all requests to\n\tthis domain.\n*\t`crawler.initialPath` -\n\tThe initial path with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialPort` -\n\tThe initial port with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialProtocol` -\n\tThe initial protocol with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.interval` -\n\tThe interval with which the crawler will spool up new requests (one per\n\ttick.) Defaults to 250ms.\n*\t`crawler.maxConcurrency` -\n\tThe maximum number of requests the crawler will run simultaneously. Defaults\n\tto 5 - the default number of http agents node will run.\n*\t`crawler.timeout` -\n\tThe maximum time in milliseconds the crawler will wait for headers before\n\taborting the request.\n*\t`crawler.listenerTTL` -\n\tThe maximum time in milliseconds the crawler will wait for async listeners.\n*\t`crawler.userAgent` -\n\tThe user agent the crawler will report. Defaults to\n\t`Node/SimpleCrawler <version> (http://www.github.com/cgiffard/node-simplecrawler)`.\n*\t`crawler.queue` -\n\tThe queue in use by the crawler (Must implement the `FetchQueue` interface)\n*\t`crawler.filterByDomain` -\n\tSpecifies whether the crawler will restrict queued requests to a given\n\tdomain/domains.\n*\t`crawler.scanSubdomains` -\n\tEnables scanning subdomains (other than www) as well as the specified domain.\n\tDefaults to false.\n*\t`crawler.ignoreWWWDomain` -\n\tTreats the `www` domain the same as the originally specified domain.\n\tDefaults to true.\n*\t`crawler.stripWWWDomain` -\n\tOr go even further and strip WWW subdomain from requests altogether!\n*\t`crawler.stripQuerystring` -\n\tSpecify to strip querystring parameters from URLs. Defaults to false.\n*\t`crawler.discoverResources` -\n\tUse simplecrawler's internal resource discovery function. Defaults to true.\n\t(switch it off if you'd prefer to discover and queue resources yourself!)\n*\t`crawler.discoverRegex` -\n\tArray of regex objects that simplecrawler uses to discover resources.\n*\t`crawler.cache` -\n\tSpecify a cache architecture to use when crawling. Must implement\n\t`SimpleCache` interface.\n*\t`crawler.useProxy` -\n\tThe crawler should use an HTTP proxy to make its requests.\n*\t`crawler.proxyHostname` -\n\tThe hostname of the proxy to use for requests.\n*\t`crawler.proxyPort` -\n\tThe port of the proxy to use for requests.\n*\t`crawler.proxyUser` -\n\tThe username for HTTP/Basic proxy authentication (leave unset for unauthenticated proxies.)\n*\t`crawler.proxyPass` -\n\tThe password for HTTP/Basic proxy authentication (leave unset for unauthenticated proxies.)\n*\t`crawler.domainWhitelist` -\n\tAn array of domains the crawler is permitted to crawl from. If other settings\n\tare more permissive, they will override this setting.\n*\t`crawler.supportedMimeTypes` -\n\tAn array of RegEx objects used to determine supported MIME types (types of\n\tdata simplecrawler will scan for links.) If you're  not using simplecrawler's\n\tresource discovery function, this won't have any effect.\n*\t`crawler.allowedProtocols` -\n\tAn array of RegEx objects used to determine whether a URL protocol is supported.\n\tThis is to deal with nonstandard protocol handlers that regular HTTP is\n\tsometimes given, like `feed:`. It does not provide support for non-http\n\tprotocols (and why would it!?)\n*\t`crawler.maxResourceSize` - \n\tThe maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n*\t`crawler.downloadUnsupported` -\n\tSimplecrawler will download files it can't parse. Defaults to true, but if\n\tyou'd rather save the RAM and GC lag, switch it off.\n*\t`crawler.needsAuth` -\n\tFlag to specify if the domain you are hitting requires basic authentication\n*\t`crawler.authUser` -\n\tUsername provided for needsAuth flag\n*\t`crawler.authPass` -\n\tPassword provided for needsAuth flag\n*\t`crawler.customHeaders` -\n\tAn object specifying a number of custom headers simplecrawler will add to\n\tevery request. These override the default headers simplecrawler sets, so\n\tbe careful with them. If you want to tamper with headers on a per-request basis,\n\tsee the `fetchqueue` event.\n*\t`crawler.acceptCookies` -\n\tFlag to indicate if the crawler should hold on to cookies\n*\t`crawler.urlEncoding` -\n\tSet this to `iso8859` to trigger URIjs' re-encoding of iso8859 URLs to unicode.\n\tDefaults to `unicode`.\n\n#### Excluding certain resources from downloading\n\nSimplecrawler has a mechanism you can use to prevent certain resources from being\nfetched, based on the URL, called *Fetch Conditions**. A fetch condition is just\na function, which, when given a parsed URL object, will return a true or a false\nvalue, indicating whether a given resource should be downloaded.\n\nYou may add as many fetch conditions as you like, and remove them at runtime.\nSimplecrawler will evaluate every single condition against every queued URL, and\nshould just one of them return a falsy value (this includes null and undefined,\nso remember to always return a value!) then the resource in question will not be\nfetched.\n\n##### Adding a fetch condition\n\nThis example fetch condition prevents URLs ending in `.pdf` from downloading.\nAdding a fetch condition assigns it an ID, which the `addFetchCondition` function\nreturns. You can use this ID to remove the condition later.\n\n```javascript\nvar conditionID = myCrawler.addFetchCondition(function(parsedURL) {\n\treturn !parsedURL.path.match(/\\.pdf$/i);\n});\n```\n\nNOTE: simplecrawler uses slightly different terminology to URIjs. `parsedURL.path`\nincludes the query string too. If you want the path without the query string,\nuse `parsedURL.uriPath`.\n\n##### Removing a fetch condition\n\nIf you stored the ID of the fetch condition you added earlier, you can remove it\nfrom the crawler:\n\n```javascript\nmyCrawler.removeFetchCondition(conditionID);\n```\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed\nat `crawler.queue` (assuming you called your Crawler() object `crawler`.) It\nprovides array access, so you can get to queue items just with array notation\nand an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate\ninterface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nThe simplest way to add to the queue is to use the crawler's own method,\n`crawler.queueURL`. This method takes a complete URL, validates and deconstructs\nit, and adds it to the queue.\n\nIf you instead want to add a resource by its components, you may call the\n`queue.add` method directly:\n\n```javascript\ncrawler.queue.add(protocol,hostname,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can\nremember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items,\nit helps to know what's inside them. These are the properties every queue item\nis expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `host` - The full domain/hostname of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The\nupside is, the queue actually has some convenient functions for getting simple\naggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network\nperformance of your crawl (so far.) This is done live, so don't check it thirty\ntimes a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the\n`crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions\nrespectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given\nstatus at any one time, and/or retreive them. That's easy with\n`crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.countWithStatus` returns the number of queued items with a given\nstatus, while `crawler.queue.getWithStatus` returns an array of the queue items\nthemselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n*\t`crawler.queue.complete` - returns the number of queue items which have been\n\tcompleted (marked as fetched)\n*\t`crawler.queue.errors` - returns the number of requests which have failed\n\t(404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if\nyour application fails or you need to abort the crawl for some reason. (Perhaps\nyou just want to finish off for the night and pick it up tomorrow!) The\n`crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be\nasynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when\nyou need to save the queue - don't call them every request or your application's\nperformance will be incredibly poor - they block like *crazy*. That said, using\nthem when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n## Cookies\n\nSimplecrawler now has an internal cookie jar, which collects and resends cookies\nautomatically, and by default.\n\nIf you want to turn this off, set the `crawler.acceptCookies` option to `false`.\n\nThe cookie jar is accessible via `crawler.cookies`, and is an event emitter itself:\n\n### Cookie Events\n\n* `addcookie` ( cookie )\nFired when a new cookie is added to the jar.\n* `removecookie` ( cookie array )\nFired when one or more cookies are removed from the jar.\n\n## Building and Testing\n\n#### Build Status:\n\n* Master: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n* Development: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=development)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\n## Contributors\n\nI'd like to extend sincere thanks to:\n\n*\t[Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support, and\n\tinitial cookie support.\n*\t[Mike Moulton](https://github.com/mmoulton) for\n\t[fixing a bug in the URL discovery mechanism]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/3), as well as\n\t[adding the `discoverycomplete` event]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/10),\n*\t[Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword\n\tnaming collision with node 0.8's EventEmitter.\n*\t[Greg Molnar](https://github.com/gregmolnar) for\n\t[adding a querystring-free path parameter to parsed URL objects.]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/31)\n*\t[Breck Yunits](https://github.com/breck7) for contributing a useful code\n\tsample demonstrating using simplecrawler for caching a website to disk!\n*\t[Luke Plaster](https://github.com/notatestuser) for enabling protocol-agnostic\n\tlink discovery\n*\t[Zeus](https://github.com/distracteddev) for fixing a bug where [default port\n\tinfo was wrongly specified in requests]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/40)\n\tand for fixing the missing request timeout handling!\n*\t[Graham Hutchinson](https://github.com/ghhutch) for adding\n\tquerystring-stripping option \n*\t[Jellyfrog](https://github.com/jellyfrog) for assisting in diagnosing some\n\tnasty EventEmitter issues.\n*\t[Brian Moeskau](https://github.com/bmoeskau) for helping to fix the confusing\n\t'async' events API, and providing invaluable feedback.\n\nAnd everybody else who has helped out in some way! :)\n\n## Licence\n\nCopyright (c) 2013, Christopher Giffard.\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, \nare permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n* Redistributions in binary form must reproduce the above copyright notice, this\n  list of conditions and the following disclaimer in the documentation and/or\n  other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR \nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n","readmeFilename":"README.markdown","_id":"simplecrawler@0.3.4","dist":{"shasum":"9a1241dd59889d35557d7534b17a872d347b7620","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.3.4.tgz"},"_from":".","_npmVersion":"1.3.8","_npmUser":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},{"name":"cgiffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}},"0.3.5":{"name":"simplecrawler","description":"Very straigntforward web crawler. Uses EventEmitter. Generates queue statistics and has a basic cache mechanism with extensible backend.","version":"0.3.5","homepage":"http://github.com/cgiffard/node-simplecrawler","author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"keywords":["simple","crawler","spider","cache","queue","simplecrawler","eventemitter"],"scripts":{"test":"mocha -R spec -t 5000"},"bin":{"crawl":"./lib/cli.js"},"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"bugs":{"url":"https://github.com/cgiffard/node-simplecrawler/issues"},"main":"./lib/index.js","engines":{"node":">=0.8.0"},"devDependencies":{"mocha":"~1.8.1","jshint":"~0.7.x","chai":"~1.2.0"},"dependencies":{"URIjs":"~1.10.2"},"readme":"# Simple web-crawler for node.js [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\nSimplecrawler is designed to provide the most basic possible API for crawling\nwebsites, while being as flexible and robust as possible. I wrote simplecrawler\nto archive, analyse, and search some very large websites. It has happily chewed\nthrough 50,000 pages and written tens of gigabytes to disk without issue.\n\n#### Example (simple mode)\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n\nCrawler.crawl(\"http://example.com/\")\n\t.on(\"fetchcomplete\",function(queueItem){\n\t\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n\t});\n```\n\n### What does simplecrawler do?\n\n* Provides a very simple event driven API using `EventEmitter`\n* Extremely configurable base for writing your own crawler\n* Provides some simple logic for autodetecting linked resources - which you can\nreplace or augment\n* Has a flexible queue system which can be frozen to disk and defrosted\n* Provides basic statistics on network performance\n* Uses buffers for fetching and managing data, preserving binary data (except\nwhen discovering links)\n\n### Installation\n\n```\nnpm install simplecrawler\n```\n\n### Getting Started\n\nThere are two ways of instantiating a new crawler - a simple but less flexible\nmethod inspired by [anemone](http://anemone.rubyforge.org), and the traditional\nmethod which provides a little more room to configure crawl parameters.\n\nRegardless of wether you use the simple or traditional methods of instantiation,\nyou'll need to require simplecrawler:\n\n```javascript\nvar Crawler = require(\"simplecrawler\");\n```\n\n#### Simple Mode\n\nSimple mode generates a new crawler for you, preconfigures it based on a URL you\nprovide, and returns the crawler to you for further configuration and so you can\nattach event handlers.\n\nSimply call `Crawler.crawl`, with a URL first parameter, and two optional\nfunctions that will be added as event listeners for `fetchcomplete` and\n`fetcherror` respectively.\n\n```javascript\nCrawler.crawl(\"http://example.com/\", function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\nAlternately, if you decide to omit these functions, you can use the returned\ncrawler object to add the event listeners yourself, and tweak configuration\noptions:\n\n```javascript\nvar crawler = Crawler.crawl(\"http://example.com/\");\n\ncrawler.interval = 500;\n\ncrawler.on(\"fetchcomplete\",function(queueItem){\n\tconsole.log(\"Completed fetching resource:\",queueItem.url);\n});\n```\n\n#### Advanced Mode\n\nThe alternative method of creating a crawler is to call the `simplecrawler`\nconstructor yourself, and to initiate the crawl manually.\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\");\n```\n\nNonstandard port? HTTPS? Want to start archiving a specific path? No problem:\n\n```javascript\nmyCrawler.initialPath = \"/archive\";\nmyCrawler.initialPort = 8080;\nmyCrawler.initialProtocol = \"https\";\n\n// Or:\nvar myCrawler = new Crawler(\"www.example.com\",\"/archive\",8080);\n\n```\n\nAnd of course, you're probably wanting to ensure you don't take down your web\nserver. Decrease the concurrency from five simultaneous requests - and increase\nthe request interval from the default 250ms like this:\n\n```javascript\nmyCrawler.interval = 10000; // Ten seconds\nmyCrawler.maxConcurrency = 1;\n```\n\nFor brevity, you may also specify the initial path and request interval when\ncreating the crawler:\n\n```javascript\nvar myCrawler = new Crawler(\"www.example.com\",\"/\",8080,300);\n```\n\n### Running the crawler\n\nFirst, you'll need to set up an event listener to get the fetched data:\n\n```javascript\nmyCrawler.on(\"fetchcomplete\",function(queueItem, responseBuffer, response) {\n\tconsole.log(\"I just received %s (%d bytes)\",queueItem.url,responseBuffer.length);\n\tconsole.log(\"It was a resource of type %s\",response.headers['content-type']);\n\t\n\t// Do something with the data in responseBuffer\n});\n```\n\nThen, when you're satisfied you're ready to go, start the crawler! It'll run\nthrough its queue finding linked resources on the domain to download, until it\ncan't find any more.\n\n```javascript\nmyCrawler.start();\n```\n\nOf course, once you've got that down pat, there's a fair bit more you can listen for...\n\n### Events\n\n* `crawlstart`\nFired when the crawl begins or is restarted.\n* `queueadd` ( queueItem )\nFired when a new item is automatically added to the queue (not when you manually\nqueue an item yourself.)\n* `queueduplicate` ( URLData )\nFired when an item cannot be added to the queue because it is already present in\nthe queue. Frequent firing of this event is normal and expected.\n* `queueerror` ( errorData , URLData )\nFired when an item cannot be added to the queue due to error.\n* `fetchstart` ( queueItem , requestOptions )\nFired when an item is spooled for fetching. If your event handler is synchronous,\nyou can modify the crawler request options (including headers) \n* `fetchheaders` ( queueItem , responseObject )\nFired when the headers for a resource are received from the server. The node http\nresponse object is returned for your perusal.\n* `fetchcomplete` ( queueItem , responseBuffer , response )\nFired when the resource is completely downloaded. The entire file data is provided\nas a buffer, as well as the response object.\n* `fetchdataerror` ( queueItem, response )\nFired when a resource can't be downloaded, because it exceeds the maximum size\nwe're prepared to receive (16MB by default.)\n* `fetchredirect` ( queueItem, parsedURL, response )\nFired when a redirect header is encountered. The new URL is validated and returned\nas a complete canonical link to the new resource.\n* `fetch404` ( queueItem, response )\nFired when a 404 HTTP status code is returned for a request.\n* `fetcherror` ( queueItem, response )\nFired when an alternate 400 or 500 series HTTP status code is returned for a\nrequest.\n* `fetchtimeout` ( queueItem, crawlerTimeoutValue )\nFired when a request time exceeds the internal crawler threshold.\n* `fetchclienterror` ( queueItem, errorData )\nFired when a request dies locally for some reason. The error data is returned as\nthe second parameter.\n* `discoverycomplete` ( queueItem, resources )\nFired when linked resources have been discovered. Passes an array of resources\n(as URLs) as the second parameter.\n* `complete`\nFired when the crawler completes processing all the items in its queue, and does\nnot find any more to add. This event returns no arguments.\n\n#### A note about HTTP error conditions\nBy default, simplecrawler does not download the response body when it encounters\nan HTTP error status in the response. If you need this information, you can listen\nto simplecrawler's error events, and through node's native `data` event\n(`response.on(\"data\",function(chunk) {...})`) you can save the information yourself.\n\nIf this is annoying, and you'd really like to retain error pages by default, let\nme know. I didn't include it because I didn't need it - but if it's important to\npeople I might put it back in. :)\n\n#### Waiting for Asynchronous Event Listeners\n\nSometimes, you might want to wait for simplecrawler to wait for you while you\nperform sone asynchronous tasks in an event listener, instead of having it\nracing off and firing the `complete` event, halting your crawl. For example,\nif you're doing your own link discovery using an asynchronous library method.\n\nSimplecrawler provides a `wait` method you can call at any time. It is available\nvia `this` from inside listeners, and on the crawler object itself. It returns\na callback function.\n\nOnce you've called this method, simplecrawler will not fire the `complete` event\nuntil either you execute the callback it returns, or a timeout is reached\n(configured in `crawler.listenerTTL`, by default 10000 msec.)\n\n##### Example Asynchronous Event Listener\n\n```javascript\ncrawler.on(\"fetchcomplete\",function(queueItem,data,res) {\n\tvar continue = this.wait();\n\tdoSomeDiscovery(data,function(foundURLs){\n\t\tfoundURLs.forEach(crawler.queueURL.bind(crawler));\n\t\tcontinue();\n\t});\n});\n```\n\n### Configuring the crawler\n\nHere's a complete list of what you can stuff with at this stage:\n\n*\t`crawler.host` -\n\tThe domain to scan. By default, simplecrawler will restrict all requests to\n\tthis domain.\n*\t`crawler.initialPath` -\n\tThe initial path with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialPort` -\n\tThe initial port with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.initialProtocol` -\n\tThe initial protocol with which the crawler will formulate its first request.\n\tDoes not restrict subsequent requests.\n*\t`crawler.interval` -\n\tThe interval with which the crawler will spool up new requests (one per\n\ttick.) Defaults to 250ms.\n*\t`crawler.maxConcurrency` -\n\tThe maximum number of requests the crawler will run simultaneously. Defaults\n\tto 5 - the default number of http agents node will run.\n*\t`crawler.timeout` -\n\tThe maximum time in milliseconds the crawler will wait for headers before\n\taborting the request.\n*\t`crawler.listenerTTL` -\n\tThe maximum time in milliseconds the crawler will wait for async listeners.\n*\t`crawler.userAgent` -\n\tThe user agent the crawler will report. Defaults to\n\t`Node/SimpleCrawler <version> (http://www.github.com/cgiffard/node-simplecrawler)`.\n*\t`crawler.queue` -\n\tThe queue in use by the crawler (Must implement the `FetchQueue` interface)\n*\t`crawler.filterByDomain` -\n\tSpecifies whether the crawler will restrict queued requests to a given\n\tdomain/domains.\n*\t`crawler.scanSubdomains` -\n\tEnables scanning subdomains (other than www) as well as the specified domain.\n\tDefaults to false.\n*\t`crawler.ignoreWWWDomain` -\n\tTreats the `www` domain the same as the originally specified domain.\n\tDefaults to true.\n*\t`crawler.stripWWWDomain` -\n\tOr go even further and strip WWW subdomain from requests altogether!\n*\t`crawler.stripQuerystring` -\n\tSpecify to strip querystring parameters from URLs. Defaults to false.\n*\t`crawler.discoverResources` -\n\tUse simplecrawler's internal resource discovery function. Defaults to true.\n\t(switch it off if you'd prefer to discover and queue resources yourself!)\n*\t`crawler.discoverRegex` -\n\tArray of regex objects that simplecrawler uses to discover resources.\n*\t`crawler.cache` -\n\tSpecify a cache architecture to use when crawling. Must implement\n\t`SimpleCache` interface.\n*\t`crawler.useProxy` -\n\tThe crawler should use an HTTP proxy to make its requests.\n*\t`crawler.proxyHostname` -\n\tThe hostname of the proxy to use for requests.\n*\t`crawler.proxyPort` -\n\tThe port of the proxy to use for requests.\n*\t`crawler.proxyUser` -\n\tThe username for HTTP/Basic proxy authentication (leave unset for unauthenticated proxies.)\n*\t`crawler.proxyPass` -\n\tThe password for HTTP/Basic proxy authentication (leave unset for unauthenticated proxies.)\n*\t`crawler.domainWhitelist` -\n\tAn array of domains the crawler is permitted to crawl from. If other settings\n\tare more permissive, they will override this setting.\n*\t`crawler.supportedMimeTypes` -\n\tAn array of RegEx objects used to determine supported MIME types (types of\n\tdata simplecrawler will scan for links.) If you're  not using simplecrawler's\n\tresource discovery function, this won't have any effect.\n*\t`crawler.allowedProtocols` -\n\tAn array of RegEx objects used to determine whether a URL protocol is supported.\n\tThis is to deal with nonstandard protocol handlers that regular HTTP is\n\tsometimes given, like `feed:`. It does not provide support for non-http\n\tprotocols (and why would it!?)\n*\t`crawler.maxResourceSize` - \n\tThe maximum resource size, in bytes, which will be downloaded. Defaults to 16MB.\n*\t`crawler.downloadUnsupported` -\n\tSimplecrawler will download files it can't parse. Defaults to true, but if\n\tyou'd rather save the RAM and GC lag, switch it off.\n*\t`crawler.needsAuth` -\n\tFlag to specify if the domain you are hitting requires basic authentication\n*\t`crawler.authUser` -\n\tUsername provided for needsAuth flag\n*\t`crawler.authPass` -\n\tPassword provided for needsAuth flag\n*\t`crawler.customHeaders` -\n\tAn object specifying a number of custom headers simplecrawler will add to\n\tevery request. These override the default headers simplecrawler sets, so\n\tbe careful with them. If you want to tamper with headers on a per-request basis,\n\tsee the `fetchqueue` event.\n*\t`crawler.acceptCookies` -\n\tFlag to indicate if the crawler should hold on to cookies\n*\t`crawler.urlEncoding` -\n\tSet this to `iso8859` to trigger URIjs' re-encoding of iso8859 URLs to unicode.\n\tDefaults to `unicode`.\n\n#### Excluding certain resources from downloading\n\nSimplecrawler has a mechanism you can use to prevent certain resources from being\nfetched, based on the URL, called *Fetch Conditions**. A fetch condition is just\na function, which, when given a parsed URL object, will return a true or a false\nvalue, indicating whether a given resource should be downloaded.\n\nYou may add as many fetch conditions as you like, and remove them at runtime.\nSimplecrawler will evaluate every single condition against every queued URL, and\nshould just one of them return a falsy value (this includes null and undefined,\nso remember to always return a value!) then the resource in question will not be\nfetched.\n\n##### Adding a fetch condition\n\nThis example fetch condition prevents URLs ending in `.pdf` from downloading.\nAdding a fetch condition assigns it an ID, which the `addFetchCondition` function\nreturns. You can use this ID to remove the condition later.\n\n```javascript\nvar conditionID = myCrawler.addFetchCondition(function(parsedURL) {\n\treturn !parsedURL.path.match(/\\.pdf$/i);\n});\n```\n\nNOTE: simplecrawler uses slightly different terminology to URIjs. `parsedURL.path`\nincludes the query string too. If you want the path without the query string,\nuse `parsedURL.uriPath`.\n\n##### Removing a fetch condition\n\nIf you stored the ID of the fetch condition you added earlier, you can remove it\nfrom the crawler:\n\n```javascript\nmyCrawler.removeFetchCondition(conditionID);\n```\n\n### The Simplecrawler Queue\n\nSimplecrawler has a queue like any other web crawler. It can be directly accessed\nat `crawler.queue` (assuming you called your Crawler() object `crawler`.) It\nprovides array access, so you can get to queue items just with array notation\nand an index.\n\n```javascript\ncrawler.queue[5];\n```\n\nFor compatibility with different backing stores, it now provides an alternate\ninterface which the crawler core makes use of:\n\n```javascript\ncrawler.queue.get(5);\n```\n\nIt's not just an array though.\n\n#### Adding to the queue\n\nThe simplest way to add to the queue is to use the crawler's own method,\n`crawler.queueURL`. This method takes a complete URL, validates and deconstructs\nit, and adds it to the queue.\n\nIf you instead want to add a resource by its components, you may call the\n`queue.add` method directly:\n\n```javascript\ncrawler.queue.add(protocol,hostname,port,path);\n```\n\nThat's it! It's basically just a URL, but comma separated (that's how you can\nremember the order.)\n\n#### Queue items\n\nBecause when working with simplecrawler, you'll constantly be handed queue items,\nit helps to know what's inside them. These are the properties every queue item\nis expected to have:\n\n* `url` - The complete, canonical URL of the resource.\n* `protocol` - The protocol of the resource (http, https)\n* `host` - The full domain/hostname of the resource\n* `port` - The port of the resource\n* `path` - The bit of the URL after the domain - includes the querystring.\n* `fetched` - Has the request for this item been completed? You can monitor this as requests are processed.\n* `status` - The internal status of the item, always a string. This can be one of:\n\t* `queued` - The resource is in the queue to be fetched, but nothing's happened to it yet.\n\t* `spooled` - A request has been made to the remote server, but we're still waiting for a response.\n\t* `headers` - The headers for the resource have been received.\n\t* `downloaded` - The item has been entirely downloaded.\n\t* `redirected` - The resource request returned a 300 series response, with a Location header and a new URL.\n\t* `notfound` - The resource could not be found. (404)\n\t* `failed` - An error occurred when attempting to fetch the resource.\n* `stateData` - An object containing state data and other information about the request:\n\t* `requestLatency` - The time taken for headers to be received after the request was made.\n\t* `requestTime` - The total time taken for the request (including download time.)\n\t* `downloadTime` - The total time taken for the resource to be downloaded.\n\t* `contentLength` - The length (in bytes) of the returned content. Calculated based on the `content-length` header.\n\t* `contentType` - The MIME type of the content.\n\t* `code` - The HTTP status code returned for the request.\n\t* `headers` - An object containing the header information returned by the server. This is the object node returns as part of the `response` object.\n\t* `actualDataSize` - The length (in bytes) of the returned content. Calculated based on what is actually received, not the `content-length` header.\n\t* `sentIncorrectSize` - True if the data length returned by the server did not match what we were told to expect by the `content-length` header.\n\nYou can address these properties like you would any other object:\n\n```javascript\ncrawler.queue[52].url;\nqueueItem.stateData.contentLength;\nqueueItem.status === \"queued\";\n```\n\nAs you can see, you can get a lot of meta-information out about each request. The\nupside is, the queue actually has some convenient functions for getting simple\naggregate data about the queue...\n\n#### Queue Statistics and Reporting\n\nFirst of all, the queue can provide some basic statistics about the network\nperformance of your crawl (so far.) This is done live, so don't check it thirty\ntimes a second. You can test the following properties:\n\n* `requestTime`\n* `requestLatency`\n* `downloadTime`\n* `contentLength`\n* `actualDataSize`\n\nAnd you can get the maximum, minimum, and average values for each with the\n`crawler.queue.max`, `crawler.queue.min`, and `crawler.queue.avg` functions\nrespectively. Like so:\n\n```javascript\nconsole.log(\"The maximum request latency was %dms.\",crawler.queue.max(\"requestLatency\"));\nconsole.log(\"The minimum download time was %dms.\",crawler.queue.min(\"downloadTime\"));\nconsole.log(\"The average resource size received is %d bytes.\",crawler.queue.avg(\"actualDataSize\"));\n```\n\nYou'll probably often need to determine how many items in the queue have a given\nstatus at any one time, and/or retreive them. That's easy with\n`crawler.queue.countWithStatus` and `crawler.queue.getWithStatus`.\n\n`crawler.queue.countWithStatus` returns the number of queued items with a given\nstatus, while `crawler.queue.getWithStatus` returns an array of the queue items\nthemselves.\n\n```javascript\nvar redirectCount = crawler.queue.countWithStatus(\"redirected\");\n\ncrawler.queue.getWithStatus(\"failed\").forEach(function(queueItem) {\n\tconsole.log(\"Whoah, the request for %s failed!\",queueItem.url);\n\t\n\t// do something...\n});\n```\n\nThen there's some even simpler convenience functions:\n\n*\t`crawler.queue.complete` - returns the number of queue items which have been\n\tcompleted (marked as fetched)\n*\t`crawler.queue.errors` - returns the number of requests which have failed\n\t(404s and other 400/500 errors, as well as client errors)\n\n#### Saving and reloading the queue (freeze/defrost)\n\nYou'll probably want to be able to save your progress and reload it later, if\nyour application fails or you need to abort the crawl for some reason. (Perhaps\nyou just want to finish off for the night and pick it up tomorrow!) The\n`crawler.queue.freeze` and `crawler.queue.defrost` functions perform this task.\n\n**A word of warning though** - they are not CPU friendly or set up to be\nasynchronous, as they rely on JSON.parse and JSON.stringify. Use them only when\nyou need to save the queue - don't call them every request or your application's\nperformance will be incredibly poor - they block like *crazy*. That said, using\nthem when your crawler commences and stops is perfectly reasonable.\n\n```javascript\n// Freeze queue\ncrawler.queue.freeze(\"mysavedqueue.json\");\n\n// Defrost queue\ncrawler.queue.defrost(\"mysavedqueue.json\");\n```\n\n## Cookies\n\nSimplecrawler now has an internal cookie jar, which collects and resends cookies\nautomatically, and by default.\n\nIf you want to turn this off, set the `crawler.acceptCookies` option to `false`.\n\nThe cookie jar is accessible via `crawler.cookies`, and is an event emitter itself:\n\n### Cookie Events\n\n* `addcookie` ( cookie )\nFired when a new cookie is added to the jar.\n* `removecookie` ( cookie array )\nFired when one or more cookies are removed from the jar.\n\n## Building and Testing\n\n#### Build Status:\n\n* Master: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=master)](https://travis-ci.org/cgiffard/node-simplecrawler)\n* Development: [![Build Status](https://travis-ci.org/cgiffard/node-simplecrawler.png?branch=development)](https://travis-ci.org/cgiffard/node-simplecrawler)\n\n## Contributors\n\nI'd like to extend sincere thanks to:\n\n*\t[Nick Crohn](https://github.com/ncrohn) for the HTTP Basic auth support, and\n\tinitial cookie support.\n*\t[Mike Moulton](https://github.com/mmoulton) for\n\t[fixing a bug in the URL discovery mechanism]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/3), as well as\n\t[adding the `discoverycomplete` event]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/10),\n*\t[Mike Iannacone](https://github.com/mikeiannacone) for correcting a keyword\n\tnaming collision with node 0.8's EventEmitter.\n*\t[Greg Molnar](https://github.com/gregmolnar) for\n\t[adding a querystring-free path parameter to parsed URL objects.]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/31)\n*\t[Breck Yunits](https://github.com/breck7) for contributing a useful code\n\tsample demonstrating using simplecrawler for caching a website to disk!\n*\t[Luke Plaster](https://github.com/notatestuser) for enabling protocol-agnostic\n\tlink discovery\n*\t[Zeus](https://github.com/distracteddev) for fixing a bug where [default port\n\tinfo was wrongly specified in requests]\n\t(https://github.com/cgiffard/node-simplecrawler/pull/40)\n\tand for fixing the missing request timeout handling!\n*\t[Graham Hutchinson](https://github.com/ghhutch) for adding\n\tquerystring-stripping option \n*\t[Jellyfrog](https://github.com/jellyfrog) for assisting in diagnosing some\n\tnasty EventEmitter issues.\n*\t[Brian Moeskau](https://github.com/bmoeskau) for helping to fix the confusing\n\t'async' events API, and providing invaluable feedback.\n\nAnd everybody else who has helped out in some way! :)\n\n## Licence\n\nCopyright (c) 2013, Christopher Giffard.\n\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without modification, \nare permitted provided that the following conditions are met:\n\n* Redistributions of source code must retain the above copyright notice, this\n  list of conditions and the following disclaimer.\n* Redistributions in binary form must reproduce the above copyright notice, this\n  list of conditions and the following disclaimer in the documentation and/or\n  other materials provided with the distribution.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR \nANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON\nANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n","readmeFilename":"README.markdown","_id":"simplecrawler@0.3.5","dist":{"shasum":"3a73919fd0883d4ca2053d60bf670883303c3af7","tarball":"http://registry.npmjs.org/simplecrawler/-/simplecrawler-0.3.5.tgz"},"_from":".","_npmVersion":"1.3.8","_npmUser":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},{"name":"cgiffard","email":"christopher.giffard@cgiffard.com"}],"directories":{}}},"readme":"","maintainers":[{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},{"name":"cgiffard","email":"christopher.giffard@cgiffard.com"}],"time":{"0.0.3":"2012-05-15T01:50:09.073Z","0.0.4":"2012-05-16T02:15:52.295Z","0.0.5":"2012-06-06T06:42:49.218Z","0.0.6":"2012-07-19T23:11:01.142Z","0.0.7":"2012-09-09T10:44:06.543Z","0.0.8":"2012-09-19T02:26:07.687Z","0.0.9":"2012-12-28T10:22:08.749Z","0.0.10":"2013-01-15T04:20:28.663Z","0.1.0":"2013-02-08T05:49:37.061Z","0.1.1":"2013-02-10T22:47:11.440Z","0.1.2":"2013-02-10T23:50:19.772Z","0.1.3":"2013-02-11T21:20:14.724Z","0.1.4":"2013-02-12T04:03:40.979Z","0.1.5":"2013-02-13T04:34:04.830Z","0.1.6":"2013-02-15T10:57:30.874Z","0.1.7":"2013-04-10T10:14:33.314Z","0.2.0":"2013-04-20T07:13:45.448Z","0.2.1":"2013-04-26T11:21:02.717Z","0.2.2":"2013-05-05T00:18:34.563Z","0.2.3":"2013-05-07T12:37:46.165Z","0.2.4":"2013-05-07T13:17:07.646Z","0.2.5":"2013-05-07T13:35:13.571Z","0.2.6":"2013-05-10T06:49:46.475Z","0.2.7":"2013-06-03T02:14:21.794Z","0.2.8":"2013-06-04T13:31:43.082Z","0.2.9":"2013-07-31T04:46:20.308Z","0.2.10":"2013-09-11T00:40:44.107Z","0.3.0":"2013-09-23T09:15:29.022Z","0.3.1":"2013-09-26T04:43:29.315Z","0.3.2":"2013-09-26T05:02:41.756Z","0.3.3":"2013-09-26T05:38:12.620Z","0.3.4":"2013-10-01T00:11:42.341Z","0.3.5":"2013-10-03T02:01:34.080Z"},"author":{"name":"Christopher Giffard","email":"christopher.giffard@cgiffard.com"},"repository":{"type":"git","url":"http://github.com/cgiffard/node-simplecrawler.git"},"_attachments":{"simplecrawler-0.3.5.tgz":{"content_type":"application/octet-stream","revpos":83,"digest":"md5-RgYRpNB8qxQLS2QSlEWukQ==","length":32276,"stub":true},"simplecrawler-0.3.4.tgz":{"content_type":"application/octet-stream","revpos":81,"digest":"md5-+MGNlm0V9nZwGDdaR9iy8g==","length":32266,"stub":true},"simplecrawler-0.3.3.tgz":{"content_type":"application/octet-stream","revpos":79,"digest":"md5-4LCtxf/iYZ2o3otcgmPKNA==","length":32090,"stub":true},"simplecrawler-0.3.2.tgz":{"content_type":"application/octet-stream","revpos":77,"digest":"md5-LxmOGZYySgURA4Cn0lJjNA==","length":32101,"stub":true},"simplecrawler-0.3.1.tgz":{"content_type":"application/octet-stream","revpos":75,"digest":"md5-KrbXyb2/eBA4kqMTlbz1dQ==","length":31926,"stub":true},"simplecrawler-0.3.0.tgz":{"content_type":"application/octet-stream","revpos":73,"digest":"md5-fHHV0tdIYKXrCuQTCwhdCw==","length":31816,"stub":true},"simplecrawler-0.2.10.tgz":{"content_type":"application/octet-stream","revpos":71,"digest":"md5-3TgUwK75yyw6WkHRVUL89Q==","length":31525,"stub":true},"simplecrawler-0.2.9.tgz":{"content_type":"application/octet-stream","revpos":69,"digest":"md5-uTe8l6TY8Je9M2J1tcJ64Q==","length":30919,"stub":true},"simplecrawler-0.2.8.tgz":{"content_type":"application/octet-stream","revpos":67,"digest":"md5-FWjfwaMVvZBSRLQxRUsl9Q==","length":30761,"stub":true},"simplecrawler-0.2.7.tgz":{"content_type":"application/octet-stream","revpos":65,"digest":"md5-KCp8bYwxKbhEnEe0zoJALg==","length":30635,"stub":true},"simplecrawler-0.2.6.tgz":{"content_type":"application/octet-stream","revpos":63,"digest":"md5-Yt7eU9GT7s5IUH45dxb85w==","length":30429,"stub":true},"simplecrawler-0.2.5.tgz":{"content_type":"application/octet-stream","revpos":61,"digest":"md5-8JnsAVXzanP/deB0UpeTcw==","length":29639,"stub":true},"simplecrawler-0.2.4.tgz":{"content_type":"application/octet-stream","revpos":59,"digest":"md5-2RUEPpapu6twLxp4FCvBOA==","length":29561,"stub":true},"simplecrawler-0.2.3.tgz":{"content_type":"application/octet-stream","revpos":57,"digest":"md5-0+fzJaEjyJV0ea48JvM01A==","length":29513,"stub":true},"simplecrawler-0.2.2.tgz":{"content_type":"application/octet-stream","revpos":55,"digest":"md5-1rytSkbvO1f53CF61sDZbQ==","length":29566,"stub":true},"simplecrawler-0.2.1.tgz":{"content_type":"application/octet-stream","revpos":54,"digest":"md5-nEYWAHy/9z2c9pXw8teH9A==","length":29483,"stub":true},"simplecrawler-0.2.0.tgz":{"content_type":"application/octet-stream","revpos":52,"digest":"md5-cGj4CDxXCftuLx6zxyBkxg==","length":29451,"stub":true},"simplecrawler-0.1.7.tgz":{"content_type":"application/octet-stream","revpos":50,"digest":"md5-QyEkttFogUFl9JB40iqJ5g==","length":23029,"stub":true},"simplecrawler-0.1.6.tgz":{"content_type":"application/octet-stream","revpos":48,"digest":"md5-auoXax2e+l13D1BMQ0oxFQ==","length":22849,"stub":true},"simplecrawler-0.1.5.tgz":{"content_type":"application/octet-stream","revpos":46,"digest":"md5-NiwxFznA4+/TU279i7iFsw==","length":22731,"stub":true},"simplecrawler-0.1.4.tgz":{"content_type":"application/octet-stream","revpos":44,"digest":"md5-I+FBz6YroTfYAM90NsUCqQ==","length":21236,"stub":true},"simplecrawler-0.1.3.tgz":{"content_type":"application/octet-stream","revpos":42,"digest":"md5-7AoW/vUMvH97fHoEiHBUFw==","length":20391,"stub":true},"simplecrawler-0.1.2.tgz":{"content_type":"application/octet-stream","revpos":40,"digest":"md5-zVoCZNcFJKemVduHudTg3w==","length":20391,"stub":true},"simplecrawler-0.1.1.tgz":{"content_type":"application/octet-stream","revpos":38,"digest":"md5-H4US8gN7ySmYjc0D1jxd6Q==","length":20191,"stub":true},"simplecrawler-0.1.0.tgz":{"content_type":"application/octet-stream","revpos":36,"digest":"md5-MBYjf9N8e0gI9l3KP0//Rw==","length":19911,"stub":true},"simplecrawler-0.0.10.tgz":{"content_type":"application/octet-stream","revpos":34,"digest":"md5-74HydWNUp8cOWKxHW2O03Q==","length":16348,"stub":true},"simplecrawler-0.0.9.tgz":{"content_type":"application/octet-stream","revpos":31,"digest":"md5-wa+vedxiQwM76FgSO/ep0w==","length":16264,"stub":true},"simplecrawler-0.0.8.tgz":{"content_type":"application/octet-stream","revpos":29,"digest":"md5-oKw9LOT9xSLCzsLnXk+SWQ==","length":16147,"stub":true},"simplecrawler-0.0.7.tgz":{"content_type":"application/octet-stream","revpos":27,"digest":"md5-g5oMSmCTdWcWQUcKYcYmrw==","length":16159,"stub":true},"simplecrawler-0.0.6.tgz":{"content_type":"application/octet-stream","revpos":25,"digest":"md5-6qyJVgFQCdMSK4vUxEN89g==","length":16115,"stub":true},"simplecrawler-0.0.5.tgz":{"content_type":"application/octet-stream","revpos":23,"digest":"md5-Nqg4MAlH4Bkd0we+nrCiIg==","length":16113,"stub":true},"simplecrawler-0.0.4.tgz":{"content_type":"application/octet-stream","revpos":21,"digest":"md5-NZ+74f8jRAh7ON6tbCM5Vg==","length":15424,"stub":true},"simplecrawler-0.0.3.tgz":{"content_type":"application/octet-stream","revpos":3,"digest":"md5-f8nKas+suTn78PUR56oaSw==","length":438,"stub":true}},"_etag":"\"BPDY46Z97Z4MNSHPCD8N0LKQI\""}